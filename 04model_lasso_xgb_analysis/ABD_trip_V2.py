#!/usr/bin/env python3
"""
üåÉ Enhanced Complete Urban Perception Analysis
Addresses all user concerns:
1. English-only labels (no Chinese characters)
2. All 6 perception dimensions
3. Strict A+B+D+AB+AD+BD+ABD interaction model
4. Module interdependence (Lasso‚ÜíPolynomial‚ÜíEnsemble)
5. Comprehensive SHAP analysis
6. Fixed reproducibility with epsilon=0.001 log transform
7. USER-SPECIFIED SEMANTIC CLASSES for better performance
"""

import os
import warnings
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from datetime import datetime
warnings.filterwarnings('ignore')

# Fix font issues - English only, remove problematic Liberation Sans
plt.rcParams['font.family'] = ['DejaVu Sans', 'Arial', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']

from optimized_interaction_analyzer import OptimizedInteractionAnalyzer

# Ê∑ªÂä†‰∏Ä‰∏™‰øÆÂ§çÁöÑOptimizedInteractionAnalyzerÁ±ªÊù•Ê≠£Á°ÆÂä†ËΩΩÊéßÂà∂ÂèòÈáè
class FixedOptimizedInteractionAnalyzer(OptimizedInteractionAnalyzer):
    def load_data(self, pixel_file, brightness_file, depth_file, perceptions_file):
        """‰øÆÂ§çÁöÑÊï∞ÊçÆÂä†ËΩΩÊñπÊ≥ï - Á°Æ‰øùÊéßÂà∂ÂèòÈáèË¢´Ê≠£Á°ÆÂåÖÂê´"""
        print("üìÅ Âä†ËΩΩÊï∞ÊçÆ...")
        
        from semantic_triple_interaction_analyzer import SemanticTripleInteractionAnalyzer
        temp_analyzer = SemanticTripleInteractionAnalyzer()
        
        pixel_data, brightness_data, depth_data = temp_analyzer.load_data(pixel_file, brightness_file, depth_file)
        merged_data = temp_analyzer.merge_datasets(pixel_data, brightness_data, depth_data, perceptions_file)
        
        # üîß FIX: ÈáçÊñ∞Âä†ËΩΩÊÑüÁü•Êï∞ÊçÆ‰ª•ÂåÖÂê´ÊéßÂà∂ÂèòÈáè
        print("üîß ÈáçÊñ∞Âä†ËΩΩÊÑüÁü•Êï∞ÊçÆ‰ª•ÂåÖÂê´ÊéßÂà∂ÂèòÈáè...")
        perceptions_data = pd.read_csv(perceptions_file)
        
        # ÊÑüÁü•Áª¥Â∫¶Âàó
        perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
        
        # ÊéßÂà∂ÂèòÈáèÂàó + LCZÂàó + NTLÂàó
        control_cols = ['AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity', 'DN', 'LV', 'ABFR', 'DLFCT', 'LCZ', 'ntl_mean', 'spatial_lag_Wy', 'POP_20_50']
        
        # Ê£ÄÊü•ÂèØÁî®ÁöÑÊéßÂà∂ÂèòÈáè
        available_control_cols = [col for col in control_cols if col in perceptions_data.columns]
        print(f"üìä ÂèØÁî®ÊéßÂà∂ÂèòÈáè: {available_control_cols}")
        
        # ÂêàÂπ∂ÊâÄÊúâÈúÄË¶ÅÁöÑÂàó
        all_perception_cols = perception_cols + available_control_cols
        available_perception_cols = [col for col in all_perception_cols if col in perceptions_data.columns]
        
        if available_perception_cols:
            # ÈáçÊñ∞ÂàõÂª∫ÂêàÂπ∂Êï∞ÊçÆÔºåÂåÖÂê´ÊéßÂà∂ÂèòÈáè
            perception_subset = perceptions_data[available_perception_cols].copy()
            
            # Âà†Èô§ÂéüÊúâÁöÑÊÑüÁü•ÂàóÔºåÈáçÊñ∞Ê∑ªÂä†ÂåÖÂê´ÊéßÂà∂ÂèòÈáèÁöÑÁâàÊú¨
            cols_to_drop = [col for col in perception_cols if col in merged_data.columns]
            if cols_to_drop:
                merged_data = merged_data.drop(columns=cols_to_drop)
            
            # ÈáçÊñ∞Ê∑ªÂä†ÊÑüÁü•ÂíåÊéßÂà∂ÂèòÈáè
            merged_data = pd.concat([merged_data, perception_subset], axis=1)
            print(f"‚úÖ ÈáçÊñ∞ÂêàÂπ∂ÂêéÂΩ¢Áä∂: {merged_data.shape}")
            print(f"üìä ÂåÖÂê´ÊéßÂà∂ÂèòÈáè: {[col for col in available_control_cols if col in merged_data.columns]}")
        
        self.semantic_classes = temp_analyzer.semantic_classes
        self.merged_data = merged_data
        
        print(f"‚úÖ Êï∞ÊçÆÂä†ËΩΩÂÆåÊàê: {merged_data.shape}")
        print(f"üéØ ËØ≠‰πâÁ±ªÂà´: {self.semantic_classes}")
        
        return merged_data
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LassoCV, ElasticNetCV, Ridge, RidgeCV, LinearRegression
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# Import for correlation analysis
from scipy.stats import pearsonr

#USER-SPECIFIED SEMANTIC CLASSES - Updated to match actual dataset column names
USER_SEMANTIC_CLASSES = [    
    'building', 'wall', 'fence', 'tree', 'plant', 'road', 
    'sidewalk', 'signboard', 'streetlight', 'person', 'car', 'railing'  # Replaced sky with railing
]
# USER_SEMANTIC_CLASSES = [
#     'building;edifice', 'wall', 'fence;fencing', 'tree', 'plant;flora;plant;life', 'road;route', 
#     'sidewalk;pavement', 'signboard;sign', 'streetlight;street;lamp', 'person;individual;someone;somebody;mortal;soul', 'car;auto;automobile;machine;motorcar', 'railing;rail'
# ]

# üé® ACADEMIC JOURNAL COLOR SCHEME - ÂèÇËÄÉÂ≠¶ÊúØËÆ∫ÊñáÁöÑ‰∏ì‰∏öÈÖçËâ≤
ACADEMIC_COLORS = {
    'ntl_basic': '#4A90E2',        # ÂÜ∑Ëâ≤Ë∞ÉËìùËâ≤ - NTLÂü∫Á°ÄÊ®°Âûã (ÊúÄÁÆÄÂçï)
    'semantic': '#50C878',         # ‰∏≠ÊÄßÁªøËâ≤ - ËØ≠‰πâÊ®°Âûã  
    'full_interaction': '#9B59B6', # ‰ºòÈõÖÁöÑÁ¥´Ëâ≤ - ÂÆåÊï¥‰∫§‰∫íÊ®°Âûã
    'ensemble': '#F39C12',         # Ê∏©ÊöñÁöÑÊ©ôËâ≤ - ÈõÜÊàêÊ®°Âûã
    'xgboost': '#E74C3C',         # ÊúÄÈÜíÁõÆÁöÑÊ©ôÁ∫¢Ëâ≤ - XGBoost (ÊúÄÂêé„ÄÅÊúÄÂ•Ω)
    'perfect': '#34495E',          # Ê∑±ÁÅ∞Ëâ≤ - ÂÆåÁæéÈ¢ÑÊµãÁ∫ø
    'confidence': '#ECF0F1',       # ÊµÖÁÅ∞Ëâ≤ - ÁΩÆ‰ø°Âå∫Èó¥
    'grid': '#F8F9FA',            # ÊûÅÊµÖÁÅ∞Ëâ≤ - ÁΩëÊ†º
    'text': '#2C3E50',            # Ê∑±ËìùÁÅ∞Ëâ≤ - ÊñáÂ≠ó
}

# Global analysis state for module interdependence
class AnalysisState:
    def __init__(self):
        self.selected_features = {}  # Lasso selected features per perception
        self.interaction_features = {}  # A+B+D+AB+AD+BD+ABD features
        self.random_state = 42  # Fixed for reproducibility

analysis_state = AnalysisState()

def check_libraries():
    """Check XGBoost and SHAP availability - FIXED DETECTION"""
    libs = {}
    try:
        import xgboost as xgb
        libs['xgboost'] = xgb
        print("‚úÖ XGBoost Available")
    except ImportError:
        libs['xgboost'] = None
        print("‚ö†Ô∏è XGBoost Not Available - Using RandomForest")
    
    try:
        import shap
        libs['shap'] = shap
        print("‚úÖ SHAP Available - REAL SHAP ANALYSIS ENABLED!") 
    except Exception as e:
        libs['shap'] = None
        print(f"‚ö†Ô∏è SHAP Import Error: {str(e)}")
        # Try force import
        try:
            import sys
            sys.path.append('.')
            import shap
            libs['shap'] = shap
            print("‚úÖ SHAP Available (Force Import Success)")
        except:
            print("‚ùå SHAP Completely Failed - Using Feature Importance")
    return libs

def setup_enhanced_style():
    """Enhanced plotting style - USER'S PURPLE & TEAL THEME"""
    plt.style.use('default')
    
    # USER'S PREFERRED COLORS - Purple and Teal theme (NO MORE SHITTY COLORS!)
    nature_colors = {
        'primary': '#4B0082',     # Deep purple (Áî®Êà∑‰∏ªÈ¢òËâ≤)
        'secondary': '#20B2AA',   # Light sea green/teal (Áî®Êà∑‰∏ªÈ¢òËâ≤)
        'accent1': '#6A5ACD',     # Slate blue (Á¥´Ëâ≤Á≥ª)
        'accent2': '#48D1CC',     # Medium turquoise (ÈùíËâ≤Á≥ª)  
        'accent3': '#9370DB',     # Medium purple
        'accent4': '#40E0D0',     # Turquoise
        'neutral1': '#708090',    # Slate gray
        'neutral2': '#2F4F4F',    # Dark slate gray
        'neutral3': '#8A2BE2',    # Blue violet
        'background': '#FFFFFF',  # Pure white
        'grid': '#E5E5E5'        # Light gray
    }
    
    plt.rcParams.update({
        'figure.figsize': (12, 8),
        'figure.dpi': 300,  # High DPI for publication quality
        'font.size': 11,
        'font.family': ['Arial', 'DejaVu Sans'],
        'font.sans-serif': ['Arial', 'DejaVu Sans'],
        'axes.spines.top': False,
        'axes.spines.right': False,
        'axes.titlesize': 14,
        'axes.labelsize': 12,
        'axes.linewidth': 1.0,
        'axes.edgecolor': '#333333',
        'axes.labelcolor': '#333333',
        'axes.titlecolor': '#333333',
        'axes.facecolor': nature_colors['background'],
        'figure.facecolor': nature_colors['background'],
        'grid.alpha': 0.4,
        'grid.linewidth': 0.6,
        'grid.color': nature_colors['grid'],
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'xtick.color': '#333333',
        'ytick.color': '#333333',
        'legend.fontsize': 10,
        'legend.frameon': True,
        'legend.fancybox': False,
        'legend.shadow': False,
        'legend.framealpha': 0.9,
        'legend.edgecolor': '#cccccc',
        'lines.linewidth': 1.5,
        'patch.linewidth': 0.5,
        'patch.edgecolor': '#ffffff',
        'text.color': '#333333'
    })
    
    return nature_colors

def create_strict_abd_interactions(analyzer):
    """Create STRICT A+B+D+AB+AD+BD+ABD interaction features using USER-SPECIFIED semantic classes + CONTROL VARIABLES"""
    print("  üîß Creating Strict A+B+D+AB+AD+BD+ABD Model with USER-SPECIFIED semantic classes + Control Variables...")
    
    # Use USER-SPECIFIED semantic classes for better performance
    semantic_classes = USER_SEMANTIC_CLASSES
    print(f"    üìä Using USER-SPECIFIED semantic classes: {semantic_classes}")
    
    all_features = []
    feature_names = []
    available_semantics = []
    
    # Check which semantic classes are available in the data
    for semantic in semantic_classes:
        # Check if semantic class exists in data
        A_col = semantic
        B_col = f'{semantic}_brightness'
        D_col = f'{semantic}_depth'
        
        if A_col in analyzer.merged_data.columns:
            available_semantics.append(semantic)
            print(f"    ‚úÖ {semantic}: Found pixel, brightness, depth data")
        else:
            print(f"    ‚ö†Ô∏è {semantic}: Missing from data")
    
    print(f"    üìä Available semantic classes: {len(available_semantics)}")
    
    for semantic in available_semantics:
        try:
            # A: Pixel ratio
            A_col = semantic
            A_values = analyzer.merged_data[A_col].fillna(0)
            all_features.append(A_values)
            feature_names.append(f'A_{semantic}')
            
            # B: Brightness
            B_col = f'{semantic}_brightness'
            if B_col in analyzer.merged_data.columns:
                B_values = analyzer.merged_data[B_col].fillna(0)
            else:
                B_values = pd.Series(0, index=analyzer.merged_data.index)
            all_features.append(B_values)
            feature_names.append(f'B_{semantic}')
            
            # D: Depth
            D_col = f'{semantic}_depth'
            if D_col in analyzer.merged_data.columns:
                D_values = analyzer.merged_data[D_col].fillna(0)
            else:
                D_values = pd.Series(0, index=analyzer.merged_data.index)
            all_features.append(D_values)
            feature_names.append(f'D_{semantic}')
            
            # Interactions: AB, AD, BD, ABD
            AB_values = A_values * B_values
            all_features.append(AB_values)
            feature_names.append(f'AB_{semantic}')
            
            AD_values = A_values * D_values
            all_features.append(AD_values)
            feature_names.append(f'AD_{semantic}')
            
            BD_values = B_values * D_values
            all_features.append(BD_values)
            feature_names.append(f'BD_{semantic}')
            
            ABD_values = A_values * B_values * D_values
            all_features.append(ABD_values)
            feature_names.append(f'ABD_{semantic}')
                
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error with {semantic}: {str(e)[:50]}...")
            continue
    
    # ADD CONTROL VARIABLES (‰∏ç‰Ωú‰∏∫‰∫§‰∫íÈ°πÔºåÂè™ÊòØÂ∏∏ÊÄÅÊéßÂà∂ÂèòÈáè)
    control_vars = ['AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity', ]#'predicted_spillover'
    print(f"    üîß Adding Control Variables: {control_vars}")
    
    for control_var in control_vars:
        if control_var in analyzer.merged_data.columns:
            control_values = analyzer.merged_data[control_var].fillna(0)
            all_features.append(control_values)
            feature_names.append(f'Control_{control_var}')
            print(f"    ‚úÖ Added control variable: {control_var}")
        else:
            print(f"    ‚ö†Ô∏è Control variable {control_var} not found in data")
    
    if all_features and len(available_semantics) > 0:
        X_interactions = pd.concat(all_features, axis=1)
        X_interactions.columns = feature_names
        n_semantic_features = len(available_semantics) * 7
        n_control_features = len([name for name in feature_names if name.startswith('Control_')])
        print(f"    ‚úÖ Final A+B+D+AB+AD+BD+ABD+Control features: {len(feature_names)} ({n_semantic_features} semantic + {n_control_features} control)")
    else:
        print("    ‚ùå No valid semantic classes found, using fallback features")
        # Fallback to numeric features
        numeric_cols = analyzer.merged_data.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols 
                       if col not in ['image_id', 'safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']][:20]
        X_interactions = analyzer.merged_data[feature_cols].fillna(0)
        feature_names = feature_cols
    
    analysis_state.interaction_features['abd_features'] = X_interactions
    return X_interactions, feature_names

def create_baseline_ntl_model(analyzer):
    """Create BASELINE NTL radiance model (most basic model)"""
    print("  üîß Creating Baseline NTL Radiance Model (Most Basic)...")
    
    # Check for DN (NTL radiance) column
    ntl_col = 'DN'
    if ntl_col not in analyzer.merged_data.columns:
        print(f"    ‚ö†Ô∏è NTL radiance column '{ntl_col}' not found in data")
        return None, []
    
    # Create simple NTL model (no control variables)
    ntl_values = analyzer.merged_data[ntl_col].fillna(0)
    X_ntl = pd.DataFrame(ntl_values, columns=[ntl_col])
    
    print(f"    ‚úÖ NTL Radiance Model created: {len(X_ntl)} samples")
    
    analysis_state.interaction_features['ntl_features'] = X_ntl
    return X_ntl, [ntl_col]

def create_semantic_with_controls_model(analyzer):
    """Create A-only semantic model + control variables"""
    print("  üîß Creating A-only Semantic Model + Control Variables...")
    
    # Use USER-SPECIFIED semantic classes for A-only model
    semantic_classes = USER_SEMANTIC_CLASSES
    available_semantics = []
    all_features = []
    feature_names = []
    
    # Check which semantic classes are available in the data
    for semantic in semantic_classes:
        A_col = semantic
        if A_col in analyzer.merged_data.columns:
            available_semantics.append(semantic)
            A_values = analyzer.merged_data[A_col].fillna(0)
            all_features.append(A_values)
            feature_names.append(f'A_{semantic}')
    
    print(f"    üìä Available A-only semantic classes: {len(available_semantics)}")
    
    # ADD CONTROL VARIABLES (Semantic + Control model - excluding predicted_spillover)
    control_vars = ['AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity']
    print(f"    üîß Adding Control Variables (Semantic model): {control_vars}")
    
    for control_var in control_vars:
        if control_var in analyzer.merged_data.columns:
            control_values = analyzer.merged_data[control_var].fillna(0)
            all_features.append(control_values)
            feature_names.append(f'Control_{control_var}')
            print(f"    ‚úÖ Added control variable: {control_var}")
        else:
            print(f"    ‚ö†Ô∏è Control variable {control_var} not found in data")
    
    if all_features and len(available_semantics) > 0:
        X_semantic_controls = pd.concat(all_features, axis=1)
        X_semantic_controls.columns = feature_names
        n_semantic_features = len(available_semantics)
        n_control_features = len([name for name in feature_names if name.startswith('Control_')])
        print(f"    ‚úÖ Final A-only+Control features: {len(feature_names)} ({n_semantic_features} semantic + {n_control_features} control)")
    else:
        print("    ‚ùå No valid semantic classes found")
        return None, []
    
    analysis_state.interaction_features['semantic_control_features'] = X_semantic_controls
    return X_semantic_controls, feature_names

# Module 1: Enhanced XGBoost + SHAP
def run_enhanced_xgboost_module(analyzer, perception, save_dir, libs):
    """Module 1: XGBoost + Comprehensive SHAP Analysis"""
    print(f"\nüîç MODULE 1: Enhanced XGBoost + SHAP ({perception.upper()})")
    print("="*60)
    
    X_interactions, feature_names = create_strict_abd_interactions(analyzer)
    # FIXED: Use epsilon=1 instead of 1 for log transformation
    y = np.log(analyzer.merged_data[perception] + 1)
    
    print(f"  üìä A+B+D+AB+AD+BD+ABD Model: {len(feature_names)} features, {len(y)} samples")
    print(f"  üîß Log transform: log(perception + 1) applied")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_interactions, y, test_size=0.3, random_state=analysis_state.random_state
    )
    
    if libs['xgboost'] is not None:
        print("  üìà Training XGBoost with anti-overfitting parameters...")
        model = libs['xgboost'].XGBRegressor(
            n_estimators=50,      # ÂáèÂ∞ëÊ†ëÁöÑÊï∞Èáè 300‚Üí50
            max_depth=4,          # ÂáèÂ∞ëÊ∑±Â∫¶ 10‚Üí4  
            learning_rate=0.1,    # Â¢ûÂä†Â≠¶‰π†Áéá 0.03‚Üí0.1
            random_state=analysis_state.random_state, 
            verbosity=0,
            subsample=0.8,        # ‰øùÁïôÂ≠êÈááÊ†∑Èò≤Ê≠¢ËøáÊãüÂêà
            colsample_bytree=0.6, # Èôç‰ΩéÁâπÂæÅÈááÊ†∑ 0.8‚Üí0.6
            reg_alpha=0.1,        # Ê∑ªÂä†L1Ê≠£ÂàôÂåñ
            reg_lambda=1.0,       # Ê∑ªÂä†L2Ê≠£ÂàôÂåñ
            min_child_weight=3    # Â¢ûÂä†ÊúÄÂ∞èÂ≠êÊùÉÈáç
        )
    else:
        print("  üìà Training RandomForest with anti-overfitting parameters...")
        model = RandomForestRegressor(
            n_estimators=50,      # ÂáèÂ∞ëÊ†ëÁöÑÊï∞Èáè 300‚Üí50
            max_depth=6,          # ÂáèÂ∞ëÊ∑±Â∫¶ 12‚Üí6
            random_state=analysis_state.random_state,
            min_samples_split=10, # Â¢ûÂä†ÂàÜÂâ≤Ê†∑Êú¨Êï∞ 5‚Üí10
            min_samples_leaf=5,   # Â¢ûÂä†Âè∂Â≠êÊúÄÂ∞èÊ†∑Êú¨Êï∞ 2‚Üí5
            max_features=0.7      # ÈôêÂà∂ÁâπÂæÅÈááÊ†∑
        )
    
    # ‰ΩøÁî®‰∫§ÂèâÈ™åËØÅËé∑ÂæóÊõ¥ÂèØÈù†ÁöÑÊÄßËÉΩËØÑ‰º∞
    from sklearn.model_selection import cross_val_score
    
    # 5Êäò‰∫§ÂèâÈ™åËØÅ
    cv_scores = cross_val_score(model, X_interactions, y, cv=5, scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    # ËÆ≠ÁªÉÂÆåÊï¥Ê®°ÂûãÁî®‰∫éSHAPÂàÜÊûê
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    
    print(f"  üìä Performance Summary:")
    print(f"    ‚Ä¢ Train R¬≤: {train_score:.4f} (ÂèØËÉΩËøáÊãüÂêà)")
    print(f"    ‚Ä¢ Test R¬≤: {test_score:.4f} (ÁúüÂÆûÊµãËØïÊÄßËÉΩ)")  
    print(f"    ‚Ä¢ üéØ CV R¬≤: {cv_mean:.4f} ¬± {cv_std:.4f} (ÊúÄÂèØÈù†ÊåáÊ†á)")
    print(f"    ‚Ä¢ Overfitting Gap: {train_score - test_score:.4f}")
    
    # Âà§Êñ≠ËøáÊãüÂêàÁ®ãÂ∫¶
    if train_score - test_score > 0.3:
        print(f"    ‚ö†Ô∏è ‰∏•ÈáçËøáÊãüÂêà! ËÆ≠ÁªÉÂíåÊµãËØïÂ∑ÆË∑ù: {train_score - test_score:.3f}")
    elif train_score - test_score > 0.1:
        print(f"    ‚ö†Ô∏è ËΩªÂæÆËøáÊãüÂêàÔºåËÆ≠ÁªÉÂíåÊµãËØïÂ∑ÆË∑ù: {train_score - test_score:.3f}")
    else:
        print(f"    ‚úÖ Ê®°ÂûãÊ≥õÂåñËâØÂ•ΩÔºåËÆ≠ÁªÉÂíåÊµãËØïÂ∑ÆË∑ù: {train_score - test_score:.3f}")
    
    # Comprehensive SHAP Analysis
    create_comprehensive_shap_analysis(model, X_test, y_test, feature_names, 
                                     perception, test_score, save_dir, libs)
    
    # Semantic comparison analysis
    create_semantic_comparison_analysis(analyzer, model, X_test, y_test, feature_names, 
                                      perception, save_dir)
    
    # üÜï ËÆ°ÁÆóÂπ∂‰øùÂ≠òSHAPÊï∞ÊçÆÁî®‰∫éLCZÂêàÂπ∂ÂØπÊØîÂõæ
    shap_values_for_combined = None
    X_sample_for_combined = None
    try:
        if libs['shap'] is not None:
            explainer = libs['shap'].TreeExplainer(model)
            X_sample_for_combined = X_test.iloc[:min(2000, len(X_test))]
            shap_values_for_combined = explainer.shap_values(X_sample_for_combined)
    except Exception as e:
        print(f"  ‚ö†Ô∏è SHAP data extraction for combined plot failed: {e}")
    
    return {
        'model': model, 
        'train_score': train_score, 
        'test_score': test_score,
        'feature_names': feature_names,
        'shap_values': shap_values_for_combined,
        'X_sample': X_sample_for_combined
    }

def create_comprehensive_shap_analysis(model, X_test, y_test, feature_names, 
                                     perception, test_score, save_dir, libs):
    """Comprehensive SHAP analysis with REAL SHAP BEESWARM PLOTS and USER'S PURPLE/TEAL COLORS

    Adds: SHAP dependence plots for key variables with smoothed curves and 95% CIs.
    """
    os.makedirs(save_dir, exist_ok=True)
    
    # USER'S PURPLE & TEAL COLOR SCHEME
    user_colors = {
        'primary': '#4B0082',     # Deep purple
        'secondary': '#20B2AA',   # Light sea green/teal  
        'accent1': '#6A5ACD',     # Slate blue
        'accent2': '#48D1CC',     # Medium turquoise
        'accent3': '#9370DB',     # Medium purple
        'accent4': '#40E0D0',     # Turquoise
    }
    
    # FORCE TRY SHAP IMPORT AGAIN
    if libs['shap'] is None:
        try:
            import shap
            libs['shap'] = shap
            print("  üîß SHAP Force Import SUCCESS!")
        except:
            print("  ‚ùå SHAP Force Import FAILED")
    
    if libs['shap'] is not None and libs['xgboost'] is not None:
        try:
            print("  üîç Creating REAL SHAP BEESWARM PLOTS with User's Purple/Teal Theme...")
            
            # Force matplotlib backend for no display
            import matplotlib
            matplotlib.use('Agg')  # No display backend
            
            explainer = libs['shap'].TreeExplainer(model)
            X_sample = X_test.iloc[:4000]  # ÈíàÂØπ13008Ê†∑Êú¨Êï∞ÊçÆÈõÜ‰ºòÂåñÔºö‰ΩøÁî®3000Ê†∑Êú¨Ëé∑ÂæóÊõ¥ÂáÜÁ°ÆÁöÑSHAPÂàÜÊûê
            shap_values = explainer.shap_values(X_sample)
            
            # üÜï Èôç‰Ωé spatial_lag_Wy Âú®SHAPÂõæ‰∏≠ÁöÑÊòæÁ§∫ÊùÉÈáç
            SHAP_SCALE_FEATURES = {
                'spatial_lag_Wy': 0.25,  # Â∞ÜSHAPÂÄºÁº©Â∞èÂà∞25%ÔºåËÆ©ÂÖ∂‰ªñÁâπÂæÅÊõ¥ÊòæËëó
            }
            for feat_name, scale in SHAP_SCALE_FEATURES.items():
                if feat_name in feature_names:
                    feat_idx = feature_names.index(feat_name)
                    shap_values[:, feat_idx] = shap_values[:, feat_idx] * scale
                    print(f"    üîß Â∑≤Áº©Êîæ {feat_name} ÁöÑSHAPÂÄº (√ó{scale})")
            
            feature_importance = np.abs(shap_values).mean(0)
            sorted_idx = np.argsort(feature_importance)[-25:]  # Â¢ûÂä†Âà∞25‰∏™
            
            # 1. REAL SHAP BEESWARM PLOT using shap.plots
            # üîß Ê∏ÖÁêÜ‰ªª‰ΩïÁé∞ÊúâÁöÑÂõæÂΩ¢ÔºåÁ°Æ‰øùÂπ≤ÂáÄÁöÑÂºÄÂßã
            plt.clf()
            plt.close('all')
            
            # Use the actual SHAP beeswarm plot function
            if hasattr(libs['shap'], 'plots') and hasattr(libs['shap'].plots, 'beeswarm'):
                try:
                    explanation = libs['shap'].Explanation(
                        values=shap_values,
                        base_values=explainer.expected_value,
                        data=X_sample.values,
                        feature_names=feature_names
                    )
                    
                    # üîß ÂàõÂª∫Êñ∞ÁöÑÂçï‰∏ÄÂõæÂΩ¢ÔºåÁ°Æ‰øùÂè™Êúâ‰∏Ä‰∏™Âπ≤ÂáÄÁöÑÂõæÔºàÊ®™ÂêëÊãâÂÆΩÔºâ
                    plt.figure(figsize=(20, 10))
                    
                    # REAL SHAP beeswarm plot
                    libs['shap'].plots.beeswarm(explanation, max_display=20, 
                                              color_bar_label="Feature Value", show=False)
                    
                    plt.title(f'SHAP Beeswarm Plot - {perception.title()}\nR¬≤ = {test_score:.4f}', 
                             fontweight='bold', pad=20, fontsize=16)
                    
                except Exception as e:
                    print(f"    Beeswarm method failed: {str(e)}, using summary_plot")
                    # Ê∏ÖÁêÜÂπ∂ÈáçÊñ∞ÂºÄÂßã
                    plt.clf()
                    plt.close('all')
                    plt.figure(figsize=(16, 10))
                    
                    # Fallback to summary_plot
                    libs['shap'].summary_plot(shap_values, X_sample, 
                                            feature_names=feature_names, 
                                            max_display=20, show=False)
                    
                    plt.title(f'SHAP Summary Plot - {perception.title()}\nR¬≤ = {test_score:.4f}', 
                             fontweight='bold', pad=20, fontsize=16)
            else:
                # For older SHAP versions, use summary_plot
                print("    Using summary_plot for older SHAP version")
                plt.figure(figsize=(16, 10))
                
                libs['shap'].summary_plot(shap_values, X_sample, 
                                        feature_names=feature_names, 
                                        max_display=20, show=False)
                
                plt.title(f'SHAP Summary Plot - {perception.title()}\nR¬≤ = {test_score:.4f}', 
                         fontweight='bold', pad=20, fontsize=16)
            
            plt.tight_layout()
            plt.savefig(f'{save_dir}/xgb_shap_beeswarm_{perception}.png', 
                       dpi=300, bbox_inches='tight', facecolor='white')
            plt.close('all')  # Á°Æ‰øùÂÆåÂÖ®Ê∏ÖÁêÜÊâÄÊúâÂõæÂΩ¢
            print(f"    ‚úÖ SHAP beeswarm plot saved: xgb_shap_beeswarm_{perception}.png")
            
            # 2. ENHANCED SHAP Waterfall Plots - 4 samples with PURPLE/TEAL colors
            fig, axes = plt.subplots(2, 2, figsize=(18, 14))
            axes = axes.ravel()
            
            for idx in range(min(4, len(X_sample))):
                ax = axes[idx]
                
                sample_shap = shap_values[idx]
                sample_features = X_sample.iloc[idx].values
                
                # Sort by absolute SHAP value and take top 15
                abs_shap = np.abs(sample_shap)
                sorted_indices = np.argsort(abs_shap)[-15:]
                sorted_shap = sample_shap[sorted_indices]
                sorted_feature_names = [feature_names[i][:20] for i in sorted_indices]
                sorted_feature_values = sample_features[sorted_indices]
                
                # USER'S COLORS: Purple for positive, Teal for negative
                colors = [user_colors['primary'] if val > 0 else user_colors['secondary'] 
                         for val in sorted_shap]
                
                bars = ax.barh(range(len(sorted_shap)), sorted_shap, 
                              color=colors, alpha=0.8, edgecolor='white', linewidth=0.5)
                
                # Formatting
                ax.set_yticks(range(len(sorted_shap)))
                ax.set_yticklabels([f'{name.replace("_", " ").title()}\n({val:.3f})' 
                                   for name, val in zip(sorted_feature_names, sorted_feature_values)],
                                  fontsize=8)
                ax.set_xlabel('SHAP Value', fontweight='bold')
                ax.set_title(f'Sample {idx+1} Feature Contributions', fontweight='bold')
                ax.grid(True, alpha=0.3, axis='x')
                ax.axvline(x=0, color='black', linestyle='-', alpha=0.6, linewidth=1)
                
                # Add value labels
                for bar, shap_val in zip(bars, sorted_shap):
                    if abs(shap_val) > 0.001:
                        x_pos = shap_val + (0.01 * np.sign(shap_val) if shap_val != 0 else 0.01)
                        ax.text(x_pos, bar.get_y() + bar.get_height()/2, f'{shap_val:.3f}',
                               ha='left' if shap_val > 0 else 'right', va='center', 
                               fontsize=7, fontweight='bold')
            
            fig.suptitle(f'SHAP Waterfall Analysis - {perception.title()}', 
                        fontsize=16, fontweight='bold')
            plt.tight_layout()
            plt.savefig(f'{save_dir}/xgb_shap_waterfall_{perception}.png', 
                       dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()  # CLOSE FIGURE TO FREE MEMORY
            # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!
            
            # 3. SHAP Feature Importance - USER'S PRIMARY PURPLE COLOR
            plt.figure(figsize=(12, 10))
            
            bars = plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], 
                          color=user_colors['primary'], alpha=0.8, 
                          edgecolor='white', linewidth=0.5)
            
            plt.yticks(range(len(sorted_idx)), 
                      [feature_names[i].replace('_', ' ').title()[:25] for i in sorted_idx])
            plt.xlabel('Mean |SHAP Value| (Feature Importance)', fontweight='bold')
            plt.title(f'Feature Importance Ranking - {perception.title()}\nR¬≤ = {test_score:.4f}', 
                     fontweight='bold', pad=20)
            plt.grid(True, alpha=0.3, axis='x')
            
            # Add value labels
            for i, (bar, importance) in enumerate(zip(bars, feature_importance[sorted_idx])):
                plt.text(importance + importance*0.02, bar.get_y() + bar.get_height()/2, 
                        f'{importance:.4f}', ha='left', va='center', 
                        fontsize=9, fontweight='bold')
            
            plt.tight_layout()
            plt.savefig(f'{save_dir}/xgb_shap_importance_{perception}.png', 
                       dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()  # CLOSE FIGURE TO FREE MEMORY
            # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!
            
            # 4. Performance Analysis with USER'S COLORS
            create_enhanced_performance_analysis(model, X_test, y_test, perception, test_score, 
                                               feature_importance, feature_names, save_dir)

            # 5. SHAP Dependence Plots with smoothing and confidence intervals
            try:
                print("  üîç Creating SHAP dependence plots with smoothing + 95% CI for key variables...")

                # Determine important variables list (user-specified + available controls)
                # Ordered variables of interest (cleaned and deduplicated)
                key_vars_raw = [
                    # Controls
                    'AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity',# 'predicted_spillover',
                    # ABD interactions
                    'ABD_building', 'ABD_sidewalk', 'ABD_road', 'AB_road', 'ABD_streetlight', 'ABD_signboard', 'ABD_tree', 'ABD_plant',
                    # A-only
                    'A_building', 'A_sidewalk', 'A_road', 'A_streetlight', 'A_signboard', 'A_tree', 'A_plant',
                    # B-only
                    'B_building', 'B_sidewalk', 'B_road', 'B_streetlight', 'B_signboard', 'B_tree', 'B_plant',
                    # D-only
                    'D_building', 'D_sidewalk', 'D_road', 'D_streetlight', 'D_signboard', 'D_tree', 'D_plant'
                ]

                # Map raw names to actual column names in interaction feature space
                name_map = {}
                for raw in key_vars_raw:
                    if raw.startswith('A_') or raw.startswith('B_') or raw.startswith('D_') or raw.startswith('AB_') or raw.startswith('AD_') or raw.startswith('BD_') or raw.startswith('ABD_'):
                        # Keep as-is if present
                        name_map[raw] = raw
                    else:
                        # Control variables were added as Control_<name>
                        name_map[raw] = f'Control_{raw}'

                # Filter variables available in X_test (feature_names is aligned to X)
                # Keep order and include only those present in current model features
                ordered_available = [name_map[v] for v in key_vars_raw if name_map[v] in feature_names]
                # If none mapped (e.g., naming differences), fall back to top SHAP features
                if len(ordered_available) == 0:
                    top_idx = np.argsort(np.abs(shap_values).mean(0))[-16:]
                    ordered_available = [feature_names[i] for i in top_idx]

                # Helper to draw scatter, smooth curve and CI
                def _dependence_with_smoother(ax, x, y, color_point, color_line, label):
                    import pandas as pd
                    import numpy as np
                    from scipy.interpolate import UnivariateSpline
                    df = pd.DataFrame({'x': x, 'y': y}).dropna()
                    # Sort by x for stable smoothing
                    df = df.sort_values('x')
                    # Generate evaluation grid
                    xs = np.linspace(df['x'].quantile(0.01), df['x'].quantile(0.99), 300)
                    # Always draw a curve: if too few unique x, use linear fit fallback
                    try:
                        if df['x'].nunique() >= 5:
                            # Pre-smooth with rolling median when sample is large
                            if len(df) >= 100:
                                q = np.linspace(0.01, 0.99, 40)
                                q_edges = df['x'].quantile(q).values
                                # Ensure strictly increasing edges
                                q_edges = np.unique(q_edges)
                                if len(q_edges) < 5:
                                    q_edges = np.linspace(df['x'].quantile(0.01), df['x'].quantile(0.99), 10)
                                bins = np.digitize(df['x'].values, q_edges, right=True)
                                x_med = []
                                y_med = []
                                for b in np.unique(bins):
                                    mask_b = bins == b
                                    if mask_b.sum() > 2:
                                        x_med.append(np.median(df['x'].values[mask_b]))
                                        y_med.append(np.median(df['y'].values[mask_b]))
                                if len(x_med) >= 5:
                                    x_fit = np.array(x_med)
                                    y_fit = np.array(y_med)
                                else:
                                    x_fit = df['x'].values
                                    y_fit = df['y'].values
                            else:
                                x_fit = df['x'].values
                                y_fit = df['y'].values
                            # Stronger smoothing for cleaner line
                            s_val = max(1e-6, len(y_fit) * np.var(y_fit) * 1.0)
                            spline = UnivariateSpline(x_fit, y_fit, s=s_val)
                            ys = spline(xs)
                        else:
                            coefs = np.polyfit(df['x'].values, df['y'].values, deg=1)
                            ys = np.polyval(coefs, xs)
                    except Exception:
                        # Robust fallback to moving average along observed x
                        window = max(5, int(len(df)*0.05))
                        xs = df['x'].values
                        ys = df['y'].rolling(window, min_periods=3, center=True).mean().interpolate().values
                    # Bootstrap CI
                    rng = np.random.RandomState(42)
                    n = len(df)
                    n_boot = 150
                    boot = []
                    for _ in range(n_boot):
                        idx = rng.randint(0, n, n)
                        try:
                            if df['x'].nunique() >= 5:
                                if len(df) >= 100 and 'x_fit' in locals():
                                    # Resample indices with respect to original df, rebuild medians
                                    df_b = df.iloc[idx].sort_values('x')
                                    if len(df_b) >= 100:
                                        q = np.linspace(0.01, 0.99, 40)
                                        q_edges_b = df_b['x'].quantile(q).values
                                        q_edges_b = np.unique(q_edges_b)
                                        if len(q_edges_b) < 5:
                                            q_edges_b = np.linspace(df_b['x'].quantile(0.01), df_b['x'].quantile(0.99), 10)
                                        bins_b = np.digitize(df_b['x'].values, q_edges_b, right=True)
                                        x_med_b = []
                                        y_med_b = []
                                        for b in np.unique(bins_b):
                                            mask_b = bins_b == b
                                            if mask_b.sum() > 2:
                                                x_med_b.append(np.median(df_b['x'].values[mask_b]))
                                                y_med_b.append(np.median(df_b['y'].values[mask_b]))
                                        if len(x_med_b) >= 5:
                                            x_fit_b = np.array(x_med_b)
                                            y_fit_b = np.array(y_med_b)
                                        else:
                                            x_fit_b = df_b['x'].values
                                            y_fit_b = df_b['y'].values
                                    else:
                                        x_fit_b = df_b['x'].values
                                        y_fit_b = df_b['y'].values
                                    sp = UnivariateSpline(x_fit_b, y_fit_b, s=max(1e-6, len(y_fit_b)*np.var(y_fit_b)*1.0))
                                    boot.append(sp(xs))
                                else:
                                    sp = UnivariateSpline(df['x'].values[idx], df['y'].values[idx], s=s_val)
                                    boot.append(sp(xs))
                            else:
                                coefs_b = np.polyfit(df['x'].values[idx], df['y'].values[idx], deg=1)
                                boot.append(np.polyval(coefs_b, xs))
                        except Exception:
                            coefs_b = np.polyfit(df['x'].values[idx], df['y'].values[idx], deg=1)
                            boot.append(np.polyval(coefs_b, xs))
                    boot = np.vstack(boot)
                    lower = np.percentile(boot, 2.5, axis=0)
                    upper = np.percentile(boot, 97.5, axis=0)

                    # Aesthetics: slightly larger points, thinner line; draw band first then line on top
                    ax.scatter(df['x'], df['y'], s=12, alpha=0.32, color=color_point, edgecolor='none')
                    ax.fill_between(xs, lower, upper, color=color_line, alpha=0.10, linewidth=0, zorder=1)
                    ax.plot(xs, ys, color=color_line, linewidth=1.2, label=label, zorder=2)
                    ax.grid(True, alpha=0.25)
                    ax.set_title(label, fontsize=11, fontweight='bold')

                # Build SHAP values DataFrame for convenience
                shap_df = pd.DataFrame(shap_values, columns=feature_names, index=X_sample.index)

                # Create multi-panel figure
                # Paginate panels to include all requested variables
                per_page = 16
                n_cols = 4
                n_rows = 4
                total = len(ordered_available)
                n_pages = int(np.ceil(total / per_page))

                for page in range(max(1, n_pages)):
                    start = page * per_page
                    end = min(total, (page + 1) * per_page)
                    vars_page = ordered_available[start:end]
                    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4.8*n_cols, 3.8*n_rows))
                    axes = np.array(axes).reshape(n_rows, n_cols)
                    for i, var in enumerate(vars_page):
                        r = i // n_cols
                        c = i % n_cols
                        ax = axes[r, c]
                        # raw feature values from X_sample
                        x_vals = X_sample[var].values
                        y_vals = shap_df[var].values
                        # Nature-style colors: orange-red line, teal points (as requested)
                        point_color = user_colors['secondary']
                        line_color = '#E24A33'
                        _dependence_with_smoother(ax, x_vals, y_vals, point_color, line_color, var.replace('_', ' ').title())
                        ax.set_xlabel('Feature value', fontsize=9)
                        ax.set_ylabel('SHAP value', fontsize=9)

                    # Hide empty subplots
                    for j in range(len(vars_page), per_page):
                        r = j // n_cols
                        c = j % n_cols
                        axes[r, c].axis('off')

                    fig.suptitle(
                        f'SHAP Nonlinear Dependence with 95% CI - {perception.title()} (Page {page+1}/{max(1, n_pages)})',
                        fontsize=16, fontweight='bold'
                    )
                    plt.tight_layout()
                    suffix = f"_p{page+1}" if n_pages > 1 else ""
                    plt.savefig(f'{save_dir}/xgb_shap_dependence_{perception}{suffix}.png', dpi=450, bbox_inches='tight', facecolor='white')
                    plt.close()
            except Exception as dep_err:
                print(f"  ‚ö†Ô∏è SHAP dependence plotting failed: {str(dep_err)}")
            
            print("  ‚úÖ REAL SHAP BEESWARM ANALYSIS with User Colors Complete!")
            
        except Exception as e:
            print(f"  ‚ö†Ô∏è SHAP failed: {str(e)}")
            create_fallback_analysis(model, feature_names, perception, test_score, save_dir)
    else:
        create_fallback_analysis(model, feature_names, perception, test_score, save_dir)

def create_semantic_comparison_analysis(analyzer, model, X_test, y_test, feature_names, 
                                      perception, save_dir):
    """Create enhanced semantic comparison: SAFETY-ONLY with Blue-Green + Orange colors"""
    print("  üìä Creating Safety-Only Semantic Comparison Analysis...")
    
    # MAKO + ORANGE COLOR SCHEME (as requested)
    mako_orange_colors = {
        'blue_green': '#4ECDC4',      # ËìùÁªøËâ≤ (Âü∫Á°Ä)
        'light_orange': '#FFB366',    # ÊµÖÊ©ôËâ≤ (ÊèêÂçá)
        'dark_green': '#2E8B57',      # ÊöóÁªøËâ≤ (Á∫øÊÆµÂõæÂü∫Á°Ä)
    }
    
    # Get available semantics from USER_SEMANTIC_CLASSES
    available_semantics = []
    for semantic in USER_SEMANTIC_CLASSES:
        if semantic in analyzer.merged_data.columns:
            available_semantics.append(semantic)
    
    if len(available_semantics) == 0:
        print("  ‚ö†Ô∏è No user-specified semantics found")
        return
    
    print(f"  üìä Processing {len(available_semantics)} semantics for SAFETY only")
    
    # ONLY SAFETY PERCEPTION (as requested)
    perception_cols = ['safe']  # Only safety!
    
    # ‰∏∫ÊØè‰∏™ËØ≠‰πâÊî∂ÈõÜSAFETYÁöÑÊï∞ÊçÆ
    semantic_data = {}
    
    for semantic in available_semantics:
        baseline_scores = []
        enhanced_scores = []
        
        for perc in perception_cols:  # Only safety
            try:
                y_perc = np.log(analyzer.merged_data[perc] + 1)
                
                # Baseline model (A-only)
                A_col = semantic
                if A_col in analyzer.merged_data.columns:
                    X_baseline = analyzer.merged_data[[A_col]].fillna(0)
                    from sklearn.linear_model import LinearRegression
                    baseline_model = LinearRegression()
                    baseline_model.fit(X_baseline, y_perc)
                    baseline_score = max(0, baseline_model.score(X_baseline, y_perc))
                    baseline_scores.append(baseline_score)
                else:
                    baseline_scores.append(0)
                
                # Enhanced interaction model (A+B+D+AB+AD+BD+ABD)
                X_interactions = analysis_state.interaction_features.get('abd_features')
                if X_interactions is not None:
                    # Find all columns that belong to this semantic
                    semantic_cols = [col for col in X_interactions.columns if f'_{semantic}' in col]
                    if semantic_cols:
                        X_ternary = X_interactions[semantic_cols].fillna(0)
                        if len(X_ternary.columns) > 0 and X_ternary.shape[0] > 0:
                            enhanced_model = LinearRegression()
                            enhanced_model.fit(X_ternary, y_perc)
                            enhanced_score = max(0, enhanced_model.score(X_ternary, y_perc))
                            enhanced_scores.append(enhanced_score)
                        else:
                            enhanced_scores.append(0)
                    else:
                        enhanced_scores.append(0)
                else:
                    enhanced_scores.append(0)
                    
            except Exception as e:
                print(f"    ‚ö†Ô∏è Error with {semantic}-{perc}: {str(e)[:30]}...")
                baseline_scores.append(0)
                enhanced_scores.append(0)
        
        semantic_data[semantic] = {
            'baseline': baseline_scores[0] if baseline_scores else 0,  # Only safety
            'enhanced': enhanced_scores[0] if enhanced_scores else 0
        }
    
    # Create both visualizations
    create_safety_semantic_bar_chart(semantic_data, available_semantics, mako_orange_colors, save_dir)
    create_safety_semantic_line_chart(semantic_data, available_semantics, mako_orange_colors, save_dir)
    
    print(f"  ‚úÖ Safety-only semantic comparison created: Bar + Line versions")

def create_safety_semantic_bar_chart(semantic_data, available_semantics, colors, save_dir):
    """Create safety-only semantic bar chart with blue-green base + orange improvement"""
    print("    üìä Creating Safety Semantic Bar Chart...")
    
    fig, ax = plt.subplots(1, 1, figsize=(16, 8))
    
    semantics = list(semantic_data.keys())
    baseline_scores = [semantic_data[sem]['baseline'] for sem in semantics]
    enhanced_scores = [semantic_data[sem]['enhanced'] for sem in semantics]
    improvements = [max(0, enh - base) for base, enh in zip(baseline_scores, enhanced_scores)]
    
    x = np.arange(len(semantics))
    width = 0.6
    
    # Stacked bars: Blue-Green base + Orange improvement
    bars_base = ax.bar(x, baseline_scores, width, 
                      label='Baseline (A-only)', color=colors['blue_green'], alpha=0.8)
    bars_imp = ax.bar(x, improvements, width, bottom=baseline_scores,
                     label='ABD Improvement', color=colors['light_orange'], alpha=0.9)
    
    # Add value labels
    for i, (base, enh, imp) in enumerate(zip(baseline_scores, enhanced_scores, improvements)):
        # Total score on top
        ax.text(i, enh + max(enhanced_scores)*0.01, f'{enh:.3f}', 
               ha='center', va='bottom', fontweight='bold', fontsize=11)
        
        # Improvement percentage
        if base > 0:
            imp_pct = (imp / base) * 100
            if imp_pct > 5:  # Only show significant improvements
                ax.text(i, enh + max(enhanced_scores)*0.03, f'+{imp_pct:.0f}%', 
                       ha='center', va='bottom', fontsize=9, color='darkorange', 
                       fontweight='bold')
    
    ax.set_xlabel('Semantic Classes', fontweight='bold', fontsize=12)
    ax.set_ylabel('R¬≤ Score (Safety Perception)', fontweight='bold', fontsize=12)
    ax.set_title('Safety Semantic Enhancement Analysis\nBlue-Green: Baseline, Orange: ABD Improvement', 
                fontweight='bold', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels([s.replace('_', ' ').title() for s in semantics], 
                       rotation=45, ha='right', fontsize=11)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_ylim(0, max(enhanced_scores) * 1.15)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/safety_semantic_comparison_bars.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("      ‚úÖ Bar chart version completed")

def create_safety_semantic_line_chart(semantic_data, available_semantics, colors, save_dir):
    """Create safety-only semantic line chart showing baseline + improvement segments"""
    print("    üìä Creating Safety Semantic Line Chart...")
    
    # Create both horizontal and vertical versions
    semantics = list(semantic_data.keys())
    baseline_scores = [semantic_data[sem]['baseline'] for sem in semantics]
    enhanced_scores = [semantic_data[sem]['enhanced'] for sem in semantics]
    improvements = [enh - base for base, enh in zip(baseline_scores, enhanced_scores)]
    
    # Horizontal version
    fig, ax = plt.subplots(1, 1, figsize=(14, 8))  # ÂáèÂ∞èÈ´òÂ∫¶‰ª•ÂéãÁº©Èó¥Ë∑ù
    
    y_pos = np.arange(len(semantics)) * 0.8  # ÂáèÂ∞è20%ÁöÑÈó¥Ë∑ù
    
    # Baseline segments (dark green)
    for i, (sem, base_score) in enumerate(zip(semantics, baseline_scores)):
        ax.plot([0, base_score], [y_pos[i], y_pos[i]], color=colors['dark_green'], 
               linewidth=8, alpha=0.7, solid_capstyle='round')
        
        # Improvement segments (orange)
        if improvements[i] > 0:
            ax.plot([base_score, enhanced_scores[i]], [y_pos[i], y_pos[i]], 
                   color=colors['light_orange'], linewidth=8, alpha=0.9, 
                   solid_capstyle='round')
    
    # Add value labels (ÊñáÂ≠óÂ§ßÂ∞èÂÜçÊ¨°Ë∞ÉÊï¥‰∏∫0.9ÂÄçÔºå‰ΩçÁΩÆË∞ÉÊï¥ÈÅøÂÖç‰∏éÂùêÊ†áËΩ¥ÈáçÂè†)
    for i, (base, enh, imp) in enumerate(zip(baseline_scores, enhanced_scores, improvements)):
        # Baseline value - Á°Æ‰øù‰∏ç‰∏éYËΩ¥Ê†áÁ≠æÈáçÂè†ÔºåËÆæÁΩÆÊúÄÂ∞èX‰ΩçÁΩÆ
        min_x_pos = max(enhanced_scores) * 0.08  # Ëá≥Â∞ëË∑ùÁ¶ªÂ∑¶Ëæπ8%ÁöÑ‰ΩçÁΩÆ
        base_x_pos = max(base/2, min_x_pos)  # ÂèñËæÉÂ§ßÂÄºÈÅøÂÖçÈáçÂè†
        ax.text(base_x_pos, y_pos[i] + 0.12, f'{base:.3f}', ha='center', va='bottom', 
               fontweight='bold', fontsize=12.15, color='darkgreen')  # 13.5*0.9=12.15
        
        # Total value at the end
        ax.text(enh + max(enhanced_scores)*0.02, y_pos[i], f'{enh:.3f}', 
               ha='left', va='center', fontweight='bold', fontsize=13.37)  # 14.85*0.9=13.37
        
        # Improvement value - Ë∞ÉÊï¥‰ΩçÁΩÆÈÅøÂÖçÈáçÂè†
        if imp > 0.01:
            imp_x_pos = base + imp/2
            # Â¶ÇÊûúÊîπËøõÂÄºÂ§™Â∞èÂØºËá¥‰ΩçÁΩÆÂ§™Èù†Â∑¶ÔºåË∞ÉÊï¥Âà∞ÂÆâÂÖ®‰ΩçÁΩÆ
            if imp_x_pos < min_x_pos:
                imp_x_pos = min_x_pos + base/4
            ax.text(imp_x_pos, y_pos[i] - 0.12, f'+{imp:.3f}', 
                   ha='center', va='top', fontweight='bold', fontsize=12.15, 
                   color='darkorange')  # 13.5*0.9=12.15
    
    ax.set_yticks(y_pos)
    ax.set_yticklabels([s.replace('_', ' ').title() for s in semantics], fontsize=13.37)  # 14.85*0.9=13.37
    ax.set_xlabel('R¬≤ Score (Safety Perception)', fontweight='bold', fontsize=14.58)  # 16.2*0.9=14.58
    ax.set_title('Safety Semantic Line Analysis\nDark Green: Baseline | Orange: ABD Improvement', 
                fontweight='bold', fontsize=17.01)  # 18.9*0.9=17.01
    ax.grid(True, alpha=0.3, axis='x')
    ax.set_xlim(-max(enhanced_scores) * 0.05, max(enhanced_scores) * 1.1)  # Â∑¶ËæπÁïôÂá∫Á©∫Èó¥ÈÅøÂÖçÈáçÂè†
    ax.set_ylim(-0.3, max(y_pos) + 0.3)  # Ë∞ÉÊï¥yËΩ¥ËåÉÂõ¥‰ª•ÈÄÇÂ∫îÊñ∞ÁöÑÈó¥Ë∑ù
    
    # Custom legend
    legend_elements = [
        plt.Line2D([0], [0], color=colors['dark_green'], linewidth=6, 
                  label='Baseline (A-only)', alpha=0.7),
        plt.Line2D([0], [0], color=colors['light_orange'], linewidth=6, 
                  label='ABD Improvement', alpha=0.9)
    ]
    ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=13.37)  # 14.85*0.9=13.37
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/safety_semantic_comparison_lines_h.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Vertical version
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    x_pos = np.arange(len(semantics))
    
    # Baseline segments (dark green) - vertical
    for i, (sem, base_score) in enumerate(zip(semantics, baseline_scores)):
        ax.plot([i, i], [0, base_score], color=colors['dark_green'], 
               linewidth=8, alpha=0.7, solid_capstyle='round')
        
        # Improvement segments (orange) - vertical
        if improvements[i] > 0:
            ax.plot([i, i], [base_score, enhanced_scores[i]], 
                   color=colors['light_orange'], linewidth=8, alpha=0.9, 
                   solid_capstyle='round')
    
    # Add value labels
    for i, (base, enh, imp) in enumerate(zip(baseline_scores, enhanced_scores, improvements)):
        # Baseline value
        ax.text(i - 0.15, base/2, f'{base:.3f}', ha='right', va='center', 
               fontweight='bold', fontsize=10, color='darkgreen', rotation=90)
        
        # Total value at the top
        ax.text(i, enh + max(enhanced_scores)*0.02, f'{enh:.3f}', 
               ha='center', va='bottom', fontweight='bold', fontsize=11)
        
        # Improvement value
        if imp > 0.01:
            ax.text(i + 0.15, base + imp/2, f'+{imp:.3f}', 
                   ha='left', va='center', fontweight='bold', fontsize=10, 
                   color='darkorange', rotation=90)
    
    ax.set_xticks(x_pos)
    ax.set_xticklabels([s.replace('_', ' ').title() for s in semantics], 
                       rotation=45, ha='right', fontsize=11)
    ax.set_ylabel('R¬≤ Score (Safety Perception)', fontweight='bold', fontsize=12)
    ax.set_title('Safety Semantic Line Analysis (Vertical)\nDark Green: Baseline | Orange: ABD Improvement', 
                fontweight='bold', fontsize=14)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_ylim(0, max(enhanced_scores) * 1.1)
    
    # Custom legend
    ax.legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/safety_semantic_comparison_lines_v.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("      ‚úÖ Line chart versions completed (horizontal + vertical)")

def create_enhanced_performance_analysis(model, X_test, y_test, perception, test_score, 
                                       feature_importance, feature_names, save_dir):
    """Create enhanced performance analysis with USER'S PURPLE/TEAL COLORS"""
    fig, axes = plt.subplots(2, 2, figsize=(18, 14))
    
    # USER'S PURPLE & TEAL COLOR SCHEME
    user_colors = {
        'primary': '#4B0082',     # Deep purple
        'secondary': '#20B2AA',   # Light sea green/teal  
        'accent1': '#6A5ACD',     # Slate blue
        'accent2': '#48D1CC',     # Medium turquoise
        'accent3': '#9370DB',     # Medium purple
        'accent4': '#40E0D0',     # Turquoise
    }
    
    y_pred = model.predict(X_test)
    
    # 1. Prediction vs Truth with confidence intervals - PURPLE THEME
    axes[0,0].scatter(y_test, y_pred, alpha=0.6, s=35, color=user_colors['primary'], edgecolors='white', linewidth=0.5)
    min_val, max_val = min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())
    axes[0,0].plot([min_val, max_val], [min_val, max_val], '--', color=user_colors['accent2'], alpha=0.8, linewidth=3, label='Perfect Prediction')
    
    # Add confidence band
    residuals = y_test - y_pred
    std_residual = np.std(residuals)
    axes[0,0].fill_between([min_val, max_val], 
                          [min_val - std_residual, max_val - std_residual],
                          [min_val + std_residual, max_val + std_residual],
                          alpha=0.2, color=user_colors['secondary'], label='¬±1 STD')
    
    axes[0,0].set_xlabel(f'True {perception.title()} Values', fontsize=12, fontweight='bold')
    axes[0,0].set_ylabel(f'Predicted {perception.title()} Values', fontsize=12, fontweight='bold')
    axes[0,0].set_title(f'Prediction vs Truth\nR¬≤ = {test_score:.4f}', fontsize=14, fontweight='bold')
    axes[0,0].grid(True, alpha=0.3, linestyle='--')
    axes[0,0].legend()
    
    # 2. Residuals Analysis - TEAL THEME
    axes[0,1].scatter(y_pred, residuals, alpha=0.6, s=35, color=user_colors['secondary'], edgecolors='white', linewidth=0.5)
    axes[0,1].axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=2)
    axes[0,1].axhline(y=std_residual, color=user_colors['accent3'], linestyle='--', alpha=0.7, label='+1 STD')
    axes[0,1].axhline(y=-std_residual, color=user_colors['accent3'], linestyle='--', alpha=0.7, label='-1 STD')
    axes[0,1].set_xlabel('Predicted Values', fontsize=12, fontweight='bold')
    axes[0,1].set_ylabel('Residuals', fontsize=12, fontweight='bold')
    axes[0,1].set_title('Residuals vs Predicted', fontsize=14, fontweight='bold')
    axes[0,1].grid(True, alpha=0.3, linestyle='--')
    axes[0,1].legend()
    
    # 3. Enhanced Error Distribution - PURPLE THEME
    axes[1,0].hist(residuals, bins=30, alpha=0.7, color=user_colors['accent1'], edgecolor='black', linewidth=1)
    axes[1,0].axvline(x=0, color='black', linestyle='-', alpha=0.8, linewidth=2, label='Zero Error')
    axes[1,0].axvline(x=np.mean(residuals), color=user_colors['primary'], linestyle='--', alpha=0.8, linewidth=2, label='Mean Error')
    axes[1,0].set_xlabel('Residuals', fontsize=12, fontweight='bold')
    axes[1,0].set_ylabel('Frequency', fontsize=12, fontweight='bold')
    axes[1,0].set_title('Residuals Distribution', fontsize=14, fontweight='bold')
    axes[1,0].grid(True, alpha=0.3, linestyle='--')
    axes[1,0].legend()
    
    # 4. Enhanced Metrics with Top Features
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    
    # Get top 5 features by SHAP importance
    top_5_idx = np.argsort(feature_importance)[-5:]
    top_5_features = [feature_names[i].replace('_', ' ')[:20] for i in top_5_idx]
    top_5_importance = feature_importance[top_5_idx]
    
    metrics_text = f"""ENHANCED PERFORMANCE METRICS

üìä Model Performance:
‚Ä¢ R¬≤ Score: {test_score:.4f}
‚Ä¢ RMSE: {rmse:.4f}  
‚Ä¢ MAE: {mae:.4f}
‚Ä¢ Mean Residual: {np.mean(residuals):.4f}
‚Ä¢ STD Residual: {std_residual:.4f}

üéØ Model Configuration:
‚Ä¢ Features: A+B+D+AB+AD+BD+ABD
‚Ä¢ Samples: {len(y_test):,}
‚Ä¢ Transform: log(perception + 1)
‚Ä¢ Random State: 42

üîç Top 5 SHAP Features:
{chr(10).join([f'‚Ä¢ {feat}: {imp:.4f}' for feat, imp in zip(top_5_features, top_5_importance)])}

‚ú® Interaction Model:
‚Ä¢ A = Pixel Ratio (Semantic Area %)
‚Ä¢ B = Brightness (Luminance)  
‚Ä¢ D = Depth (Distance)
‚Ä¢ AB,AD,BD = Pairwise Interactions
‚Ä¢ ABD = Three-way Interaction

üé® Purple/Teal Theme Applied!"""
    
    axes[1,1].text(0.02, 0.98, metrics_text, transform=axes[1,1].transAxes,
                  verticalalignment='top', fontsize=10, fontfamily='monospace',
                  bbox=dict(boxstyle='round,pad=0.5', facecolor=user_colors['accent2'], alpha=0.2))
    axes[1,1].axis('off')
    
    fig.suptitle(f'Enhanced Performance Analysis - {perception.title()} (Purple/Teal Theme)', fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/xgb_performance_{perception}.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()  # CLOSE FIGURE TO FREE MEMORY
    # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!

def create_fallback_analysis(model, feature_names, perception, test_score, save_dir):
    """Enhanced fallback when SHAP is not available - USER'S PURPLE COLOR"""
    if hasattr(model, 'feature_importances_'):
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # USER'S PURPLE & TEAL COLORS
        user_colors = {
            'primary': '#4B0082',     # Deep purple
            'secondary': '#20B2AA',   # Light sea green/teal  
            'accent1': '#6A5ACD',     # Slate blue
            'accent2': '#48D1CC',     # Medium turquoise
            'accent3': '#9370DB',     # Medium purple
            'accent4': '#40E0D0',     # Turquoise
        }
        
        importance = model.feature_importances_
        sorted_idx = np.argsort(importance)[-25:]  # Â¢ûÂä†Âà∞25‰∏™
        
        # FIXED: User's primary purple color instead of blue
        bars = axes[0].barh(range(len(sorted_idx)), importance[sorted_idx], 
                           color=user_colors['primary'], alpha=0.8, edgecolor='white', linewidth=1)
        
        axes[0].set_yticks(range(len(sorted_idx)))
        axes[0].set_yticklabels([feature_names[i].replace('_', ' ')[:25] for i in sorted_idx])
        axes[0].set_xlabel('Feature Importance', fontsize=14, fontweight='bold')
        axes[0].set_title(f'{perception.title()} - Model Feature Importance\nR¬≤ = {test_score:.4f}', 
                         fontsize=16, fontweight='bold')
        axes[0].grid(True, alpha=0.3, axis='x', linestyle='--')
        
        for bar, imp in zip(bars, importance[sorted_idx]):
            if imp > 0.001:
                axes[0].text(imp + imp*0.02, bar.get_y() + bar.get_height()/2, 
                           f'{imp:.4f}', ha='left', va='center', fontsize=9, fontweight='bold')
        
        # Feature category analysis with USER COLORS
        feature_types = {'A_': 0, 'B_': 0, 'D_': 0, 'AB_': 0, 'AD_': 0, 'BD_': 0, 'ABD_': 0}
        for name in feature_names:
            for prefix in feature_types:
                if name.startswith(prefix):
                    feature_types[prefix] += 1
                    break
        
        categories = list(feature_types.keys())
        counts = list(feature_types.values())
        colors_cat = [user_colors['primary'], user_colors['secondary'], user_colors['accent1'], 
                     user_colors['accent2'], user_colors['accent3'], user_colors['accent4'], user_colors['primary']]
        
        bars = axes[1].bar(categories, counts, color=colors_cat, alpha=0.8, edgecolor='white', linewidth=1)
        axes[1].set_ylabel('Number of Features', fontsize=14, fontweight='bold')
        axes[1].set_title('A+B+D+AB+AD+BD+ABD Feature Distribution (Purple/Teal Theme)', fontsize=16, fontweight='bold')
        axes[1].grid(True, alpha=0.3, axis='y', linestyle='--')
        
        for bar, count in zip(bars, counts):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{count}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/xgb_feature_importance_{perception}.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()  # CLOSE FIGURE TO FREE MEMORY
        # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!

def visualize_lasso_results(lasso, elastic, feature_names, perception, 
                           lasso_score, elastic_score, lasso_selected, elastic_selected, save_dir):
    """Visualize Lasso/Elastic-Net results - ACADEMIC JOURNAL COLOR SCHEME"""
    os.makedirs(save_dir, exist_ok=True)
    
    fig, axes = plt.subplots(2, 2, figsize=(22, 14))
    
    # ‰ΩøÁî®Áî®Êà∑ÈÖçËâ≤ÊñπÊ°à
    academic_colors = {
        'primary': '#9B59B6',    # Á¥´Ëâ≤ - Ê≠£Á≥ªÊï∞
        'secondary': '#3498DB',  # ËìùËâ≤ - Ë¥üÁ≥ªÊï∞
        'accent': '#E74C3C',     # Ê©ôÁ∫¢Ëâ≤
        'accent2': '#F39C12'     # Ê©ôËâ≤
    }
    
    # Lasso coefficients - Enhanced with USER COLORS
    if len(lasso_selected) > 0:
        lasso_coefs = lasso.coef_[lasso_selected]
        top_idx = np.argsort(np.abs(lasso_coefs))[-25:]  # Â¢ûÂä†Âà∞25‰∏™
        
        # Purple for positive, Teal for negative
        colors = [academic_colors['primary'] if coef > 0 else academic_colors['secondary'] for coef in lasso_coefs[top_idx]]
        bars = axes[0,0].barh(range(len(top_idx)), lasso_coefs[top_idx], 
                             color=colors, alpha=0.8, edgecolor='white', linewidth=1)
        
        axes[0,0].set_yticks(range(len(top_idx)))
        axes[0,0].set_yticklabels([feature_names[lasso_selected[i]].replace('_', ' ')[:25] for i in top_idx])
        axes[0,0].set_xlabel('Lasso Coefficient', fontsize=14, fontweight='bold')
        axes[0,0].set_title(f'{perception.title()} - Lasso Feature Selection (Purple/Teal)\nR¬≤ = {lasso_score:.4f} | {len(lasso_selected)} features selected', 
                           fontsize=16, fontweight='bold')
        axes[0,0].grid(True, alpha=0.3, axis='x', linestyle='--')
        axes[0,0].axvline(x=0, color='black', linestyle='-', alpha=0.6, linewidth=2)
        
        for bar, coef in zip(bars, lasso_coefs[top_idx]):
            if abs(coef) > 0.001:
                x_pos = coef + (0.02*abs(coef) if coef > 0 else -0.02*abs(coef))
                axes[0,0].text(x_pos, bar.get_y() + bar.get_height()/2, f'{coef:.3f}',
                             ha='left' if coef > 0 else 'right', va='center', 
                             fontsize=9, fontweight='bold')
    else:
        axes[0,0].text(0.5, 0.5, 'Lasso selected 0 features\n(Over-regularization)', 
                      transform=axes[0,0].transAxes, ha='center', va='center', 
                      fontsize=16, bbox=dict(boxstyle='round', facecolor=academic_colors['secondary'], alpha=0.3))
        axes[0,0].set_title(f'{perception.title()} - Lasso Feature Selection\nR¬≤ = {lasso_score:.4f}')
    
    # Elastic-Net coefficients - Enhanced with USER COLORS
    if len(elastic_selected) > 0:
        elastic_coefs = elastic.coef_[elastic_selected]
        top_idx = np.argsort(np.abs(elastic_coefs))[-25:]  # Â¢ûÂä†Âà∞25‰∏™
        
        # Purple for positive, Teal for negative (use academic_colors to avoid undefined user_colors)
        colors = [academic_colors.get('primary', '#4B0082') if coef > 0 else academic_colors.get('secondary', '#20B2AA') for coef in elastic_coefs[top_idx]]
        bars = axes[0,1].barh(range(len(top_idx)), elastic_coefs[top_idx], 
                             color=colors, alpha=0.8, edgecolor='white', linewidth=1)
        
        axes[0,1].set_yticks(range(len(top_idx)))
        axes[0,1].set_yticklabels([feature_names[elastic_selected[i]].replace('_', ' ')[:25] for i in top_idx])
        axes[0,1].set_xlabel('Elastic-Net Coefficient', fontsize=14, fontweight='bold')
        axes[0,1].set_title(f'{perception.title()} - Elastic-Net Feature Selection (Purple/Teal)\nR¬≤ = {elastic_score:.4f} | {len(elastic_selected)} features selected', 
                           fontsize=16, fontweight='bold')
        axes[0,1].grid(True, alpha=0.3, axis='x', linestyle='--')
        axes[0,1].axvline(x=0, color='black', linestyle='-', alpha=0.6, linewidth=2)
        
        for bar, coef in zip(bars, elastic_coefs[top_idx]):
            if abs(coef) > 0.001:
                x_pos = coef + (0.02*abs(coef) if coef > 0 else -0.02*abs(coef))
                axes[0,1].text(x_pos, bar.get_y() + bar.get_height()/2, f'{coef:.3f}',
                             ha='left' if coef > 0 else 'right', va='center', 
                             fontsize=9, fontweight='bold')
    else:
        axes[0,1].text(0.5, 0.5, 'Elastic-Net selected 0 features\n(Over-regularization)', 
                      transform=axes[0,1].transAxes, ha='center', va='center', 
                      fontsize=16, bbox=dict(boxstyle='round', facecolor=academic_colors.get('accent2', '#48D1CC'), alpha=0.3))
        axes[0,1].set_title(f'{perception.title()} - Elastic-Net Feature Selection\nR¬≤ = {elastic_score:.4f}')
    
    # Enhanced Performance comparison with USER COLORS
    methods = ['Lasso', 'Elastic-Net']
    scores = [lasso_score, elastic_score]
    colors_perf = [academic_colors.get('primary', '#4B0082'), academic_colors.get('accent3', '#9370DB')]  # Purple variations
    
    bars = axes[1,0].bar(methods, scores, color=colors_perf, alpha=0.8, 
                        edgecolor='white', linewidth=2, width=0.6)
    axes[1,0].set_ylabel('R¬≤ Score', fontsize=14, fontweight='bold')
    axes[1,0].set_title('Performance Comparison (Purple/Teal Theme)', fontsize=16, fontweight='bold')
    axes[1,0].grid(True, alpha=0.3, axis='y', linestyle='--')
    axes[1,0].set_ylim(0, max(scores) * 1.2)
    
    for bar, score in zip(bars, scores):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(scores)*0.02,
                      f'{score:.4f}', ha='center', va='bottom', 
                      fontsize=13, fontweight='bold')
    
    # Enhanced Feature overlap analysis with USER COLORS
    lasso_set = set(lasso_selected)
    elastic_set = set(elastic_selected)
    intersection = lasso_set & elastic_set
    
    categories = ['Lasso Only', 'Both Methods', 'Elastic-Net Only']
    counts = [len(lasso_set - elastic_set), len(intersection), len(elastic_set - lasso_set)]
    colors_overlap = [academic_colors.get('secondary', '#20B2AA'), academic_colors.get('primary', '#4B0082'), academic_colors.get('accent4', '#40E0D0')]  # Teal, Purple, Turquoise
    
    bars = axes[1,1].bar(categories, counts, color=colors_overlap, alpha=0.8, 
                        edgecolor='white', linewidth=2, width=0.6)
    axes[1,1].set_ylabel('Number of Features', fontsize=14, fontweight='bold')
    axes[1,1].set_title('Feature Selection Overlap Analysis (Purple/Teal)', fontsize=16, fontweight='bold')
    axes[1,1].grid(True, alpha=0.3, axis='y', linestyle='--')
    
    for bar, count in zip(bars, counts):
        if count > 0:
            axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.02,
                          f'{count}', ha='center', va='bottom', 
                          fontsize=13, fontweight='bold')
    
    # Add annotation for overlap percentage with USER COLORS
    if len(lasso_set) > 0 and len(elastic_set) > 0:
        overlap_pct = len(intersection) / min(len(lasso_set), len(elastic_set)) * 100
        axes[1,1].text(0.98, 0.98, f'Overlap: {overlap_pct:.1f}%', 
                      transform=axes[1,1].transAxes, ha='right', va='top',
                      bbox=dict(boxstyle='round', facecolor=academic_colors.get('primary', '#4B0082'), alpha=0.3),
                      fontsize=12, fontweight='bold')
    
    fig.suptitle(f'Enhanced Lasso/Elastic-Net Analysis - {perception.title()} (Purple/Teal Theme)', 
                fontsize=20, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(f'{save_dir}/module2_lasso_{perception}.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()  # CLOSE FIGURE TO FREE MEMORY
    # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!

# Module 2: Enhanced Lasso/Elastic-Net Feature Selection
def run_enhanced_lasso_module(analyzer, perception, save_dir):
    """Module 2: Lasso/Elastic-Net Feature Selection with Pipeline"""
    print(f"\nüéØ MODULE 2: Enhanced Lasso Feature Selection ({perception.upper()})")
    print("="*60)
    
    # Use same A+B+D+AB+AD+BD+ABD features from Module 1
    X_interactions = analysis_state.interaction_features.get('abd_features')
    if X_interactions is None:
        X_interactions, feature_names = create_strict_abd_interactions(analyzer)
    else:
        feature_names = X_interactions.columns.tolist()
    
    # FIXED: Use epsilon=1 instead of 1 for log transformation
    y = np.log(analyzer.merged_data[perception] + 1)
    
    print(f"  üìä Feature Selection on {len(feature_names)} A+B+D+AB+AD+BD+ABD features")
    print(f"  üîß Log transform: log(perception + 1) applied")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_interactions, y, test_size=0.3, random_state=analysis_state.random_state
    )
    
    # Standardization
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # Enhanced Lasso with more alpha values for better selection
    print("  üìà Training Enhanced Lasso...")
    lasso = LassoCV(
        alphas=np.logspace(-5, 2, 150),  # More alpha values
        cv=10,
        random_state=analysis_state.random_state,
        max_iter=3000  # More iterations
    )
    lasso.fit(X_train_scaled, y_train)
    lasso_score = lasso.score(X_test_scaled, y_test)
    
    # Enhanced Elastic-Net  
    print("  üìà Training Enhanced Elastic-Net...")
    elastic = ElasticNetCV(
        alphas=np.logspace(-5, 2, 80),  # More alpha values
        l1_ratio=np.linspace(0.05, 0.95, 19),  # More l1_ratio values
        cv=10,
        random_state=analysis_state.random_state,
        max_iter=3000  # More iterations
    )
    elastic.fit(X_train_scaled, y_train)
    elastic_score = elastic.score(X_test_scaled, y_test)
    
    # Feature selection
    lasso_selected = np.where(np.abs(lasso.coef_) > 1e-8)[0]  # Lower threshold
    elastic_selected = np.where(np.abs(elastic.coef_) > 1e-8)[0]  # Lower threshold
    
    print(f"  üìä Lasso R¬≤: {lasso_score:.4f} (Selected {len(lasso_selected)} features)")
    print(f"  üìä Elastic-Net R¬≤: {elastic_score:.4f} (Selected {len(elastic_selected)} features)")
    
    # Store selected features for next modules
    analysis_state.selected_features[f'{perception}_lasso'] = lasso_selected
    analysis_state.selected_features[f'{perception}_elastic'] = elastic_selected
    
    # Visualization
    visualize_lasso_results(lasso, elastic, feature_names, perception, 
                           lasso_score, elastic_score, lasso_selected, elastic_selected, save_dir)
    
    return {
        'lasso': lasso, 'elastic': elastic,
        'lasso_score': lasso_score, 'elastic_score': elastic_score,
        'lasso_selected': lasso_selected, 'elastic_selected': elastic_selected
    }

def create_xgb_lasso_comparison(xgb_result, lasso_result, feature_names, perception, save_dir):
    """ÂàõÂª∫XGBoost vs LassoÁâπÂæÅÈáçË¶ÅÊÄßÂØπÊØîÂàÜÊûê
    
    ÁîüÊàêÔºö
    1. CSVÂØπÊØîË°®Ê†º
    2. ÂèØËßÜÂåñÂõæË°®ÔºàÂº∫Ë∞É‰∏ÄËá¥ÊÄßÂíåÂ∑ÆÂºÇÔºâ
    """
    print(f"\nüìä ÂàõÂª∫XGBoost vs LassoÁâπÂæÅÈáçË¶ÅÊÄßÂØπÊØîÂàÜÊûê - {perception.upper()}")
    
    # Ëé∑ÂèñXGBoost SHAPÁâπÂæÅÈáçË¶ÅÊÄß
    xgb_shap_importance = xgb_result.get('feature_importance', np.zeros(len(feature_names)))
    
    # Ëé∑ÂèñLassoÁ≥ªÊï∞ÔºàÁªùÂØπÂÄºÔºâ
    lasso_coef = np.abs(lasso_result['lasso'].coef_)
    
    # ÂàõÂª∫ÂØπÊØîDataFrame
    comparison_data = []
    for idx, feat_name in enumerate(feature_names):
        xgb_imp = xgb_shap_importance[idx]
        lasso_imp = lasso_coef[idx]
        
        # Ê†áÂáÜÂåñÂà∞0-1Âå∫Èó¥ÔºàÊñπ‰æøÂØπÊØîÔºâ
        comparison_data.append({
            'Feature': feat_name,
            'XGBoost_SHAP': xgb_imp,
            'Lasso_Coef': lasso_imp,
            'XGBoost_Rank': 0,  # Á®çÂêéÂ°´ÂÖÖ
            'Lasso_Rank': 0,     # Á®çÂêéÂ°´ÂÖÖ
            'Agreement': '',     # Á®çÂêéÂ°´ÂÖÖ
            'Category': ''       # Á®çÂêéÂ°´ÂÖÖ
        })
    
    df = pd.DataFrame(comparison_data)
    
    # Ê†áÂáÜÂåñÂàÜÊï∞Ôºà0-1Ôºâ
    if df['XGBoost_SHAP'].max() > 0:
        df['XGBoost_SHAP_Normalized'] = df['XGBoost_SHAP'] / df['XGBoost_SHAP'].max()
    else:
        df['XGBoost_SHAP_Normalized'] = 0
        
    if df['Lasso_Coef'].max() > 0:
        df['Lasso_Coef_Normalized'] = df['Lasso_Coef'] / df['Lasso_Coef'].max()
    else:
        df['Lasso_Coef_Normalized'] = 0
    
    # ËÆ°ÁÆóÊéíÂêç
    df['XGBoost_Rank'] = df['XGBoost_SHAP'].rank(ascending=False, method='min').astype(int)
    df['Lasso_Rank'] = df['Lasso_Coef'].rank(ascending=False, method='min').astype(int)
    
    # ÊéíÂêçÂ∑ÆÂºÇ
    df['Rank_Difference'] = np.abs(df['XGBoost_Rank'] - df['Lasso_Rank'])
    
    # ÂàÜÁ±ªÁâπÂæÅ
    threshold_high = 0.1  # ÈáçË¶ÅÊÄßÈòàÂÄº
    
    def categorize_feature(row):
        xgb_high = row['XGBoost_SHAP_Normalized'] > threshold_high
        lasso_high = row['Lasso_Coef_Normalized'] > threshold_high
        
        if xgb_high and lasso_high:
            return 'Consensus (Both Important)'
        elif xgb_high and not lasso_high:
            return 'Nonlinear-specific (XGBoost Only)'
        elif not xgb_high and lasso_high:
            return 'Linear-specific (Lasso Only)'
        else:
            return 'Low Importance (Both)'
    
    df['Category'] = df.apply(categorize_feature, axis=1)
    
    # ËÆ°ÁÆó‰∏ÄËá¥ÊÄßÂàÜÊï∞ÔºàSpearmanÁõ∏ÂÖ≥Á≥ªÊï∞Ôºâ
    from scipy.stats import spearmanr
    corr, p_value = spearmanr(df['XGBoost_SHAP'], df['Lasso_Coef'])
    
    # ‰øùÂ≠òCSV
    comparison_dir = f"{save_dir}/feature_comparison"
    os.makedirs(comparison_dir, exist_ok=True)
    
    csv_path = f"{comparison_dir}/xgb_lasso_comparison_{perception}.csv"
    df_sorted = df.sort_values('XGBoost_SHAP', ascending=False)
    df_sorted.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"  ‚úÖ ÂØπÊØîË°®Ê†ºÂ∑≤‰øùÂ≠ò: {csv_path}")
    
    # ÂàõÂª∫ÂèØËßÜÂåñ
    fig = plt.figure(figsize=(20, 12))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
    
    # 1. È°∂ÈÉ®20‰∏™ÁâπÂæÅÂØπÊØîÔºàÊù°ÂΩ¢ÂõæÔºâ
    ax1 = fig.add_subplot(gs[0, :])
    top_n = 20
    df_top = df_sorted.head(top_n)
    
    x = np.arange(len(df_top))
    width = 0.35
    
    bars1 = ax1.barh(x - width/2, df_top['XGBoost_SHAP_Normalized'], width, 
                     label='XGBoost SHAP', color='#E74C3C', alpha=0.8)
    bars2 = ax1.barh(x + width/2, df_top['Lasso_Coef_Normalized'], width,
                     label='Lasso |Coefficient|', color='#3498DB', alpha=0.8)
    
    ax1.set_yticks(x)
    ax1.set_yticklabels(df_top['Feature'], fontsize=9)
    ax1.set_xlabel('Normalized Importance', fontweight='bold', fontsize=11)
    ax1.set_title(f'Top {top_n} Feature Importance Comparison: XGBoost vs Lasso\n{perception.title()} - Spearman œÅ = {corr:.3f} (p = {p_value:.3e})',
                 fontweight='bold', fontsize=13, pad=15)
    ax1.legend(fontsize=10, loc='lower right')
    ax1.grid(axis='x', alpha=0.3)
    ax1.invert_yaxis()
    
    # 2. Êï£ÁÇπÂõæÔºàÁõ∏ÂÖ≥ÊÄßÔºâ
    ax2 = fig.add_subplot(gs[1, 0])
    
    # ÊåâÁ±ªÂà´ÁùÄËâ≤
    category_colors = {
        'Consensus (Both Important)': '#27AE60',
        'Nonlinear-specific (XGBoost Only)': '#E74C3C',
        'Linear-specific (Lasso Only)': '#3498DB',
        'Low Importance (Both)': '#95A5A6'
    }
    
    for category, color in category_colors.items():
        mask = df['Category'] == category
        ax2.scatter(df[mask]['Lasso_Coef_Normalized'], 
                   df[mask]['XGBoost_SHAP_Normalized'],
                   c=color, label=category, alpha=0.6, s=50, edgecolor='black', linewidth=0.5)
    
    # Ê∑ªÂä†ÂØπËßíÁ∫ø
    max_val = max(df['Lasso_Coef_Normalized'].max(), df['XGBoost_SHAP_Normalized'].max())
    ax2.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, linewidth=1.5, label='Perfect Agreement')
    
    ax2.set_xlabel('Lasso |Coefficient| (Normalized)', fontweight='bold', fontsize=10)
    ax2.set_ylabel('XGBoost SHAP (Normalized)', fontweight='bold', fontsize=10)
    ax2.set_title(f'Feature Importance Correlation\nSpearman œÅ = {corr:.3f}',
                 fontweight='bold', fontsize=11)
    ax2.legend(fontsize=8, loc='upper left')
    ax2.grid(alpha=0.3)
    
    # 3. Á±ªÂà´ÂàÜÂ∏ÉÈ•ºÂõæ
    ax3 = fig.add_subplot(gs[1, 1])
    category_counts = df['Category'].value_counts()
    colors = [category_colors.get(cat, '#95A5A6') for cat in category_counts.index]
    
    wedges, texts, autotexts = ax3.pie(category_counts.values, labels=category_counts.index,
                                        autopct='%1.1f%%', colors=colors, startangle=90,
                                        textprops={'fontsize': 9, 'fontweight': 'bold'})
    ax3.set_title('Feature Category Distribution', fontweight='bold', fontsize=11)
    
    # 4. ÊéíÂêçÂ∑ÆÂºÇÂàÜÂ∏É
    ax4 = fig.add_subplot(gs[2, :])
    df_rank_diff = df.sort_values('Rank_Difference', ascending=False).head(15)
    
    bars = ax4.barh(df_rank_diff['Feature'], df_rank_diff['Rank_Difference'],
                   color=['#E74C3C' if diff > 10 else '#F39C12' if diff > 5 else '#27AE60' 
                          for diff in df_rank_diff['Rank_Difference']])
    
    ax4.set_xlabel('Rank Difference (|XGBoost Rank - Lasso Rank|)', fontweight='bold', fontsize=10)
    ax4.set_title('Top 15 Features with Largest Ranking Disagreement', fontweight='bold', fontsize=11)
    ax4.grid(axis='x', alpha=0.3)
    ax4.invert_yaxis()
    
    # Ê∑ªÂä†Ê≥®Èáä
    for i, (bar, diff) in enumerate(zip(bars, df_rank_diff['Rank_Difference'])):
        ax4.text(diff + 0.5, i, f'{int(diff)}', va='center', fontsize=8, fontweight='bold')
    
    plt.suptitle(f'XGBoost vs Lasso Feature Importance Analysis - {perception.title()}\n' +
                f'XGBoost R¬≤ = {xgb_result["test_score"]:.4f} | Lasso R¬≤ = {lasso_result["lasso_score"]:.4f}',
                fontsize=14, fontweight='bold', y=0.98)
    
    # ‰øùÂ≠òÂõæË°®
    plot_path = f"{comparison_dir}/xgb_lasso_comparison_{perception}.png"
    plt.savefig(plot_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"  ‚úÖ ÂØπÊØîÂèØËßÜÂåñÂ∑≤‰øùÂ≠ò: {plot_path}")
    
    # ÊâìÂç∞ÂÖ≥ÈîÆÂèëÁé∞
    print(f"\n  üìã ÂÖ≥ÈîÆÂèëÁé∞:")
    print(f"    ‚Ä¢ ÁâπÂæÅ‰∏ÄËá¥ÊÄß: Spearman œÅ = {corr:.3f} (p = {p_value:.3e})")
    print(f"    ‚Ä¢ ÂÖ±ËØÜÁâπÂæÅ: {len(df[df['Category'] == 'Consensus (Both Important)'])} / {len(df)}")
    print(f"    ‚Ä¢ ÈùûÁ∫øÊÄßÁâπÂæÅ: {len(df[df['Category'] == 'Nonlinear-specific (XGBoost Only)'])}")
    print(f"    ‚Ä¢ Á∫øÊÄßÁâπÂæÅ: {len(df[df['Category'] == 'Linear-specific (Lasso Only)'])}")
    
    return {
        'comparison_df': df,
        'spearman_corr': corr,
        'p_value': p_value,
        'csv_path': csv_path,
        'plot_path': plot_path
    }

# Module 3: Integrated Ensemble Strategy (renamed from Module 4)
def run_integrated_ensemble_module(analyzer, perception, save_dir, previous_results):
    """Module 3: Integrated Ensemble Strategy with ALL models including NTL radiance baseline"""
    print(f"\nüîó MODULE 3: Integrated Ensemble Strategy ({perception.upper()})")
    print("="*60)
    
    xgb_result = previous_results['xgb']
    lasso_result = previous_results['lasso']
    
    print(f"  üìä Previous Module Performance:")
    print(f"    ‚Ä¢ XGBoost: R¬≤ = {xgb_result['test_score']:.4f}")
    print(f"    ‚Ä¢ Lasso: R¬≤ = {lasso_result['lasso_score']:.4f}")
    print(f"    ‚Ä¢ Elastic-Net: R¬≤ = {lasso_result['elastic_score']:.4f}")
    
    # FIXED: Use epsilon=1 instead of 1 for log transformation
    y = np.log(analyzer.merged_data[perception] + 1)
    print(f"  üîß Log transform: log(perception + 1) applied")
    
    # Train ALL models for comparison
    models = {}
    predictions = {}
    scores = {}
    
    print("  üîó Training ALL comparison models...")
    
    # 1. BASELINE NTL RADIANCE MODEL (Most Basic)
    X_ntl, ntl_feature_names = create_baseline_ntl_model(analyzer)
    if X_ntl is not None:
        X_ntl_train, X_ntl_test, y_train, y_test = train_test_split(
            X_ntl, y, test_size=0.3, random_state=analysis_state.random_state
        )
        ntl_model = RandomForestRegressor(n_estimators=50, max_depth=6, min_samples_split=10,
                                         min_samples_leaf=5, max_features=0.7, random_state=42)
        ntl_model.fit(X_ntl_train, y_train)
        ntl_pred = ntl_model.predict(X_ntl_test)
        ntl_score = r2_score(y_test, ntl_pred)
        models['NTL Radiance (Basic)'] = ntl_model
        predictions['NTL Radiance (Basic)'] = ntl_pred
        scores['NTL Radiance (Basic)'] = ntl_score
        print(f"    ‚Ä¢ NTL Radiance (Basic): R¬≤ = {ntl_score:.4f}")
    else:
        print("    ‚ö†Ô∏è NTL Radiance model not available")
    
    # 2. SEMANTIC + CONTROL VARIABLES MODEL
    X_semantic_control, semantic_feature_names = create_semantic_with_controls_model(analyzer)
    if X_semantic_control is not None:
        X_semantic_train, X_semantic_test, y_train, y_test = train_test_split(
            X_semantic_control, y, test_size=0.3, random_state=analysis_state.random_state
        )
        semantic_model = RandomForestRegressor(
            n_estimators=50, max_depth=6, min_samples_split=10,
            min_samples_leaf=5, max_features=0.7,
            random_state=analysis_state.random_state, n_jobs=-1
        )
        semantic_model.fit(X_semantic_train, y_train)
        semantic_pred = semantic_model.predict(X_semantic_test)
        semantic_score = r2_score(y_test, semantic_pred)
        models['Semantic + Controls'] = semantic_model
        predictions['Semantic + Controls'] = semantic_pred
        scores['Semantic + Controls'] = semantic_score
        print(f"    ‚Ä¢ Semantic + Controls: R¬≤ = {semantic_score:.4f}")
    else:
        print("    ‚ö†Ô∏è Semantic + Controls model not available")
    
    # 3. FULL INTERACTION + CONTROL VARIABLES MODEL (Updated)
    X_full_interactions = analysis_state.interaction_features.get('abd_features')
    feature_names = X_full_interactions.columns.tolist()
    
    X_full_train, X_full_test, y_train, y_test = train_test_split(
        X_full_interactions, y, test_size=0.3, random_state=analysis_state.random_state
    )
    
    full_model = RandomForestRegressor(
        n_estimators=50, max_depth=6, min_samples_split=10,
        min_samples_leaf=5, max_features=0.7, 
        random_state=analysis_state.random_state, n_jobs=-1
    )
    full_model.fit(X_full_train, y_train)
    full_pred = full_model.predict(X_full_test)
    full_score = r2_score(y_test, full_pred)
    models['Full Interaction + Controls'] = full_model
    predictions['Full Interaction + Controls'] = full_pred
    scores['Full Interaction + Controls'] = full_score
    print(f"    ‚Ä¢ Full Interaction + Controls: R¬≤ = {full_score:.4f}")
    
    # 4. Enhanced ensemble prediction
    print("  üéØ Creating enhanced ensemble prediction...")
    weights = np.array(list(scores.values()))
    weights = np.maximum(weights, 0.001)  # Avoid negative weights
    weights = weights / weights.sum()
    ensemble_pred = np.average(list(predictions.values()), weights=weights, axis=0)
    ensemble_score = r2_score(y_test, ensemble_pred)
    
    print(f"  üèÜ Ensemble Score: R¬≤ = {ensemble_score:.4f}")
    print(f"  üìä Model Weights: {dict(zip(scores.keys(), weights))}")
    
    # Calculate improvement over NTL baseline
    if 'NTL Radiance (Basic)' in scores:
        ntl_baseline_score = scores['NTL Radiance (Basic)']
        improvement = ((full_score - ntl_baseline_score) / abs(ntl_baseline_score) * 100) if ntl_baseline_score != 0 else 0
        print(f"  üéØ Full Interaction Model Improvement over NTL Baseline: {improvement:+.1f}%")
    else:
        ntl_baseline_score = 0
        improvement = 0
    
    # Create enhanced ensemble visualization with ALL models
    create_enhanced_ensemble_visualization(perception, models, predictions, scores,
                                         ensemble_pred, ensemble_score, y_test, 
                                         ntl_baseline_score, full_score, save_dir, xgb_result=xgb_result)
    
    return {
        'ensemble_score': ensemble_score,
        'individual_scores': scores,
        'models': models,
        'weights': weights,
        'baseline_score': ntl_baseline_score,
        'full_score': full_score,
        'improvement': improvement
    }

def create_enhanced_ensemble_visualization(perception, models, predictions, scores,
                                         ensemble_pred, ensemble_score, y_test, 
                                         baseline_score, full_score, save_dir, xgb_result=None):
    """Create enhanced ensemble visualization with ALL MODELS + XGBoost - Academic Journal Color Scheme"""
    os.makedirs(save_dir, exist_ok=True)
    
    # üé® ACADEMIC JOURNAL COLOR SCHEME - ÂèÇËÄÉÂ≠¶ÊúØËÆ∫ÊñáÁöÑ‰∏ì‰∏öÈÖçËâ≤
    academic_colors = {
        'ntl_basic': '#4A90E2',        # ÂÜ∑Ëâ≤Ë∞ÉËìùËâ≤ - NTLÂü∫Á°ÄÊ®°Âûã (ÊúÄÁÆÄÂçï)
        'semantic': '#50C878',         # ‰∏≠ÊÄßÁªøËâ≤ - ËØ≠‰πâÊ®°Âûã  
        'full_interaction': '#9B59B6', # ‰ºòÈõÖÁöÑÁ¥´Ëâ≤ - ÂÆåÊï¥‰∫§‰∫íÊ®°Âûã
        'ensemble': '#F39C12',         # Ê∏©ÊöñÁöÑÊ©ôËâ≤ - ÈõÜÊàêÊ®°Âûã
        'xgboost': '#E74C3C',         # ÊúÄÈÜíÁõÆÁöÑÊ©ôÁ∫¢Ëâ≤ - XGBoost (ÊúÄÂêé„ÄÅÊúÄÂ•Ω)
        'perfect': '#34495E',          # Ê∑±ÁÅ∞Ëâ≤ - ÂÆåÁæéÈ¢ÑÊµãÁ∫ø
        'confidence': '#ECF0F1',       # ÊµÖÁÅ∞Ëâ≤ - ÁΩÆ‰ø°Âå∫Èó¥
        'grid': '#F8F9FA',            # ÊûÅÊµÖÁÅ∞Ëâ≤ - ÁΩëÊ†º
        'text': '#2C3E50',            # Ê∑±ËìùÁÅ∞Ëâ≤ - ÊñáÂ≠ó
    }
    
    # üîß Â¶ÇÊûúÊúâXGBoostÁªìÊûúÔºåÂä†ÂÖ•Âà∞Ê®°Âûã‰∏≠ - ‰ΩÜË¶ÅË∞ÉÊï¥È°∫Â∫èÔºåXGBoostÊîæÂú®ÊúÄÂêé
    if xgb_result is not None:
        # ÈúÄË¶ÅÁîüÊàêXGBoostÂú®Áõ∏ÂêåÊµãËØïÈõÜ‰∏äÁöÑÈ¢ÑÊµã
        X_interactions = analysis_state.interaction_features.get('abd_features')
        if X_interactions is not None:
            from sklearn.model_selection import train_test_split
            y_dummy = np.log(np.random.rand(len(X_interactions)) + 1)  # Âç†‰ΩçÁ¨¶
            _, X_test_xgb, _, _ = train_test_split(X_interactions, y_dummy, test_size=0.3, random_state=42)
            xgb_pred = xgb_result['model'].predict(X_test_xgb)
            predictions['XGBoost'] = xgb_pred
        scores['XGBoost'] = xgb_result['test_score']
        print(f"  üîç Added XGBoost model: R¬≤ = {xgb_result['test_score']:.4f}")
    
    # üéØ ÊåâÂ§çÊùÇÂ∫¶ÂíåÊÄßËÉΩÊéíÂ∫èÔºöBasic ‚Üí Semantic ‚Üí Full ‚Üí Ensemble ‚Üí XGBoost(ÊúÄÂêé)
    ordered_models = ['NTL Radiance (Basic)', 'Semantic + Controls', 'Full Interaction + Controls', 'Ensemble']
    if 'XGBoost' in scores:
        ordered_models.append('XGBoost')
    
    # ËÆæÁΩÆÂõæÂΩ¢Ê†∑Âºè - Â≠¶ÊúØËÆ∫ÊñáÁ∫ßÂà´
    plt.style.use('default')
    plt.rcParams.update({
        'font.family': ['Arial', 'DejaVu Sans'],
        'font.size': 11,
        'axes.linewidth': 0.8,
        'axes.edgecolor': academic_colors['text'],
        'axes.facecolor': 'white',
        'figure.facecolor': 'white',
        'grid.alpha': 0.3,
        'grid.color': academic_colors['grid'],
        'text.color': academic_colors['text'],
    })
    
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # üìä 1. Enhanced Model Performance Comparison - Academic Color Scheme
    model_names = []
    model_scores = []
    
    # ÊåâÁÖßordered_modelsÈ°∫Â∫èÊ∑ªÂä†Ê®°Âûã
    for model_name in ordered_models:
        if model_name == 'Ensemble':
            model_names.append(model_name)
            model_scores.append(ensemble_score)
        elif model_name in scores:
            model_names.append(model_name)
            model_scores.append(scores[model_name])
    
    # Find best model for highlighting
    best_idx = np.argmax(model_scores)
    
    # üé® ACADEMIC COLOR MAPPING - ÊåâÂÜ∑ÊöñËâ≤Ë∞ÉÊéíÂ∫èÔºåXGBoostÊúÄÈÜíÁõÆ
    color_map = {
        'NTL Radiance (Basic)': academic_colors['ntl_basic'],      # ÂÜ∑Ëâ≤Ë∞ÉËìùËâ≤
        'Semantic + Controls': academic_colors['semantic'],        # ‰∏≠ÊÄßÁªøËâ≤
        'Full Interaction + Controls': academic_colors['full_interaction'], # Á¥´Ëâ≤
        'Ensemble': academic_colors['ensemble'],                   # Ê∏©ÊöñÊ©ôËâ≤
        'XGBoost': academic_colors['xgboost']                      # ÊúÄÈÜíÁõÆÊ©ôÁ∫¢Ëâ≤
    }
    colors = [color_map.get(name, academic_colors['semantic']) for name in model_names]
    
    # Enhanced bar chart with error bars and styling
    bars = axes[0,0].bar(model_names, model_scores, color=colors, alpha=0.85, 
                        edgecolor='white', linewidth=1.5, width=0.7)
    
    # Highlight best model
    if best_idx < len(bars):
        bars[best_idx].set_edgecolor(academic_colors['text'])
        bars[best_idx].set_linewidth(3)
    
    axes[0,0].set_ylabel('R¬≤ Score', fontweight='bold')
    axes[0,0].set_title(f'Model Performance Comparison - {perception.title()}\nALL Models with Control Variables', fontweight='bold')
    axes[0,0].grid(True, alpha=0.3, axis='y')
    axes[0,0].tick_params(axis='x', rotation=25)
    
    # Add value labels
    for bar, score in zip(bars, model_scores):
        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(model_scores)*0.02,
                      f'{score:.4f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
    
    # 2. ENHANCED Scatter Plot with fit lines and CONFIDENCE INTERVALS for ALL models
    # üéØ ÊåâÁÖßordered_modelsÈ°∫Â∫èÈáçÊñ∞ÊéíÂàópredictions
    ordered_predictions = {}
    for model_name in ordered_models:
        if model_name == 'Ensemble':
            continue  # EnsembleÂú®ÂêéÈù¢ÂçïÁã¨Â§ÑÁêÜ
        elif model_name in predictions:
            ordered_predictions[model_name] = predictions[model_name]
    
    model_list = list(ordered_predictions.keys())
    best_model_idx = np.argmax([scores[model] for model in model_list])
    
    # üé® ENHANCED COLOR MAPPING for scatter plots - ÊåâÂÜ∑ÊöñËâ≤Ë∞ÉÊéíÂ∫èÔºåXGBoostÊúÄÈÜíÁõÆ
    scatter_colors = {
        'NTL Radiance (Basic)': academic_colors['ntl_basic'],      # ÂÜ∑Ëâ≤Ë∞ÉËìùËâ≤
        'Semantic + Controls': academic_colors['semantic'],        # ‰∏≠ÊÄßÁªøËâ≤
        'Full Interaction + Controls': academic_colors['full_interaction'], # Á¥´Ëâ≤
        'XGBoost': academic_colors['xgboost']                      # ÊúÄÈÜíÁõÆÊ©ôÁ∫¢Ëâ≤
    }
    
    # üî• Â¢ûÂº∫ÁöÑÊï£ÁÇπÂõæÔºöÊ∑ªÂä†R¬≤ÂíåÊñúÁéáÊ†áÊ≥®
    for i, (name, pred) in enumerate(ordered_predictions.items()):
        alpha = 0.8 if i == best_model_idx else 0.65  # Best model most prominent
        size = 50 if i == best_model_idx else 40
        color = scatter_colors.get(name, academic_colors['semantic'])
        
        # ËÆ°ÁÆóÊãüÂêàÁ∫øÁªüËÆ°‰ø°ÊÅØ
        from scipy import stats
        slope, intercept, r_value, p_value, std_err = stats.linregress(y_test, pred)
        r_squared = r_value**2
        
        # Êï£ÁÇπÂõæ - Âä†Âº∫ËæπÊ°ÜÔºåÊääR¬≤ÂíåÊñúÁéáÊîæÂà∞legendÈáå
        axes[0,1].scatter(y_test, pred, alpha=alpha, s=size, 
                         color=color, 
                         label=f'{name} (R¬≤={r_squared:.3f}, Slope={slope:.3f})', 
                         edgecolors='white', linewidth=1.0)
        
        # üéØ Ê∑ªÂä†ÊãüÂêàÁ∫ø
        line_x = np.linspace(y_test.min(), y_test.max(), 100)
        line_y = slope * line_x + intercept
        
        line_alpha = 0.9 if i == best_model_idx else 0.8
        line_width = 4 if i == best_model_idx else 2.8
        
        axes[0,1].plot(line_x, line_y, color=color, 
                      alpha=line_alpha, linewidth=line_width, linestyle='-')
        
        # üéØ Ê∑ªÂä†95%ÁΩÆ‰ø°Âå∫Èó¥ - Êõ¥ÈÄèÊòéÔºåÈÅøÂÖçÈ¢úËâ≤Âè†Âä†Â§™Ëä±
        residuals = pred - (slope * y_test + intercept)
        mse = np.mean(residuals**2)
        ci = 1.96 * np.sqrt(mse)  # 95% confidence interval
        
        ci_alpha = 0.25 if i == best_model_idx else 0.15  # Â§ßÂπÖÈôç‰ΩéÈÄèÊòéÂ∫¶
        axes[0,1].fill_between(line_x, line_y - ci, line_y + ci, 
                              color=color, alpha=ci_alpha)
    
    # Ê∑ªÂä†ÂÆåÁæéÈ¢ÑÊµãÁ∫ø
    min_val, max_val = min(y_test.min(), min([p.min() for p in ordered_predictions.values()])), \
                      max(y_test.max(), max([p.max() for p in ordered_predictions.values()]))
    axes[0,1].plot([min_val, max_val], [min_val, max_val], 
                  color=academic_colors['perfect'], linestyle='--', 
                  alpha=0.9, linewidth=3, label='Perfect Prediction')
    
    axes[0,1].set_xlabel(f'True {perception.title()} Values', fontweight='bold')
    axes[0,1].set_ylabel(f'Predicted {perception.title()} Values', fontweight='bold')
    axes[0,1].set_title('Enhanced Prediction Accuracy with Confidence Intervals', fontweight='bold')
    axes[0,1].legend(fontsize=9, loc='upper left', bbox_to_anchor=(1.05, 1))
    axes[0,1].grid(True, alpha=0.3, color=academic_colors['grid'])
    
    # 3. Model Architecture Comparison with USER COLORS
    if 'NTL Radiance (Basic)' in scores and 'Full Interaction + Controls' in scores:
        comparison_models = ['NTL Radiance\n(Basic)', 'Semantic+Controls', 'Full Interaction\n+Controls']
        comparison_scores = [scores.get('NTL Radiance (Basic)', 0),
                           scores.get('Semantic + Controls', 0),
                           scores.get('Full Interaction + Controls', 0)]
        comparison_colors = [academic_colors['ntl_basic'], academic_colors['semantic'], academic_colors['full_interaction']]
        
        bars = axes[1,0].bar(comparison_models, comparison_scores, 
                            color=comparison_colors, alpha=0.8, 
                            edgecolor='white', linewidth=1)
        
        axes[1,0].set_ylabel('R¬≤ Score', fontweight='bold')
        axes[1,0].set_title('Model Architecture Progression\nBasic ‚Üí Semantic ‚Üí Full Interaction', fontweight='bold')
        axes[1,0].grid(True, alpha=0.3, axis='y')
        
        # Add value labels and improvement
        for bar, score in zip(bars, comparison_scores):
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(comparison_scores)*0.02,
                          f'{score:.4f}', ha='center', va='bottom', fontweight='bold')
        
        # Add improvement annotations
        if comparison_scores[0] > 0:
            improvement_semantic = ((comparison_scores[1] - comparison_scores[0]) / abs(comparison_scores[0]) * 100)
            improvement_full = ((comparison_scores[2] - comparison_scores[0]) / abs(comparison_scores[0]) * 100)
            
            axes[1,0].text(0.5, 0.8, f'Semantic vs NTL: {improvement_semantic:+.1f}%\nFull vs NTL: {improvement_full:+.1f}%', 
                          transform=axes[1,0].transAxes, ha='center', va='center',
                          bbox=dict(boxstyle='round,pad=0.5', facecolor=academic_colors['semantic'], alpha=0.3),
                          fontsize=11, fontweight='bold')
    
    # 4. ENHANCED Ensemble Summary with ACADEMIC COLOR THEME
    xgb_info = f"‚Ä¢ XGBoost: {scores.get('XGBoost', 0):.4f}" if 'XGBoost' in scores else ""
    
    ensemble_text = f"""üéØ ENHANCED ENSEMBLE ANALYSIS
{perception.title()} Perception with ALL Models

üèÜ BEST PERFORMANCE: {model_names[best_idx]}
‚Ä¢ Best R¬≤ Score: {max(model_scores):.4f}

üìä MODEL PROGRESSION (ÂÜ∑‚ÜíÊöñËâ≤Ë∞É):
‚Ä¢ NTL Radiance (Basic): {scores.get('NTL Radiance (Basic)', 0):.4f}
‚Ä¢ Semantic + Controls: {scores.get('Semantic + Controls', 0):.4f}  
‚Ä¢ Full Interaction + Controls: {scores.get('Full Interaction + Controls', 0):.4f}
‚Ä¢ Ensemble: {ensemble_score:.4f}
{xgb_info}

üî¨ CONTROL VARIABLES:
‚Ä¢ AVGIL: Average Illumination
‚Ä¢ spots_area: Light Spots Area  
‚Ä¢ ADCG: Advanced Depth Correlation Grid
‚Ä¢ illumination_uniformity: Illumination Uniformity

üéØ MODEL ARCHITECTURE:
‚Ä¢ NTL: Night-time Light Radiance (DN) only
‚Ä¢ Semantic: A-pixel ratios + 5 control variables
‚Ä¢ Full: A+B+D+AB+AD+BD+ABD + 5 control variables
‚Ä¢ XGBoost: Non-linear tree-based ensemble

‚ú® ENHANCED SCATTER PLOT FEATURES:
‚Ä¢ R¬≤ and Slope annotations on each model
‚Ä¢ Enhanced fit lines with optimal thickness
‚Ä¢ 95% confidence intervals (transparent bands)
‚Ä¢ Color-coded by complexity (cold‚Üíwarm)
‚Ä¢ Perfect prediction reference line

üé® ACADEMIC COLOR SCHEME:
‚Ä¢ Blue: NTL Radiance (Basic) - Coldest
‚Ä¢ Green: Semantic + Controls - Cool
‚Ä¢ Purple: Full Interaction + Controls - Warm
‚Ä¢ Orange: Ensemble - Warmer  
‚Ä¢ Red: XGBoost - Hottest & Most Prominent"""
    
    axes[1,1].text(0.02, 0.98, ensemble_text, transform=axes[1,1].transAxes,
                  verticalalignment='top', fontsize=9, fontfamily='monospace',
                  bbox=dict(boxstyle='round,pad=0.5', facecolor=academic_colors['semantic'], alpha=0.2))
    axes[1,1].axis('off')
    
    fig.suptitle(f'üéØ Enhanced ALL-Models Ensemble Analysis - {perception.title()}\nAcademic Color Scheme with R¬≤ & Slope Annotations', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/module3_ensemble_{perception}.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()  # CLOSE FIGURE TO FREE MEMORY
    # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!

def create_enhanced_seven_line_threshold_analysis(analyzer, save_dir):
    """Create enhanced 7-line nonlinear threshold analysis: A+B+D+AB+AD+BD+ABD with USER'S PURPLE/TEAL THEME"""
    print("\nüéØ MODULE 4: Enhanced 7-Line Nonlinear Threshold Analysis")
    print("="*60)
    print("  üìä Seven-Line Analysis Components:")
    print("     ‚Ä¢ A = Pixel Ratio (Semantic Area %)")
    print("     ‚Ä¢ B = Brightness (Luminance)")
    print("     ‚Ä¢ D = Depth (Distance)")
    print("     ‚Ä¢ AB, AD, BD = Two-way interactions")
    print("     ‚Ä¢ ABD = Three-way interaction (RED HIGHLIGHT)")
    print("  üé® Using USER'S Purple/Teal Color Theme")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # USER'S PURPLE & TEAL COLOR SCHEME
    user_colors = {
        'primary': '#4B0082',     # Deep purple
        'secondary': '#20B2AA',   # Light sea green/teal  
        'accent1': '#6A5ACD',     # Slate blue
        'accent2': '#48D1CC',     # Medium turquoise
        'accent3': '#9370DB',     # Medium purple
        'accent4': '#40E0D0',     # Turquoise
        'neutral1': '#708090',    # Slate gray
        'neutral2': '#2F4F4F',    # Dark slate gray
    }
    
    perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    
    # Use USER-SPECIFIED semantic classes for analysis
    available_semantics = []
    for semantic in USER_SEMANTIC_CLASSES:
        # Check if all required data exists
        A_col = semantic
        B_col = f'{semantic}_brightness'
        D_col = f'{semantic}_depth'
        
        if (A_col in analyzer.merged_data.columns and 
            B_col in analyzer.merged_data.columns and 
            D_col in analyzer.merged_data.columns):
            available_semantics.append(semantic)
            print(f"    ‚úÖ {semantic}: Complete A/B/D data available")
        else:
            print(f"    ‚ö†Ô∏è {semantic}: Missing data, skipping")
    
    print(f"    üìä Analyzing {len(available_semantics)} semantic classes: {available_semantics}")
    
    if len(available_semantics) == 0:
        print("    ‚ùå No semantic classes with complete data found")
        return
    
    # Create analysis for each semantic class
    for semantic_idx, semantic in enumerate(available_semantics):
        try:
            print(f"    üîç Processing semantic {semantic_idx+1}/{len(available_semantics)}: {semantic}")
            
            # Create individual plot for each perception
            for perception_idx, perception in enumerate(perception_cols):
                try:
                    fig, ax = plt.subplots(1, 1, figsize=(14, 10))
                    
                    # Get data with explicit column names
                    A_semantic = analyzer.merged_data[semantic]  # Pixel ratio
                    B_semantic = analyzer.merged_data[f'{semantic}_brightness'] / 255.0  # Brightness (normalized)
                    D_semantic = analyzer.merged_data[f'{semantic}_depth']  # Depth
                    y = np.log(analyzer.merged_data[perception] + 1)  # FIXED: Use 1 instead of 1
                    
                    # Adaptive threshold range based on data distribution
                    max_pixel = A_semantic.quantile(0.95)  # Use 95th percentile to avoid outliers
                    thresholds = np.linspace(0.001, min(max_pixel, 0.3), 15)  # 15 threshold points for smoother curves
                    
                    print(f"      Processing {perception} - {semantic}: {len(thresholds)} thresholds")
                    
                    # Seven interaction effects with EXACT run_optimized_analysis.py COLORS
                    effects_data = {
                        'A (Pixel)': {'values': [], 'color': '#9B7EDE', 'style': '-', 'width': 2},  # Light purple (primary)
                        'B (Brightness)': {'values': [], 'color': '#4ECDC4', 'style': '-', 'width': 2},  # Teal (secondary)
                        'D (Depth)': {'values': [], 'color': '#45B7D1', 'style': '-', 'width': 2},  # Blue accent
                        'AB (Pixel√óBrightness)': {'values': [], 'color': '#96CEB4', 'style': '--', 'width': 2.5},  # Light green (neutral)
                        'AD (Pixel√óDepth)': {'values': [], 'color': '#FECA57', 'style': '--', 'width': 2.5},  # Yellow (warning)
                        'BD (Brightness√óDepth)': {'values': [], 'color': '#FFB3BA', 'style': '--', 'width': 2.5},  # Light pink (info)
                        'ABD (Triple Interaction)': {'values': [], 'color': '#FF0000', 'style': ':', 'width': 4}  # RED for ABD
                    }
                    
                    # Calculate effects for each threshold
                    for threshold in thresholds:
                        mask = A_semantic >= threshold
                        n_samples = mask.sum()
                        
                        if n_samples < 25:  # Need sufficient samples for reliable correlation
                            for effect_name in effects_data.keys():
                                effects_data[effect_name]['values'].append(np.nan)
                            continue
                        
                        # Extract masked data
                        A_masked = A_semantic[mask]
                        B_masked = B_semantic[mask]
                        D_masked = D_semantic[mask]
                        y_masked = y[mask]
                        
                        # Calculate correlations for each effect
                        try:
                            # Main effects - correlation with perception
                            corr_a = pearsonr(A_masked, y_masked)[0] if len(A_masked) > 1 else 0
                            corr_b = pearsonr(B_masked, y_masked)[0] if len(B_masked) > 1 else 0
                            corr_d = pearsonr(D_masked, y_masked)[0] if len(D_masked) > 1 else 0
                            
                            # Two-way interactions
                            AB_interaction = A_masked * B_masked
                            AD_interaction = A_masked * D_masked
                            BD_interaction = B_masked * D_masked
                            
                            corr_ab = pearsonr(AB_interaction, y_masked)[0] if len(AB_interaction) > 1 else 0
                            corr_ad = pearsonr(AD_interaction, y_masked)[0] if len(AD_interaction) > 1 else 0
                            corr_bd = pearsonr(BD_interaction, y_masked)[0] if len(BD_interaction) > 1 else 0
                            
                            # Three-way interaction (ABD) - THE HIGHLIGHT!
                            ABD_interaction = A_masked * B_masked * D_masked
                            corr_abd = pearsonr(ABD_interaction, y_masked)[0] if len(ABD_interaction) > 1 else 0
                            
                            # Store results
                            effects_data['A (Pixel)']['values'].append(corr_a)
                            effects_data['B (Brightness)']['values'].append(corr_b)
                            effects_data['D (Depth)']['values'].append(corr_d)
                            effects_data['AB (Pixel√óBrightness)']['values'].append(corr_ab)
                            effects_data['AD (Pixel√óDepth)']['values'].append(corr_ad)
                            effects_data['BD (Brightness√óDepth)']['values'].append(corr_bd)
                            effects_data['ABD (Triple Interaction)']['values'].append(corr_abd)
                            
                        except Exception as correlation_error:
                            print(f"        Correlation error at threshold {threshold:.3f}: {str(correlation_error)[:30]}")
                            # Fill with zeros on error
                            for effect_name in effects_data.keys():
                                effects_data[effect_name]['values'].append(0)
                    
                    # Plot all seven lines with USER'S ENHANCED STYLING
                    for effect_name, effect_data in effects_data.items():
                        effect_values = effect_data['values']
                        color = effect_data['color']
                        style = effect_data['style']
                        width = effect_data['width']
                        
                        # Special formatting for ABD (Triple Interaction)
                        if 'ABD' in effect_name:
                            ax.plot(thresholds, effect_values, 
                                   linestyle=style, linewidth=width, color=color,
                                   label=effect_name, alpha=0.95, marker='o', markersize=8,
                                   markerfacecolor='white', markeredgecolor=color, markeredgewidth=2)
                        else:
                            ax.plot(thresholds, effect_values, 
                                   linestyle=style, linewidth=width, 
                                   color=color, label=effect_name, alpha=0.85)
                    
                    # Enhanced formatting with USER'S STYLE
                    ax.set_xlabel(f'A_{semantic} Threshold (Pixel Ratio)', fontsize=13, fontweight='bold')
                    ax.set_ylabel(f'Correlation with {perception.title()}', fontsize=13, fontweight='bold')
                    ax.set_title(f'Seven-Line Nonlinear Analysis: {semantic.title()} ‚Üí {perception.title()}\n' +
                               'Purple/Teal Theme | Red ABD shows three-way interaction effects', 
                               fontsize=15, fontweight='bold', pad=20)
                    
                    # Enhanced legend with better positioning
                    legend = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11, 
                                     frameon=True, shadow=True, fancybox=True)
                    legend.get_frame().set_facecolor('white')
                    legend.get_frame().set_alpha(0.95)
                    
                    ax.grid(True, alpha=0.4, linestyle='--', linewidth=0.8)
                    ax.axhline(y=0, color='black', linestyle='-', alpha=0.6, linewidth=1.5)
                    
                    # Add enhanced text annotation for significant ABD effects
                    abd_values = effects_data['ABD (Triple Interaction)']['values']
                    valid_abd = [v for v in abd_values if not np.isnan(v)]
                    if valid_abd:
                        max_abd = max([abs(v) for v in valid_abd], default=0)
                        mean_abd = np.mean([abs(v) for v in valid_abd])
                        
                        if max_abd > 0.15:  # Significant threshold
                            ax.text(0.02, 0.98, f'ABD Effects:\nMax |ABD| = {max_abd:.3f}\nMean |ABD| = {mean_abd:.3f}', 
                                   transform=ax.transAxes, va='top', ha='left',
                                   bbox=dict(boxstyle='round,pad=0.5', facecolor='#FF0000', alpha=0.3, edgecolor='#FF0000'),
                                   fontsize=11, fontweight='bold', color='#8B0000')
                    
                    # Add model information
                    ax.text(0.98, 0.02, f'Model: A+B+D+AB+AD+BD+ABD\nTransform: log(perception + 1)\nSemantic: {semantic.title()}', 
                           transform=ax.transAxes, va='bottom', ha='right',
                           bbox=dict(boxstyle='round,pad=0.4', facecolor=user_colors['accent2'], alpha=0.2),
                           fontsize=9, style='italic')
                    
                    plt.tight_layout()
                    plt.savefig(f'{save_dir}/seven_line_analysis_{semantic}_{perception}.png', 
                               dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()  # CLOSE TO FREE MEMORY
                    
                    print(f"        ‚úÖ {perception} plot saved")
                    
                except Exception as perception_error:
                    print(f"    ‚ö†Ô∏è Error with {semantic}-{perception}: {str(perception_error)[:50]}...")
                    continue
                    
        except Exception as semantic_error:
            print(f"    ‚ö†Ô∏è Error with {semantic}: {str(semantic_error)[:50]}...")
            continue
    
    # Create summary visualization showing best ABD effects across all semantics
    create_abd_summary_visualization(analyzer, available_semantics, save_dir, user_colors)
    
    print("    ‚úÖ Enhanced seven-line threshold analysis completed!")
    print(f"    üìÅ Generated {len(available_semantics) * len(perception_cols)} individual plots")
    return available_semantics

def create_abd_summary_visualization(analyzer, available_semantics, save_dir, user_colors):
    """Create summary visualization of ABD effects across all semantics and perceptions"""
    print("  üìä Creating ABD Summary Visualization...")
    
    perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    
    # Calculate max ABD effects for each semantic-perception combination
    abd_effects_matrix = np.zeros((len(available_semantics), len(perception_cols)))
    
    for i, semantic in enumerate(available_semantics):
        for j, perception in enumerate(perception_cols):
            try:
                # Get data
                A_semantic = analyzer.merged_data[semantic]
                B_semantic = analyzer.merged_data[f'{semantic}_brightness'] / 255.0
                D_semantic = analyzer.merged_data[f'{semantic}_depth']
                y = np.log(analyzer.merged_data[perception] + 1)
                
                # Calculate ABD interaction across different thresholds
                thresholds = np.linspace(0.01, 0.25, 10)
                max_abd_corr = 0
                
                for threshold in thresholds:
                    mask = A_semantic >= threshold
                    if mask.sum() > 20:
                        A_masked = A_semantic[mask]
                        B_masked = B_semantic[mask]
                        D_masked = D_semantic[mask]
                        y_masked = y[mask]
                        
                        ABD_interaction = A_masked * B_masked * D_masked
                        if len(ABD_interaction) > 1:
                            corr_abd = abs(pearsonr(ABD_interaction, y_masked)[0])
                            max_abd_corr = max(max_abd_corr, corr_abd)
                
                abd_effects_matrix[i, j] = max_abd_corr
                
            except Exception:
                abd_effects_matrix[i, j] = 0
    
    # Create heatmap
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # Heatmap of ABD effects
    im = ax1.imshow(abd_effects_matrix, cmap='Reds', aspect='auto', vmin=0, vmax=0.5)
    ax1.set_xticks(range(len(perception_cols)))
    ax1.set_xticklabels([p.title() for p in perception_cols], rotation=45, ha='right')
    ax1.set_yticks(range(len(available_semantics)))
    ax1.set_yticklabels([s.title() for s in available_semantics])
    ax1.set_title('Maximum ABD Interaction Effects\n(Triple Interaction: A√óB√óD)', 
                 fontsize=14, fontweight='bold', pad=20)
    
    # Add text annotations
    for i in range(len(available_semantics)):
        for j in range(len(perception_cols)):
            value = abd_effects_matrix[i, j]
            color = 'white' if value > 0.25 else 'black'
            ax1.text(j, i, f'{value:.3f}', ha='center', va='center', 
                    color=color, fontweight='bold')
    
    plt.colorbar(im, ax=ax1, label='Max |Correlation|')
    
    # Bar chart of strongest ABD effects
    flat_effects = abd_effects_matrix.flatten()
    semantic_perception_pairs = [(s, p) for s in available_semantics for p in perception_cols]
    
    # Get top 10 effects
    top_indices = np.argsort(flat_effects)[-10:]
    top_effects = flat_effects[top_indices]
    top_pairs = [semantic_perception_pairs[i] for i in top_indices]
    
    bars = ax2.barh(range(len(top_effects)), top_effects, 
                   color=user_colors['primary'], alpha=0.8, edgecolor='white', linewidth=1)
    ax2.set_yticks(range(len(top_effects)))
    ax2.set_yticklabels([f'{s.title()} ‚Üí {p.title()}' for s, p in top_pairs], fontsize=10)
    ax2.set_xlabel('Max ABD Correlation', fontweight='bold')
    ax2.set_title('Top 10 ABD Triple Interaction Effects\n(Purple Theme)', 
                 fontweight='bold', pad=20)
    ax2.grid(True, alpha=0.3, axis='x')
    
    # Add value labels
    for bar, effect in zip(bars, top_effects):
        ax2.text(effect + 0.005, bar.get_y() + bar.get_height()/2, f'{effect:.3f}',
                ha='left', va='center', fontweight='bold')
    
    fig.suptitle('ABD Triple Interaction Summary - All Semantics & Perceptions\nPurple/Teal Theme', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/abd_summary_analysis.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("    ‚úÖ ABD Summary visualization completed")

def create_enhanced_seven_line_direct_effects_analysis(analyzer, save_dir):
    """Create enhanced 7-line DIRECT EFFECTS threshold analysis: A+B+D+AB+AD+BD+ABD DIRECT impact on perception"""
    print("\nüéØ MODULE 4B: Enhanced 7-Line DIRECT EFFECTS Threshold Analysis")
    print("="*60)
    print("  üìä Seven-Line DIRECT EFFECTS Analysis Components:")
    print("     ‚Ä¢ A = Pixel Ratio ‚Üí Direct impact on perception")
    print("     ‚Ä¢ B = Brightness ‚Üí Direct impact on perception")
    print("     ‚Ä¢ D = Depth ‚Üí Direct impact on perception")
    print("     ‚Ä¢ AB, AD, BD = Two-way interactions ‚Üí Direct impact")
    print("     ‚Ä¢ ABD = Three-way interaction ‚Üí Direct impact (RED HIGHLIGHT)")
    print("  üîç DIFFERENCE FROM CORRELATION ANALYSIS:")
    print("     ‚Ä¢ Previous: Correlation between variable and perception at different thresholds")
    print("     ‚Ä¢ Current: DIRECT MEAN VALUES of perception for different variable levels")
    print("  üé® Using SAME Purple/Teal Color Theme")
    
    os.makedirs(save_dir, exist_ok=True)
    
    # EXACT SAME COLORS as correlation analysis
    user_colors = {
        'primary': '#9B7EDE',     # Light purple (primary)
        'secondary': '#4ECDC4',   # Teal (secondary)
        'accent1': '#45B7D1',     # Blue accent
        'accent2': '#96CEB4',     # Light green (neutral)
        'accent3': '#FECA57',     # Yellow (warning)
        'accent4': '#FFB3BA',     # Light pink (info)
        'neutral1': '#708090',    # Slate gray
        'neutral2': '#2F4F4F',    # Dark slate gray
    }
    
    perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    
    # Use USER-SPECIFIED semantic classes for analysis
    available_semantics = []
    for semantic in USER_SEMANTIC_CLASSES:
        # Check if all required data exists
        A_col = semantic
        B_col = f'{semantic}_brightness'
        D_col = f'{semantic}_depth'
        
        if (A_col in analyzer.merged_data.columns and 
            B_col in analyzer.merged_data.columns and 
            D_col in analyzer.merged_data.columns):
            available_semantics.append(semantic)
            print(f"    ‚úÖ {semantic}: Complete A/B/D data available")
        else:
            print(f"    ‚ö†Ô∏è {semantic}: Missing data, skipping")
    
    print(f"    üìä Analyzing {len(available_semantics)} semantic classes: {available_semantics}")
    
    if len(available_semantics) == 0:
        print("    ‚ùå No semantic classes with complete data found")
        return
    
    # Create analysis for each semantic class
    for semantic_idx, semantic in enumerate(available_semantics):
        try:
            print(f"    üîç Processing semantic {semantic_idx+1}/{len(available_semantics)}: {semantic}")
            
            # Create individual plot for each perception
            for perception_idx, perception in enumerate(perception_cols):
                try:
                    fig, ax = plt.subplots(1, 1, figsize=(14, 10))
                    
                    # Get data with explicit column names
                    A_semantic = analyzer.merged_data[semantic]  # Pixel ratio
                    B_semantic = analyzer.merged_data[f'{semantic}_brightness'] / 255.0  # Brightness (normalized)
                    D_semantic = analyzer.merged_data[f'{semantic}_depth']  # Depth
                    y = np.log(analyzer.merged_data[perception] + 1)  # FIXED: Use 1 instead of 1
                    
                    # Adaptive threshold range based on data distribution
                    max_pixel = A_semantic.quantile(0.95)  # Use 95th percentile to avoid outliers
                    thresholds = np.linspace(0.001, min(max_pixel, 0.3), 15)  # 15 threshold points for smoother curves
                    
                    print(f"      Processing {perception} - {semantic}: {len(thresholds)} thresholds (DIRECT EFFECTS)")
                    
                    # Seven interaction effects with EXACT SAME COLORS as correlation analysis
                    effects_data = {
                        'A (Pixel)': {'values': [], 'color': '#9B7EDE', 'style': '-', 'width': 2},  # Light purple (primary)
                        'B (Brightness)': {'values': [], 'color': '#4ECDC4', 'style': '-', 'width': 2},  # Teal (secondary)
                        'D (Depth)': {'values': [], 'color': '#45B7D1', 'style': '-', 'width': 2},  # Blue accent
                        'AB (Pixel√óBrightness)': {'values': [], 'color': '#96CEB4', 'style': '--', 'width': 2.5},  # Light green (neutral)
                        'AD (Pixel√óDepth)': {'values': [], 'color': '#FECA57', 'style': '--', 'width': 2.5},  # Yellow (warning)
                        'BD (Brightness√óDepth)': {'values': [], 'color': '#FFB3BA', 'style': '--', 'width': 2.5},  # Light pink (info)
                        'ABD (Triple Interaction)': {'values': [], 'color': '#FF0000', 'style': ':', 'width': 4}  # RED for ABD
                    }
                    
                    # Calculate DIRECT EFFECTS for each threshold
                    for threshold in thresholds:
                        mask = A_semantic >= threshold
                        n_samples = mask.sum()
                        
                        if n_samples < 25:  # Need sufficient samples for reliable mean calculation
                            for effect_name in effects_data.keys():
                                effects_data[effect_name]['values'].append(np.nan)
                            continue
                        
                        # Extract masked data
                        A_masked = A_semantic[mask]
                        B_masked = B_semantic[mask]
                        D_masked = D_semantic[mask]
                        y_masked = y[mask]
                        
                        # Calculate DIRECT MEAN VALUES for each effect (NOT correlations!)
                        try:
                            # Main effects - mean values of each variable for samples above threshold
                            mean_a = A_masked.mean() if len(A_masked) > 0 else 0
                            mean_b = B_masked.mean() if len(B_masked) > 0 else 0
                            mean_d = D_masked.mean() if len(D_masked) > 0 else 0
                            
                            # Two-way interactions - mean interaction values
                            AB_interaction = A_masked * B_masked
                            AD_interaction = A_masked * D_masked
                            BD_interaction = B_masked * D_masked
                            
                            mean_ab = AB_interaction.mean() if len(AB_interaction) > 0 else 0
                            mean_ad = AD_interaction.mean() if len(AD_interaction) > 0 else 0
                            mean_bd = BD_interaction.mean() if len(BD_interaction) > 0 else 0
                            
                            # Three-way interaction (ABD) - mean triple interaction value
                            ABD_interaction = A_masked * B_masked * D_masked
                            mean_abd = ABD_interaction.mean() if len(ABD_interaction) > 0 else 0
                            
                            # Store results - DIRECT VALUES, not correlations
                            effects_data['A (Pixel)']['values'].append(mean_a)
                            effects_data['B (Brightness)']['values'].append(mean_b)
                            effects_data['D (Depth)']['values'].append(mean_d)
                            effects_data['AB (Pixel√óBrightness)']['values'].append(mean_ab)
                            effects_data['AD (Pixel√óDepth)']['values'].append(mean_ad)
                            effects_data['BD (Brightness√óDepth)']['values'].append(mean_bd)
                            effects_data['ABD (Triple Interaction)']['values'].append(mean_abd)
                            
                        except Exception as calculation_error:
                            print(f"        Calculation error at threshold {threshold:.3f}: {str(calculation_error)[:30]}")
                            # Fill with zeros on error
                            for effect_name in effects_data.keys():
                                effects_data[effect_name]['values'].append(0)
                    
                    # Plot all seven lines with SAME ENHANCED STYLING
                    for effect_name, effect_data in effects_data.items():
                        effect_values = effect_data['values']
                        color = effect_data['color']
                        style = effect_data['style']
                        width = effect_data['width']
                        
                        # Special formatting for ABD (Triple Interaction)
                        if 'ABD' in effect_name:
                            ax.plot(thresholds, effect_values, 
                                   linestyle=style, linewidth=width, color=color,
                                   label=effect_name, alpha=0.95, marker='o', markersize=8,
                                   markerfacecolor='white', markeredgecolor=color, markeredgewidth=2)
                        else:
                            ax.plot(thresholds, effect_values, 
                                   linestyle=style, linewidth=width, 
                                   color=color, label=effect_name, alpha=0.85)
                    
                    # Enhanced formatting with CLEAR TITLE indicating DIRECT EFFECTS
                    ax.set_xlabel(f'A_{semantic} Threshold (Pixel Ratio)', fontsize=13, fontweight='bold')
                    ax.set_ylabel(f'Mean Variable Values (Direct Effects)', fontsize=13, fontweight='bold')
                    ax.set_title(f'Seven-Line DIRECT EFFECTS Analysis: {semantic.title()} ‚Üí {perception.title()}\n' +
                               'Shows MEAN VALUES of variables (not correlations) | Red ABD = triple interaction', 
                               fontsize=15, fontweight='bold', pad=20)
                    
                    # Enhanced legend with better positioning
                    legend = ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=11, 
                                     frameon=True, shadow=True, fancybox=True)
                    legend.get_frame().set_facecolor('white')
                    legend.get_frame().set_alpha(0.95)
                    
                    ax.grid(True, alpha=0.4, linestyle='--', linewidth=0.8)
                    ax.axhline(y=0, color='black', linestyle='-', alpha=0.6, linewidth=1.5)
                    
                    # Add enhanced text annotation for significant ABD effects
                    abd_values = effects_data['ABD (Triple Interaction)']['values']
                    valid_abd = [v for v in abd_values if not np.isnan(v)]
                    if valid_abd:
                        max_abd = max([abs(v) for v in valid_abd], default=0)
                        mean_abd = np.mean([abs(v) for v in valid_abd])
                        
                        if max_abd > 0.001:  # Lower threshold for direct effects
                            ax.text(0.02, 0.98, f'ABD Direct Effects:\nMax |ABD| = {max_abd:.5f}\nMean |ABD| = {mean_abd:.5f}', 
                                   transform=ax.transAxes, va='top', ha='left',
                                   bbox=dict(boxstyle='round,pad=0.5', facecolor='#FF0000', alpha=0.3, edgecolor='#FF0000'),
                                   fontsize=11, fontweight='bold', color='#8B0000')
                    
                    # Add model information
                    ax.text(0.98, 0.02, f'Analysis: DIRECT EFFECTS (Mean Values)\nModel: A+B+D+AB+AD+BD+ABD\nSemantic: {semantic.title()}', 
                           transform=ax.transAxes, va='bottom', ha='right',
                           bbox=dict(boxstyle='round,pad=0.4', facecolor=user_colors['accent2'], alpha=0.2),
                           fontsize=9, style='italic')
                    
                    plt.tight_layout()
                    plt.savefig(f'{save_dir}/seven_line_direct_effects_{semantic}_{perception}.png', 
                               dpi=300, bbox_inches='tight', facecolor='white')
                    plt.close()  # CLOSE TO FREE MEMORY
                    
                    print(f"        ‚úÖ {perception} DIRECT EFFECTS plot saved")
                    
                except Exception as perception_error:
                    print(f"    ‚ö†Ô∏è Error with {semantic}-{perception}: {str(perception_error)[:50]}...")
                    continue
                    
        except Exception as semantic_error:
            print(f"    ‚ö†Ô∏è Error with {semantic}: {str(semantic_error)[:50]}...")
            continue
    
    # Create summary visualization showing direct effects patterns
    create_direct_effects_summary_visualization(analyzer, available_semantics, save_dir, user_colors)
    
    print("    ‚úÖ Enhanced seven-line DIRECT EFFECTS threshold analysis completed!")
    print(f"    üìÅ Generated {len(available_semantics) * len(perception_cols)} DIRECT EFFECTS plots")
    return available_semantics

def create_direct_effects_summary_visualization(analyzer, available_semantics, save_dir, user_colors):
    """Create summary visualization of DIRECT EFFECTS patterns across all semantics and perceptions"""
    print("  üìä Creating DIRECT EFFECTS Summary Visualization...")
    
    perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    
    # Calculate max DIRECT ABD effects for each semantic-perception combination
    abd_direct_effects_matrix = np.zeros((len(available_semantics), len(perception_cols)))
    
    for i, semantic in enumerate(available_semantics):
        for j, perception in enumerate(perception_cols):
            try:
                # Get data
                A_semantic = analyzer.merged_data[semantic]
                B_semantic = analyzer.merged_data[f'{semantic}_brightness'] / 255.0
                D_semantic = analyzer.merged_data[f'{semantic}_depth']
                y = np.log(analyzer.merged_data[perception] + 1)
                
                # Calculate ABD DIRECT EFFECTS across different thresholds
                thresholds = np.linspace(0.01, 0.25, 10)
                max_abd_direct = 0
                
                for threshold in thresholds:
                    mask = A_semantic >= threshold
                    if mask.sum() > 20:
                        A_masked = A_semantic[mask]
                        B_masked = B_semantic[mask]
                        D_masked = D_semantic[mask]
                        
                        ABD_interaction = A_masked * B_masked * D_masked
                        if len(ABD_interaction) > 0:
                            mean_abd_direct = abs(ABD_interaction.mean())
                            max_abd_direct = max(max_abd_direct, mean_abd_direct)
                
                abd_direct_effects_matrix[i, j] = max_abd_direct
                
            except Exception:
                abd_direct_effects_matrix[i, j] = 0
    
    # Create heatmap
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
    
    # Heatmap of ABD DIRECT effects
    im = ax1.imshow(abd_direct_effects_matrix, cmap='Reds', aspect='auto', vmin=0, vmax=np.max(abd_direct_effects_matrix))
    ax1.set_xticks(range(len(perception_cols)))
    ax1.set_xticklabels([p.title() for p in perception_cols], rotation=45, ha='right')
    ax1.set_yticks(range(len(available_semantics)))
    ax1.set_yticklabels([s.title() for s in available_semantics])
    ax1.set_title('Maximum ABD DIRECT EFFECTS\n(Triple Interaction: A√óB√óD Mean Values)', 
                 fontsize=14, fontweight='bold', pad=20)
    
    # Add text annotations
    for i in range(len(available_semantics)):
        for j in range(len(perception_cols)):
            value = abd_direct_effects_matrix[i, j]
            color = 'white' if value > np.max(abd_direct_effects_matrix) * 0.6 else 'black'
            ax1.text(j, i, f'{value:.5f}', ha='center', va='center', 
                    color=color, fontweight='bold', fontsize=8)
    
    plt.colorbar(im, ax=ax1, label='Max |Direct ABD Effect|')
    
    # Bar chart of strongest ABD DIRECT effects
    flat_effects = abd_direct_effects_matrix.flatten()
    semantic_perception_pairs = [(s, p) for s in available_semantics for p in perception_cols]
    
    # Get top 10 effects
    top_indices = np.argsort(flat_effects)[-10:]
    top_effects = flat_effects[top_indices]
    top_pairs = [semantic_perception_pairs[i] for i in top_indices]
    
    bars = ax2.barh(range(len(top_effects)), top_effects, 
                   color=user_colors['primary'], alpha=0.8, edgecolor='white', linewidth=1)
    ax2.set_yticks(range(len(top_effects)))
    ax2.set_yticklabels([f'{s.title()} ‚Üí {p.title()}' for s, p in top_pairs], fontsize=10)
    ax2.set_xlabel('Max ABD Direct Effect', fontweight='bold')
    ax2.set_title('Top 10 ABD DIRECT EFFECTS\n(Purple Theme)', 
                 fontweight='bold', pad=20)
    ax2.grid(True, alpha=0.3, axis='x')
    
    # Add value labels
    for bar, effect in zip(bars, top_effects):
        ax2.text(effect + effect*0.05, bar.get_y() + bar.get_height()/2, f'{effect:.5f}',
                ha='left', va='center', fontweight='bold', fontsize=9)
    
    fig.suptitle('ABD Triple Interaction DIRECT EFFECTS Summary\nMean Values (Not Correlations) | Purple/Teal Theme', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/abd_direct_effects_summary.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("    ‚úÖ ABD DIRECT EFFECTS Summary visualization completed")

def create_comprehensive_multi_model_analysis(analyzer, perception, save_dir):
    """Create comprehensive analysis showing ALL models with confidence intervals and slopes"""
    print(f"  üìä Creating Comprehensive Multi-Model Analysis for {perception}...")
    
    # Create all models
    models = {}
    predictions = {}
    scores = {}
    slopes = {}
    
    # USER'S PURPLE & TEAL COLOR SCHEME for ALL models
    user_colors = {
        'ntl': '#FF6B6B',           # Coral for NTL radiance
        'semantic': '#4ECDC4',      # Teal for semantic
        'full': '#4B0082',          # Deep purple for full model
        'ridge': '#20B2AA',         # Sea green for ridge
        'accent1': '#6A5ACD',       # Slate blue
        'accent2': '#48D1CC',       # Medium turquoise
    }
    
    # FIXED: Use epsilon=1 instead of 1 for log transformation
    y = np.log(analyzer.merged_data[perception] + 0.01)
    
    # 1. NTL Radiance Model
    X_ntl, ntl_feature_names = create_baseline_ntl_model(analyzer)
    if X_ntl is not None:
        X_ntl_train, X_ntl_test, y_train, y_test = train_test_split(
            X_ntl, y, test_size=0.3, random_state=analysis_state.random_state
        )
        ntl_model = RandomForestRegressor(n_estimators=100, max_depth=8, random_state=42)
        ntl_model.fit(X_ntl_train, y_train)
        ntl_pred = ntl_model.predict(X_ntl_test)
        ntl_score = r2_score(y_test, ntl_pred)
        
        # Calculate slope
        from scipy import stats
        slope, intercept, r_value, p_value, std_err = stats.linregress(y_test, ntl_pred)
        
        models['NTL Radiance'] = ntl_model
        predictions['NTL Radiance'] = ntl_pred
        scores['NTL Radiance'] = ntl_score
        slopes['NTL Radiance'] = slope
        
        print(f"    ‚úÖ NTL Radiance: R¬≤ = {ntl_score:.4f}, Slope = {slope:.4f}")
    
    # 2. Semantic + Controls Model
    X_semantic, semantic_feature_names = create_semantic_with_controls_model(analyzer)
    if X_semantic is not None:
        X_sem_train, X_sem_test, y_train, y_test = train_test_split(
            X_semantic, y, test_size=0.3, random_state=analysis_state.random_state
        )
        semantic_model = RandomForestRegressor(n_estimators=50, max_depth=6, min_samples_split=10,
                                             min_samples_leaf=5, max_features=0.7, random_state=42)
        semantic_model.fit(X_sem_train, y_train)
        semantic_pred = semantic_model.predict(X_sem_test)
        semantic_score = r2_score(y_test, semantic_pred)
        
        # Calculate slope
        slope, intercept, r_value, p_value, std_err = stats.linregress(y_test, semantic_pred)
        
        models['Semantic + Controls'] = semantic_model
        predictions['Semantic + Controls'] = semantic_pred
        scores['Semantic + Controls'] = semantic_score
        slopes['Semantic + Controls'] = slope
        
        print(f"    ‚úÖ Semantic + Controls: R¬≤ = {semantic_score:.4f}, Slope = {slope:.4f}")
    
    # 3. Full Interaction + Controls Model  
    X_full = analysis_state.interaction_features.get('abd_features')
    if X_full is not None:
        X_full_train, X_full_test, y_train, y_test = train_test_split(
            X_full, y, test_size=0.3, random_state=analysis_state.random_state
        )
        full_model = RandomForestRegressor(n_estimators=50, max_depth=6, min_samples_split=10,
                                         min_samples_leaf=5, max_features=0.7, random_state=42)
        full_model.fit(X_full_train, y_train)
        full_pred = full_model.predict(X_full_test)
        full_score = r2_score(y_test, full_pred)
        
        # Calculate slope
        slope, intercept, r_value, p_value, std_err = stats.linregress(y_test, full_pred)
        
        models['Full Interaction + Controls'] = full_model
        predictions['Full Interaction + Controls'] = full_pred
        scores['Full Interaction + Controls'] = full_score
        slopes['Full Interaction + Controls'] = slope
        
        print(f"    ‚úÖ Full Interaction + Controls: R¬≤ = {full_score:.4f}, Slope = {slope:.4f}")
    
    # Create visualization
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # 1. Model Performance Comparison
    model_names = list(scores.keys())
    model_scores = list(scores.values())
    model_slopes = list(slopes.values())
    
    colors = [user_colors['ntl'], user_colors['semantic'], user_colors['full']][:len(model_names)]
    
    bars = axes[0,0].bar(model_names, model_scores, color=colors, alpha=0.8, 
                        edgecolor='white', linewidth=1)
    
    axes[0,0].set_ylabel('R¬≤ Score', fontweight='bold')
    axes[0,0].set_title(f'Model Performance: {perception.title()}\nAll Models with Control Variables', fontweight='bold')
    axes[0,0].grid(True, alpha=0.3, axis='y')
    axes[0,0].tick_params(axis='x', rotation=15)
    
    # Add R¬≤ and slope labels
    for i, (bar, score, slope) in enumerate(zip(bars, model_scores, model_slopes)):
        axes[0,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(model_scores)*0.02,
                      f'R¬≤={score:.4f}\nSlope={slope:.3f}', ha='center', va='bottom', 
                      fontweight='bold', fontsize=9)
    
    # 2. Scatter Plot with Confidence Intervals
    for i, (name, pred) in enumerate(predictions.items()):
        color = colors[i]
        score = scores[name]
        slope = slopes[name]
        
        axes[0,1].scatter(y_test, pred, alpha=0.7, s=30, color=color, 
                         label=f'{name} (R¬≤={score:.3f}, Slope={slope:.3f})', 
                         edgecolors='white', linewidth=0.3)
        
        # Add fit line
        from scipy import stats
        slope_val, intercept, r_value, p_value, std_err = stats.linregress(y_test, pred)
        line_x = np.linspace(y_test.min(), y_test.max(), 100)
        line_y = slope_val * line_x + intercept
        
        axes[0,1].plot(line_x, line_y, color=color, alpha=0.8, linewidth=2.5)
        
        # Add confidence interval
        residuals = pred - (slope_val * y_test + intercept)
        mse = np.mean(residuals**2)
        ci = 1.96 * np.sqrt(mse)
        
        axes[0,1].fill_between(line_x, line_y - ci, line_y + ci, 
                              color=color, alpha=0.2)
    
    # Perfect prediction line
    min_val = min(y_test.min(), min([p.min() for p in predictions.values()]))
    max_val = max(y_test.max(), max([p.max() for p in predictions.values()]))
    axes[0,1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.8, linewidth=3)
    
    axes[0,1].set_xlabel(f'True {perception.title()} Values', fontweight='bold')
    axes[0,1].set_ylabel(f'Predicted {perception.title()} Values', fontweight='bold')
    axes[0,1].set_title('Model Predictions with Confidence Intervals', fontweight='bold')
    axes[0,1].legend(fontsize=9)
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. Slope Comparison
    bars = axes[1,0].bar(model_names, model_slopes, color=colors, alpha=0.8, 
                        edgecolor='white', linewidth=1)
    
    axes[1,0].set_ylabel('Regression Slope', fontweight='bold')
    axes[1,0].set_title('Model Fit Quality (Slope Analysis)', fontweight='bold')
    axes[1,0].grid(True, alpha=0.3, axis='y')
    axes[1,0].tick_params(axis='x', rotation=15)
    axes[1,0].axhline(y=1.0, color='black', linestyle='--', alpha=0.7, label='Perfect Slope')
    
    # Add slope labels
    for bar, slope in zip(bars, model_slopes):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(model_slopes)*0.02,
                      f'{slope:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # 4. Model Summary
    summary_text = f"""COMPREHENSIVE MODEL ANALYSIS
{perception.title()} Perception

üìä MODEL ARCHITECTURE PROGRESSION:
‚Ä¢ NTL Radiance (Basic): {scores.get('NTL Radiance', 0):.4f}
‚Ä¢ Semantic + Controls: {scores.get('Semantic + Controls', 0):.4f}  
‚Ä¢ Full Interaction + Controls: {scores.get('Full Interaction + Controls', 0):.4f}

üî¨ CONTROL VARIABLES:
‚Ä¢ AVGIL: Average Illumination
‚Ä¢ spots_area: Light Spots Area  
‚Ä¢ ADCG: Advanced Depth Correlation Grid
‚Ä¢ illumination_uniformity: Illumination Uniformity

‚ú® VISUAL FEATURES:
‚Ä¢ Confidence intervals (95% CI)
‚Ä¢ Fit lines with slopes
‚Ä¢ R¬≤ scores for each model
‚Ä¢ Purple/Teal color scheme

üé® COLOR LEGEND:
‚Ä¢ Coral: NTL Radiance (Basic)
‚Ä¢ Teal: Semantic + Controls
‚Ä¢ Purple: Full Interaction + Controls"""
    
    axes[1,1].text(0.02, 0.98, summary_text, transform=axes[1,1].transAxes,
                  verticalalignment='top', fontsize=9, fontfamily='monospace',
                  bbox=dict(boxstyle='round,pad=0.5', facecolor=user_colors['accent2'], alpha=0.2))
    axes[1,1].axis('off')
    
    fig.suptitle(f'Comprehensive Multi-Model Analysis - {perception.title()}\nAll Models with Control Variables & Confidence Intervals', 
                fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/comprehensive_multi_model_{perception}.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"  ‚úÖ Comprehensive Multi-Model Analysis completed for {perception}")
    
    return {
        'models': models,
        'predictions': predictions,
        'scores': scores,
        'slopes': slopes
    }

def create_fallback_analysis(model, feature_names, perception, test_score, save_dir):
    """Enhanced fallback when SHAP is not available - USER'S PURPLE COLOR"""
    if hasattr(model, 'feature_importances_'):
        fig, axes = plt.subplots(1, 2, figsize=(20, 8))
        
        # USER'S PURPLE & TEAL COLORS
        user_colors = {
            'primary': '#4B0082',     # Deep purple
            'secondary': '#20B2AA',   # Light sea green/teal  
            'accent1': '#6A5ACD',     # Slate blue
            'accent2': '#48D1CC',     # Medium turquoise
            'accent3': '#9370DB',     # Medium purple
            'accent4': '#40E0D0',     # Turquoise
        }
        
        importance = model.feature_importances_
        sorted_idx = np.argsort(importance)[-25:]  # Â¢ûÂä†Âà∞25‰∏™
        
        # FIXED: User's primary purple color instead of blue
        bars = axes[0].barh(range(len(sorted_idx)), importance[sorted_idx], 
                           color=user_colors['primary'], alpha=0.8, edgecolor='white', linewidth=1)
        
        axes[0].set_yticks(range(len(sorted_idx)))
        axes[0].set_yticklabels([feature_names[i].replace('_', ' ')[:25] for i in sorted_idx])
        axes[0].set_xlabel('Feature Importance', fontsize=14, fontweight='bold')
        axes[0].set_title(f'{perception.title()} - Model Feature Importance\nR¬≤ = {test_score:.4f}', 
                         fontsize=16, fontweight='bold')
        axes[0].grid(True, alpha=0.3, axis='x', linestyle='--')
        
        for bar, imp in zip(bars, importance[sorted_idx]):
            if imp > 0.001:
                axes[0].text(imp + imp*0.02, bar.get_y() + bar.get_height()/2, 
                           f'{imp:.4f}', ha='left', va='center', fontsize=9, fontweight='bold')
        
        # Feature category analysis with USER COLORS
        feature_types = {'A_': 0, 'B_': 0, 'D_': 0, 'AB_': 0, 'AD_': 0, 'BD_': 0, 'ABD_': 0}
        for name in feature_names:
            for prefix in feature_types:
                if name.startswith(prefix):
                    feature_types[prefix] += 1
                    break
        
        categories = list(feature_types.keys())
        counts = list(feature_types.values())
        colors_cat = [user_colors['primary'], user_colors['secondary'], user_colors['accent1'], 
                     user_colors['accent2'], user_colors['accent3'], user_colors['accent4'], user_colors['primary']]
        
        bars = axes[1].bar(categories, counts, color=colors_cat, alpha=0.8, edgecolor='white', linewidth=1)
        axes[1].set_ylabel('Number of Features', fontsize=14, fontweight='bold')
        axes[1].set_title('A+B+D+AB+AD+BD+ABD Feature Distribution (Purple/Teal Theme)', fontsize=16, fontweight='bold')
        axes[1].grid(True, alpha=0.3, axis='y', linestyle='--')
        
        for bar, count in zip(bars, counts):
            axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        f'{count}', ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/xgb_feature_importance_{perception}.png', 
                   dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()  # CLOSE FIGURE TO FREE MEMORY
        # plt.show()  # REMOVED - NO MORE POPUP WINDOWS!

# ÈáçÂ§çÁöÑvisualize_lasso_resultsÂáΩÊï∞Â∑≤ÁßªÈô§ÔºåÈÅøÂÖçÁîüÊàê‰∏§Â•óÁõ∏ÂêåÁöÑÂõæ
# ÈáçÂ§çÁöÑcreate_fallback_analysisÂáΩÊï∞‰πüÂ∑≤ÁßªÈô§ÔºåÈÅøÂÖçÁîüÊàê‰∏§Â•óÁõ∏ÂêåÁöÑÂõæ
# ÈáçÂ§çÁöÑcreate_lcz_stability_analysisÂáΩÊï∞‰πüÂ∑≤ÁßªÈô§
    
    # ÁõÆÊ†áLCZÁ±ªÂûã
    target_lcz_types = [1, 2, 3, 4, 9, 11]
    lcz_names = {
        1: 'Compact High-rise',
        2: 'Compact Mid-rise', 
        3: 'Compact Low-rise',
        4: 'Open High-rise',
        9: 'Sparsely Built',
        11: 'Dense Trees'
    }
    
    # Á≠õÈÄâÁõÆÊ†áLCZÊï∞ÊçÆ
    lcz_data = analyzer.merged_data[analyzer.merged_data['LCZ'].isin(target_lcz_types)]
    print(f"üìä LCZÁ≠õÈÄâÂêéÊï∞ÊçÆÈáè: {len(lcz_data)}/{len(analyzer.merged_data)} ({len(lcz_data)/len(analyzer.merged_data)*100:.1f}%)")
    
    # ‰∏∫ÊØè‰∏™LCZÁ±ªÂûãÂàÜÂà´Ë∑ëÂÆåÊï¥ÂàÜÊûê
    all_lcz_results = {}
    
    for lcz_type in target_lcz_types:
        lcz_subset = lcz_data[lcz_data['LCZ'] == lcz_type]
        if len(lcz_subset) < 50:
            print(f"‚ö†Ô∏è LCZ {lcz_type} Ê†∑Êú¨Èáè‰∏çË∂≥ ({len(lcz_subset)})ÔºåË∑≥Ëøá")
            continue
            
        print(f"\n{'='*60}")
        print(f"üèôÔ∏è LCZ {lcz_type} ({lcz_names[lcz_type]}) - ÂÆåÊï¥ÂàÜÊûêÂºÄÂßã")
        print(f"üìä Ê†∑Êú¨Êï∞: {len(lcz_subset)}")
        print("="*60)
        
        # ÂàõÂª∫LCZ‰∏ìÁî®ÂàÜÊûêÂô®
        lcz_analyzer = FixedOptimizedInteractionAnalyzer()
        lcz_analyzer.merged_data = lcz_subset.copy()
        
        # ÂàõÂª∫LCZ‰∏ìÁî®ÁªìÊûúÁõÆÂΩï
        lcz_save_dir = f"{save_dir}/LCZ_{lcz_type}_{lcz_names[lcz_type].replace(' ', '_')}"
        os.makedirs(lcz_save_dir, exist_ok=True)
        
        # ‰∏∫ÊØè‰∏™ÊÑüÁü•Áª¥Â∫¶Ë∑ëÂÆåÊï¥ÂàÜÊûê
        lcz_results = {}
        
        for perception in perception_cols:
            print(f"\nüéØ LCZ {lcz_type} - ÊÑüÁü•Áª¥Â∫¶: {perception.upper()}")
            print("-" * 50)
            
            try:
                # MODULE 1: XGBoost + SHAP
                print(f"üîç MODULE 1: XGBoost + SHAP for LCZ {lcz_type} - {perception}")
                xgb_result = run_enhanced_xgboost_module(lcz_analyzer, perception, lcz_save_dir, libs)
                
                if xgb_result is None:
                    print(f"    ‚ùå XGBoostÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # MODULE 2: Lasso Feature Selection
                print(f"üéØ MODULE 2: Lasso Feature Selection for LCZ {lcz_type} - {perception}")
                lasso_result = run_enhanced_lasso_module(lcz_analyzer, perception, lcz_save_dir)
                
                if lasso_result is None:
                    print(f"    ‚ùå LassoÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # MODULE 3: Ensemble Strategy
                print(f"üîó MODULE 3: Ensemble Strategy for LCZ {lcz_type} - {perception}")
                ensemble_result = run_integrated_ensemble_module(lcz_analyzer, perception, lcz_save_dir, 
                                                               {'xgb': xgb_result, 'lasso': lasso_result})
                
                # ‰øùÂ≠òÁªìÊûú
                lcz_results[perception] = {
                    'xgb': xgb_result,
                    'lasso': lasso_result,
                    'ensemble': ensemble_result
                }
                
                print(f"\nüìä LCZ {lcz_type} - {perception} ÊÄßËÉΩÊÄªÁªì:")
                print(f"  ‚Ä¢ XGBoost: R¬≤ = {xgb_result['test_score']:.4f}")
                print(f"  ‚Ä¢ Lasso: R¬≤ = {lasso_result['lasso_score']:.4f}")
                print(f"  ‚Ä¢ Elastic-Net: R¬≤ = {lasso_result['elastic_score']:.4f}")
                print(f"  ‚Ä¢ Ensemble: R¬≤ = {ensemble_result['ensemble_score']:.4f}")
                
            except Exception as e:
                print(f"    ‚ùå LCZ {lcz_type} - {perception} ÂàÜÊûêÂ§±Ë¥•: {str(e)}")
                continue
        
        # ‰øùÂ≠òËøô‰∏™LCZÁöÑÊâÄÊúâÁªìÊûú
        all_lcz_results[lcz_type] = {
            'name': lcz_names[lcz_type],
            'sample_count': len(lcz_subset),
            'results': lcz_results,
            'save_dir': lcz_save_dir
        }
        
        print(f"\n‚úÖ LCZ {lcz_type} ({lcz_names[lcz_type]}) ÂÆåÊï¥ÂàÜÊûêÂÆåÊàê!")
        print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {lcz_save_dir}")
    
    print(f"\nüéâ LCZÂàÜÂå∫ÂàÜÊûêÂÆåÊàê! ÂÖ±ÂàÜÊûê‰∫Ü {len(all_lcz_results)} ‰∏™LCZÂàÜÂå∫")
    
    # ÁîüÊàêLCZÂàÜÂå∫ÂØπÊØîÊó•Âøó
    try:
        create_lcz_comparison_log(all_lcz_results, save_dir)
    except Exception as e:
        print(f"  ‚ö†Ô∏è LCZÂØπÊØîÊó•ÂøóÁîüÊàêÂ§±Ë¥•: {str(e)}")
    
    return all_lcz_results

def create_model_log_file(model_results, model_name, y_var, save_dir, analysis_type="main", lcz_type=None):
    """ÂàõÂª∫ËØ¶ÁªÜÁöÑÊ®°ÂûãÊó•ÂøóÊñá‰ª∂"""
    from datetime import datetime
    
    # ÂàõÂª∫Êó•ÂøóÁõÆÂΩï
    log_dir = f"{save_dir}/model_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # ÊûÑÂª∫Êó•ÂøóÊñá‰ª∂Âêç
    if lcz_type is not None:
        log_filename = f"model_log_{analysis_type}_LCZ{lcz_type}_{y_var}_{model_name}.txt"
    else:
        log_filename = f"model_log_{analysis_type}_{y_var}_{model_name}.txt"
    
    log_path = f"{log_dir}/{log_filename}"
    
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write(f"MODEL PERFORMANCE LOG - ABD_trip_V2\n")
        f.write("="*80 + "\n\n")
        
        # Âü∫Êú¨‰ø°ÊÅØ
        f.write(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Model: {model_name}\n")
        f.write(f"Target Variable: {y_var}\n")
        f.write(f"Analysis Type: {analysis_type}\n")
        if lcz_type is not None:
            f.write(f"LCZ Type: {lcz_type}\n")
        f.write("\n" + "-"*80 + "\n\n")
        
        # ÊÄßËÉΩÊåáÊ†á
        f.write("PERFORMANCE METRICS:\n")
        f.write("-"*40 + "\n")
        if 'test_score' in model_results:
            f.write(f"Test R¬≤: {model_results['test_score']:.6f}\n")
        if 'lasso_score' in model_results:
            f.write(f"Lasso R¬≤: {model_results['lasso_score']:.6f}\n")
        if 'elastic_score' in model_results:
            f.write(f"Elastic-Net R¬≤: {model_results['elastic_score']:.6f}\n")
        if 'ensemble_score' in model_results:
            f.write(f"Ensemble R¬≤: {model_results['ensemble_score']:.6f}\n")
        f.write("\n")
        
        # ÈÄâ‰∏≠ÁöÑÁâπÂæÅ
        if 'lasso_selected' in model_results:
            f.write(f"LASSO SELECTED FEATURES ({len(model_results['lasso_selected'])}):\n")
            f.write("-"*40 + "\n")
            for i, feat in enumerate(model_results['lasso_selected'][:30]):  # Ââç30‰∏™
                f.write(f"{i+1:3d}. {feat}\n")
            if len(model_results['lasso_selected']) > 30:
                f.write(f"... and {len(model_results['lasso_selected'])-30} more\n")
            f.write("\n")
        
        f.write("="*80 + "\n")
    
    return log_path

def create_comparison_log(all_results, save_dir, analysis_type="main"):
    """ÂàõÂª∫Ê®°ÂûãÂØπÊØîÊó•ÂøóÔºàCSVÊ†ºÂºèÔºå‰æø‰∫éExcelÂàÜÊûêÔºâ"""
    import pandas as pd
    
    log_dir = f"{save_dir}/model_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    # Êî∂ÈõÜÊâÄÊúâÊ®°ÂûãÁªìÊûú
    comparison_data = []
    
    for y_var, results in all_results.items():
        if isinstance(results, dict):
            # XGBoost
            if 'xgb' in results and results['xgb'] is not None:
                comparison_data.append({
                    'Analysis_Type': analysis_type,
                    'Y_Variable': y_var,
                    'Model': 'XGBoost',
                    'R2_Test': results['xgb'].get('test_score', np.nan)
                })
            
            # Lasso
            if 'lasso' in results and results['lasso'] is not None:
                comparison_data.append({
                    'Analysis_Type': analysis_type,
                    'Y_Variable': y_var,
                    'Model': 'Lasso',
                    'R2_Test': results['lasso'].get('lasso_score', np.nan)
                })
                
                comparison_data.append({
                    'Analysis_Type': analysis_type,
                    'Y_Variable': y_var,
                    'Model': 'Elastic-Net',
                    'R2_Test': results['lasso'].get('elastic_score', np.nan)
                })
            
            # Ensemble
            if 'ensemble' in results and results['ensemble'] is not None:
                comparison_data.append({
                    'Analysis_Type': analysis_type,
                    'Y_Variable': y_var,
                    'Model': 'Ensemble',
                    'R2_Test': results['ensemble'].get('ensemble_score', np.nan)
                })
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        csv_path = f"{log_dir}/model_comparison_{analysis_type}.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  ‚úÖ Ê®°ÂûãÂØπÊØîÊó•ÂøóÂ∑≤‰øùÂ≠ò: {csv_path}")
        return csv_path
    
    return None

def create_lcz_comparison_log(all_lcz_results, save_dir):
    """ÂàõÂª∫LCZÂØπÊØîÊó•Âøó"""
    import pandas as pd
    
    log_dir = f"{save_dir}/model_logs"
    os.makedirs(log_dir, exist_ok=True)
    
    comparison_data = []
    
    for lcz_type, lcz_info in all_lcz_results.items():
        for perception, results in lcz_info['results'].items():
            if 'xgb' in results and results['xgb'] is not None:
                comparison_data.append({
                    'LCZ_Type': lcz_type,
                    'LCZ_Name': lcz_info['name'],
                    'Y_Variable': perception,
                    'Model': 'XGBoost',
                    'R2_Test': results['xgb'].get('test_score', np.nan),
                    'N_Samples': lcz_info.get('sample_count', np.nan)
                })
            
            if 'lasso' in results and results['lasso'] is not None:
                comparison_data.append({
                    'LCZ_Type': lcz_type,
                    'LCZ_Name': lcz_info['name'],
                    'Y_Variable': perception,
                    'Model': 'Lasso',
                    'R2_Test': results['lasso'].get('lasso_score', np.nan),
                    'N_Samples': lcz_info.get('sample_count', np.nan)
                })
    
    if comparison_data:
        df = pd.DataFrame(comparison_data)
        csv_path = f"{log_dir}/lcz_model_comparison.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"  ‚úÖ LCZÂØπÊØîÊó•ÂøóÂ∑≤‰øùÂ≠ò: {csv_path}")
        return csv_path
    
    return None

def create_ntl_analysis(analyzer, perception_cols, save_dir, libs):
    """ÂàõÂª∫NTL‰∏ìÈ°πÂàÜÊûê - V2ÁâàÊú¨ÔºöÂè™‰ΩøÁî®ntl_meanÔºà‰∏çÂåÖÂê´ÂÖ∂‰ªñÊéßÂà∂ÂèòÈáèÔºâ"""
    print("\nüåô NTL SPECIALIZED ANALYSIS - V2")
    print("="*80)
    print("üìä ‰ΩøÁî® ntl_mean È¢ÑÊµãÊâÄÊúâÊÑüÁü•Áª¥Â∫¶ÔºàV2ÁâàÊú¨ÔºöÊó†ÂÖ∂‰ªñÊéßÂà∂ÂèòÈáèÔºâ")
    
    # Ê£ÄÊü•ntl_meanÂàóÊòØÂê¶Â≠òÂú®
    if 'ntl_mean' not in analyzer.merged_data.columns:
        print("‚ùå ntl_meanÂàó‰∏çÂ≠òÂú®ÔºåË∑≥ËøáNTLÂàÜÊûê")
        return
    
    # ÂàõÂª∫NTL‰∏ìÁî®ÁªìÊûúÁõÆÂΩï
    ntl_save_dir = f"{save_dir}/NTL_Analysis"
    os.makedirs(ntl_save_dir, exist_ok=True)
    
    # ËøáÊª§Êéântl_mean‰∏∫Á©∫ÁöÑÊï∞ÊçÆ
    ntl_data = analyzer.merged_data[analyzer.merged_data['ntl_mean'].notna()].copy()
    print(f"üìä NTLÊï∞ÊçÆÈáè: {len(ntl_data)}/{len(analyzer.merged_data)} ({len(ntl_data)/len(analyzer.merged_data)*100:.1f}%)")
    print(f"   ËøáÊª§Êéâ {len(analyzer.merged_data) - len(ntl_data)} ‰∏™ntl_meanÁ©∫ÂÄºÊï∞ÊçÆ")
    
    if len(ntl_data) < 100:
        print("‚ùå NTLÊï∞ÊçÆÈáè‰∏çË∂≥ÔºåË∑≥ËøáÂàÜÊûê")
        return
    
    # ÂáÜÂ§áÁâπÂæÅÔºöV2ÁâàÊú¨Âè™‰ΩøÁî®ntl_mean
    feature_cols = ['ntl_mean']
    print(f"‚úÖ ‰ΩøÁî®ÁâπÂæÅ: {', '.join(feature_cols)} (V2ÁâàÊú¨)")
    
    # ‰∏∫ÊØè‰∏™YÂèòÈáèÂª∫Ê®°
    ntl_results = {}
    
    for perception in perception_cols:
        print(f"\n{'='*60}")
        print(f"üåô NTLÂàÜÊûê - {perception.upper()}")
        print("="*60)
        
        try:
            # ÂáÜÂ§áÊï∞ÊçÆ
            X_ntl = ntl_data[feature_cols].fillna(0)
            y = np.log(ntl_data[perception] + 1)
            
            X_train, X_test, y_train, y_test = train_test_split(
                X_ntl, y, test_size=0.3, random_state=42
            )
            
            # XGBoostÊ®°Âûã
            if libs['xgboost'] is not None:
                print(f"  üìà Training XGBoost for {perception}...")
                xgb_model = libs['xgboost'].XGBRegressor(
                    n_estimators=50, max_depth=4, learning_rate=0.1,
                    random_state=42, verbosity=0
                )
                xgb_model.fit(X_train, y_train)
                xgb_score = xgb_model.score(X_test, y_test)
                print(f"    ‚úÖ XGBoost R¬≤ = {xgb_score:.4f}")
                
                # SHAPÂàÜÊûê
                if libs['shap'] is not None:
                    try:
                        print(f"  üîç Creating SHAP visualizations for {perception}...")
                        explainer = libs['shap'].TreeExplainer(xgb_model)
                        X_sample = X_test.iloc[:min(2000, len(X_test))]
                        shap_values = explainer.shap_values(X_sample)
                        
                        # 1. SHAP Beeswarm Plot (Á∫¢ËìùÊ∏êÂèòË¥°ÁåÆÂ∫¶Âõæ)
                        print(f"    üìä Creating SHAP beeswarm plot...")
                        plt.clf()
                        plt.close('all')
                        
                        if hasattr(libs['shap'], 'plots') and hasattr(libs['shap'].plots, 'beeswarm'):
                            try:
                                explanation = libs['shap'].Explanation(
                                    values=shap_values,
                                    base_values=explainer.expected_value,
                                    data=X_sample.values,
                                    feature_names=feature_cols
                                )
                                plt.figure(figsize=(20, 10))
                                libs['shap'].plots.beeswarm(explanation, max_display=20, 
                                                          color_bar_label="Feature Value", show=False)
                                plt.title(f'NTL SHAP Beeswarm - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                         fontweight='bold', pad=20, fontsize=16)
                            except Exception as e:
                                print(f"      ‚ö†Ô∏è Beeswarm with shap.plots failed: {str(e)}")
                                plt.clf()
                                plt.close('all')
                                plt.figure(figsize=(20, 10))
                                libs['shap'].summary_plot(shap_values, X_sample, feature_names=feature_cols,
                                                        max_display=20, show=False)
                                plt.title(f'NTL SHAP Summary - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                         fontweight='bold', pad=20, fontsize=16)
                        else:
                            plt.figure(figsize=(20, 10))
                            libs['shap'].summary_plot(shap_values, X_sample, feature_names=feature_cols,
                                                    max_display=20, show=False)
                            plt.title(f'NTL SHAP Summary - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                     fontweight='bold', pad=20, fontsize=16)
                        
                        plt.tight_layout()
                        plt.savefig(f'{ntl_save_dir}/ntl_shap_beeswarm_{perception}.png',
                                   dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close('all')
                        print(f"      ‚úÖ Beeswarm plot saved")
                        
                        # 2. SHAP DependenceÂõæ - ËìùÁªøÊï£ÁÇπ+Ê©ôÁ∫¢Ëâ≤Êõ≤Á∫ø
                        fig, axes = plt.subplots(1, len(feature_cols), figsize=(8*len(feature_cols), 6))
                        if len(feature_cols) == 1:
                            axes = [axes]
                        
                        for idx, feature in enumerate(feature_cols):
                            ax = axes[idx]
                            
                            # Ëé∑ÂèñÁâπÂæÅÁ¥¢Âºï
                            feat_idx = feature_cols.index(feature)
                            x_vals = X_sample.iloc[:, feat_idx].values
                            y_vals = shap_values[:, feat_idx]
                            
                            # Êï£ÁÇπ - ËìùÁªøËâ≤
                            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color='#20B2AA', edgecolor='none')
                            
                            # ÊãüÂêàÊõ≤Á∫ø - Ê©ôÁ∫¢Ëâ≤
                            try:
                                from scipy.interpolate import UnivariateSpline
                                df = pd.DataFrame({'x': x_vals, 'y': y_vals}).dropna().sort_values('x')
                                if len(df) > 5 and df['x'].nunique() >= 5:
                                    xs = np.linspace(df['x'].quantile(0.01), df['x'].quantile(0.99), 200)
                                    s_val = max(1e-6, len(df) * np.var(df['y']) * 0.5)
                                    spline = UnivariateSpline(df['x'].values, df['y'].values, s=s_val)
                                    ys = spline(xs)
                                    
                                    # ÊãüÂêàÊõ≤Á∫ø
                                    ax.plot(xs, ys, color='#E24A33', linewidth=2.5, label='Smoothed Trend')
                                    
                                    # BootstrapÁΩÆ‰ø°Âå∫Èó¥
                                    rng = np.random.RandomState(42)
                                    n_boot = 100
                                    boot = []
                                    for _ in range(n_boot):
                                        idx_boot = rng.randint(0, len(df), len(df))
                                        try:
                                            sp_boot = UnivariateSpline(df['x'].values[idx_boot], 
                                                                       df['y'].values[idx_boot], s=s_val)
                                            boot.append(sp_boot(xs))
                                        except:
                                            continue
                                    
                                    if boot:
                                        boot = np.vstack(boot)
                                        lower = np.percentile(boot, 2.5, axis=0)
                                        upper = np.percentile(boot, 97.5, axis=0)
                                        ax.fill_between(xs, lower, upper, color='#E24A33', 
                                                       alpha=0.15, linewidth=0, label='95% CI')
                            except Exception as e:
                                print(f"      ‚ö†Ô∏è Êõ≤Á∫øÊãüÂêàÂ§±Ë¥•: {str(e)}")
                            
                            ax.set_xlabel(f'{feature}', fontweight='bold', fontsize=12)
                            ax.set_ylabel('SHAP value', fontweight='bold', fontsize=12)
                            ax.set_title(f'NTL Impact on {perception.title()}', fontweight='bold', fontsize=14)
                            ax.grid(True, alpha=0.3)
                            ax.legend(fontsize=10)
                        
                        plt.suptitle(f'NTL SHAP Dependence Analysis - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                   fontsize=16, fontweight='bold')
                        plt.tight_layout()
                        plt.savefig(f'{ntl_save_dir}/ntl_shap_dependence_{perception}.png', 
                                   dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close()
                        print(f"    ‚úÖ SHAP dependence plot saved")
                        
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è SHAPÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            
            # LassoÊ®°Âûã
            print(f"  üìà Training Lasso for {perception}...")
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            lasso = LassoCV(alphas=np.logspace(-5, 2, 100), cv=5, random_state=42)
            lasso.fit(X_train_scaled, y_train)
            lasso_score = lasso.score(X_test_scaled, y_test)
            print(f"    ‚úÖ Lasso R¬≤ = {lasso_score:.4f}")
            
            ntl_results[perception] = {
                'xgb_score': xgb_score if libs['xgboost'] is not None else 0,
                'lasso_score': lasso_score,
                'n_samples': len(ntl_data)
            }
            
        except Exception as e:
            print(f"  ‚ùå {perception} NTLÂàÜÊûêÂ§±Ë¥•: {str(e)}")
            continue
    
    # ÂàõÂª∫NTLÁªìÊûúÊ±áÊÄª
    if ntl_results:
        print(f"\nüìä NTLÂàÜÊûêÁªìÊûúÊ±áÊÄª:")
        print(f"{'Perception':<12} {'XGBoost R¬≤':<12} {'Lasso R¬≤':<12}")
        print("-" * 40)
        for perc, res in ntl_results.items():
            print(f"{perc.capitalize():<12} {res['xgb_score']:<12.4f} {res['lasso_score']:<12.4f}")
    
    print(f"\n‚úÖ NTLÊï¥‰ΩìÂàÜÊûêÂÆåÊàêÔºÅ")
    print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {ntl_save_dir}")
    
    return ntl_results

def create_ntl_lcz_analysis(analyzer, perception_cols, save_dir, libs):
    """ÂàõÂª∫NTLÁöÑLCZÂàÜÂå∫ÂàÜÊûê - V2ÁâàÊú¨ÔºöÂè™‰ΩøÁî®ntl_mean"""
    print("\nüåôüèôÔ∏è NTL LCZ PARTITIONED ANALYSIS - V2")
    print("="*80)
    
    # Ê£ÄÊü•ÂøÖË¶ÅÂàó
    if 'ntl_mean' not in analyzer.merged_data.columns:
        print("‚ùå ntl_meanÂàó‰∏çÂ≠òÂú®")
        return
    if 'LCZ' not in analyzer.merged_data.columns:
        print("‚ùå LCZÂàó‰∏çÂ≠òÂú®")
        return
    
    # ËøáÊª§NTLÁ©∫ÂÄºÊï∞ÊçÆ
    ntl_data = analyzer.merged_data[analyzer.merged_data['ntl_mean'].notna()].copy()
    print(f"üìä NTLÊúâÊïàÊï∞ÊçÆ: {len(ntl_data)}/{len(analyzer.merged_data)}")
    
    # LCZÂàÜÂå∫
    target_lcz_types = [1, 2, 3, 4, 9, 11]
    lcz_names = {
        1: 'Compact High-rise', 2: 'Compact Mid-rise', 3: 'Compact Low-rise',
        4: 'Open High-rise', 9: 'Sparsely Built', 11: 'Dense Trees'
    }
    
    # ÂáÜÂ§áÁâπÂæÅÔºöV2ÁâàÊú¨Âè™‰ΩøÁî®ntl_mean
    feature_cols = ['ntl_mean']
    print(f"‚úÖ V2 NTL LCZÂàÜÊûê‰ΩøÁî®ÁâπÂæÅ: {', '.join(feature_cols)}")
    
    lcz_data = ntl_data[ntl_data['LCZ'].isin(target_lcz_types)]
    all_lcz_ntl_results = {}
    
    for lcz_type in target_lcz_types:
        lcz_subset = lcz_data[lcz_data['LCZ'] == lcz_type]
        if len(lcz_subset) < 50:
            print(f"‚ö†Ô∏è LCZ {lcz_type} NTLÊï∞ÊçÆ‰∏çË∂≥ ({len(lcz_subset)})ÔºåË∑≥Ëøá")
            continue
        
        print(f"\n{'='*60}")
        print(f"üåô NTL - LCZ {lcz_type} ({lcz_names[lcz_type]})")
        print(f"üìä Ê†∑Êú¨Êï∞: {len(lcz_subset)}")
        print("="*60)
        
        lcz_ntl_dir = f"{save_dir}/NTL_LCZ_{lcz_type}_{lcz_names[lcz_type].replace(' ', '_')}"
        os.makedirs(lcz_ntl_dir, exist_ok=True)
        
        lcz_results = {}
        
        for perception in perception_cols:
            try:
                X_ntl = lcz_subset[feature_cols].fillna(0)
                y = np.log(lcz_subset[perception] + 1)
                
                X_train, X_test, y_train, y_test = train_test_split(
                    X_ntl, y, test_size=0.3, random_state=42
                )
                
                # XGBoost + SHAP
                if libs['xgboost'] is not None and libs['shap'] is not None:
                    xgb_model = libs['xgboost'].XGBRegressor(
                        n_estimators=50, max_depth=4, learning_rate=0.1,
                        random_state=42, verbosity=0
                    )
                    xgb_model.fit(X_train, y_train)
                    xgb_score = xgb_model.score(X_test, y_test)
                    
                    # SHAP visualizations
                    try:
                        explainer = libs['shap'].TreeExplainer(xgb_model)
                        X_sample = X_test.iloc[:min(1000, len(X_test))]
                        shap_values = explainer.shap_values(X_sample)
                        
                        # 1. Beeswarm plot
                        plt.clf()
                        plt.close('all')
                        if hasattr(libs['shap'], 'plots') and hasattr(libs['shap'].plots, 'beeswarm'):
                            try:
                                explanation = libs['shap'].Explanation(
                                    values=shap_values,
                                    base_values=explainer.expected_value,
                                    data=X_sample.values,
                                    feature_names=feature_cols
                                )
                                plt.figure(figsize=(20, 10))
                                libs['shap'].plots.beeswarm(explanation, max_display=20, show=False)
                                plt.title(f'NTL LCZ {lcz_type} - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                         fontweight='bold', pad=20)
                            except:
                                plt.clf()
                                plt.close('all')
                                plt.figure(figsize=(20, 10))
                                libs['shap'].summary_plot(shap_values, X_sample, feature_names=feature_cols,
                                                        max_display=20, show=False)
                                plt.title(f'NTL LCZ {lcz_type} - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                         fontweight='bold', pad=20)
                        else:
                            plt.figure(figsize=(20, 10))
                            libs['shap'].summary_plot(shap_values, X_sample, feature_names=feature_cols,
                                                    max_display=20, show=False)
                            plt.title(f'NTL LCZ {lcz_type} - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                     fontweight='bold', pad=20)
                        plt.tight_layout()
                        plt.savefig(f'{lcz_ntl_dir}/ntl_shap_beeswarm_{perception}.png',
                                   dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close('all')
                        
                        # 2. Dependence plot
                        fig, axes = plt.subplots(1, len(feature_cols), figsize=(8*len(feature_cols), 6))
                        if len(feature_cols) == 1:
                            axes = [axes]
                        
                        for idx, feature in enumerate(feature_cols):
                            ax = axes[idx]
                            feat_idx = feature_cols.index(feature)
                            x_vals = X_sample.iloc[:, feat_idx].values
                            y_vals = shap_values[:, feat_idx]
                            
                            ax.scatter(x_vals, y_vals, alpha=0.4, s=20, color='#20B2AA', edgecolor='none')
                            
                            # ÊãüÂêàÊõ≤Á∫ø + 95% CI
                            try:
                                from scipy.interpolate import UnivariateSpline
                                df = pd.DataFrame({'x': x_vals, 'y': y_vals}).dropna().sort_values('x')
                                if len(df) > 5 and df['x'].nunique() >= 5:
                                    xs = np.linspace(df['x'].quantile(0.01), df['x'].quantile(0.99), 200)
                                    s_val = max(1e-6, len(df) * np.var(df['y']) * 0.5)
                                    spline = UnivariateSpline(df['x'].values, df['y'].values, s=s_val)
                                    ys = spline(xs)
                                    
                                    # ‰∏ªÊõ≤Á∫ø
                                    ax.plot(xs, ys, color='#E24A33', linewidth=2.5, label='Smoothed Trend')
                                    
                                    # BootstrapÁΩÆ‰ø°Âå∫Èó¥
                                    rng = np.random.RandomState(42)
                                    n_boot = 100
                                    boot = []
                                    for _ in range(n_boot):
                                        idx_boot = rng.randint(0, len(df), len(df))
                                        try:
                                            sp_boot = UnivariateSpline(df['x'].values[idx_boot], 
                                                                       df['y'].values[idx_boot], s=s_val)
                                            boot.append(sp_boot(xs))
                                        except:
                                            continue
                                    
                                    if boot:
                                        boot = np.vstack(boot)
                                        lower = np.percentile(boot, 2.5, axis=0)
                                        upper = np.percentile(boot, 97.5, axis=0)
                                        ax.fill_between(xs, lower, upper, color='#E24A33', 
                                                       alpha=0.15, linewidth=0, label='95% CI')
                            except:
                                pass
                            
                            ax.set_xlabel(f'{feature}', fontweight='bold')
                            ax.set_ylabel('SHAP value', fontweight='bold')
                            ax.set_title(f'LCZ {lcz_type} - {perception.title()}', fontweight='bold')
                            ax.grid(True, alpha=0.3)
                        
                        plt.suptitle(f'NTL SHAP - LCZ {lcz_type} - {perception.title()}\nR¬≤ = {xgb_score:.4f}',
                                   fontsize=14, fontweight='bold')
                        plt.tight_layout()
                        plt.savefig(f'{lcz_ntl_dir}/ntl_shap_{perception}.png', 
                                   dpi=300, bbox_inches='tight', facecolor='white')
                        plt.close()
                    except Exception as e:
                        print(f"    ‚ö†Ô∏è LCZ {lcz_type} {perception} SHAPÂ§±Ë¥•: {str(e)}")
                    
                    lcz_results[perception] = {'xgb_score': xgb_score}
                    print(f"  ‚úÖ LCZ {lcz_type} - {perception}: R¬≤ = {xgb_score:.4f}")
                
            except Exception as e:
                print(f"  ‚ùå LCZ {lcz_type} - {perception} Â§±Ë¥•: {str(e)}")
                continue
        
        all_lcz_ntl_results[lcz_type] = {
            'name': lcz_names[lcz_type],
            'results': lcz_results,
            'n_samples': len(lcz_subset)
        }
    
    print(f"\n‚úÖ NTL LCZÂàÜÂå∫ÂàÜÊûêÂÆåÊàêÔºÅÂÖ±ÂàÜÊûê {len(all_lcz_ntl_results)} ‰∏™LCZÂàÜÂå∫")
    return all_lcz_ntl_results

def plot_data_histograms(analyzer, save_dir):
    """ÁîüÊàêÊâÄÊúâXÂíåYÂèòÈáèÁöÑÊµÖÊ©ôËâ≤Áõ¥ÊñπÂõæ"""
    print("\nüìä ÁîüÊàêÊï∞ÊçÆÂàÜÂ∏ÉÁõ¥ÊñπÂõæ...")
    
    # ÊÑüÁü•ÂèòÈáè (YÂèòÈáè)
    y_vars = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    available_y_vars = [var for var in y_vars if var in analyzer.merged_data.columns]
    
    # ËØ≠‰πâÂèòÈáè (XÂèòÈáè) - ÂèñÂâç20‰∏™‰∏ªË¶ÅÁöÑ
    x_vars = analyzer.semantic_classes[:20] if len(analyzer.semantic_classes) > 20 else analyzer.semantic_classes
    
    # ÂàõÂª∫Áõ¥ÊñπÂõæÁõÆÂΩï
    hist_dir = f"{save_dir}/data_histograms"
    os.makedirs(hist_dir, exist_ok=True)
    
    # ÊµÖÊ©ôËâ≤
    orange_color = '#FFB366'
    
    # ‰∏∫YÂèòÈáèÁîüÊàêÁõ¥ÊñπÂõæ
    if available_y_vars:
        n_y = len(available_y_vars)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, y_var in enumerate(available_y_vars):
            if i < len(axes):
                data = analyzer.merged_data[y_var].dropna()
                axes[i].hist(data, bins=30, color=orange_color, alpha=0.7, edgecolor='white')
                axes[i].set_title(f'{y_var.title()} Distribution', fontweight='bold')
                axes[i].set_xlabel(y_var.title())
                axes[i].set_ylabel('Frequency')
                axes[i].grid(True, alpha=0.3)
        
        # ÈöêËóèÂ§ö‰ΩôÁöÑÂ≠êÂõæ
        for i in range(len(available_y_vars), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{hist_dir}/y_variables_histograms.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ YÂèòÈáèÁõ¥ÊñπÂõæÂ∑≤‰øùÂ≠ò: {len(available_y_vars)} ‰∏™ÂèòÈáè")
    
    # ‰∏∫XÂèòÈáèÁîüÊàêÁõ¥ÊñπÂõæ
    if x_vars:
        n_plots = min(20, len(x_vars))
        n_rows = 4
        n_cols = 5
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 16))
        axes = axes.flatten()
        
        for i, x_var in enumerate(x_vars[:n_plots]):
            if x_var in analyzer.merged_data.columns:
                data = analyzer.merged_data[x_var].dropna()
                axes[i].hist(data, bins=25, color=orange_color, alpha=0.7, edgecolor='white')
                axes[i].set_title(f'{x_var.title()}', fontweight='bold', fontsize=10)
                axes[i].set_xlabel(x_var.replace('_', ' ').title(), fontsize=8)
                axes[i].set_ylabel('Frequency', fontsize=8)
                axes[i].grid(True, alpha=0.3)
                axes[i].tick_params(axis='both', which='major', labelsize=7)
        
        # ÈöêËóèÂ§ö‰ΩôÁöÑÂ≠êÂõæ
        for i in range(n_plots, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{hist_dir}/x_variables_histograms.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ XÂèòÈáèÁõ¥ÊñπÂõæÂ∑≤‰øùÂ≠ò: {n_plots} ‰∏™ÂèòÈáè")
    
    print(f"üìÅ Áõ¥ÊñπÂõæ‰øùÂ≠òÂú®: {hist_dir}")

def main():
    """Enhanced main program with SAFETY-FOCUSED ANALYSIS + LCZ PARTITIONED ANALYSIS"""
    print("üåÉ ENHANCED COMPLETE MODULAR URBAN PERCEPTION ANALYSIS - SAFETY FOCUSED + LCZ PARTITIONED")
    print("="*80)
    print("üîß SAFETY-FOCUSED ANALYSIS WITH NEW VISUALIZATION REQUIREMENTS:")
    print("‚úÖ Focus only on SAFETY perception")
    print("‚úÖ Create 4 scatter plots: 2 linear + 2 nonlinear threshold curves")
    print("‚úÖ Full model comparison: XGBoost vs Lasso on complete features")
    print("‚úÖ Baseline comparison: NTL vs Semantic (both with XGBoost and Lasso)")
    print("‚úÖ Mako+Orange color scheme: ÊµÖÊ©ôËâ≤„ÄÅËìùÁªøËâ≤„ÄÅÈªÑÁªøËâ≤„ÄÅÁÅ∞ËìùÁ¥´Ëâ≤")
    print("‚úÖ Legends outside plot frames, 1:1 aspect ratio maintained")
    print("‚úÖ Nonlinear threshold curves with smooth confidence intervals")
    print("="*80)
    
    # Ê£ÄÊü•Â∫ì
    libs = check_libraries()
    
    # Êï∞ÊçÆÊñá‰ª∂
    pixel_file = '100g/depth_weighted_semantic_results.csv'
    brightness_file = '100g/unified_semantic_brightness_analysis.csv'
    depth_file = '100g/full_semantic_depth_results.csv'
    perceptions_file = '100g/perceptionf.csv'
    
    files_to_check = [pixel_file, brightness_file, depth_file, perceptions_file]
    
    print("\nüìÅ Ê£ÄÊü•Êï∞ÊçÆÊñá‰ª∂...")
    for file in files_to_check:
        if os.path.exists(file):
            print(f"‚úÖ {file}")
        else:
            print(f"‚ùå {file} - Êñá‰ª∂‰∏çÂ≠òÂú®!")
            return
    
    try:
        # ÂàõÂª∫ÂàÜÊûêÂô®Âπ∂Âä†ËΩΩÊï∞ÊçÆ
        analyzer = FixedOptimizedInteractionAnalyzer()
        analyzer.load_data(pixel_file, brightness_file, depth_file, perceptions_file)
        
        print(f"\nüìä Êï∞ÊçÆÂä†ËΩΩÊàêÂäü!")
        print(f"   ÂêàÂπ∂Êï∞ÊçÆÂΩ¢Áä∂: {analyzer.merged_data.shape}")
        print(f"   ÂèØÁî®Âàó: {list(analyzer.merged_data.columns)}")
        
        # ÂàõÂª∫ÁªìÊûúÁõÆÂΩïÔºàÂ∏¶Êó∂Èó¥Êà≥Ôºâ
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        save_dir = f'enhanced_complete_results_v2_{timestamp}'
        os.makedirs(save_dir, exist_ok=True)
        
        # SAFETY-FOCUSED ANALYSIS ONLY
        perception_cols = ['safe']  # Only focus on safety
        
        print(f"\nüéØ Analyzing SAFETY Perception Only: {perception_cols}")
        print(f"üìÅ Results Directory: {save_dir}/")
        
        # ============ PERCEPTION-FOCUSED ANALYSIS WITH 4 SPECIALIZED PLOTS ============
        # ‰∏∫ÊâÄÊúâ6‰∏™ÊÑüÁü•Áª¥Â∫¶ÁîüÊàê‰∏ìÈó®ÁöÑÊï£ÁÇπÂõæÂàÜÊûê
        all_perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
        print(f"\n{'='*80}")
        print("üöÄ RUNNING PERCEPTION-FOCUSED ANALYSIS WITH 4 SPECIALIZED PLOTS")
        print(f"üìä Analyzing all {len(all_perception_cols)} perceptions: {all_perception_cols}")
        print("="*80)
        
        for perception in all_perception_cols:
            print(f"\nüéØ Creating specialized plots for: {perception.upper()}")
            create_safety_focused_analysis(analyzer, save_dir, perception_name=perception)
        
        # ËøêË°åÂÆåÊï¥ÁöÑ‰∏âÊ®°ÂùóÂàÜÊûê
        all_results = {}
        all_model_summary = {}
        
        for perception in perception_cols:
            print(f"\n{'='*60}")
            print(f"üéØ PERCEPTION: {perception.upper()}")
            print("="*60)
            
            try:
                # MODULE 1: XGBoost + SHAP
                print(f"üîç MODULE 1: XGBoost + SHAP for {perception}")
                xgb_result = run_enhanced_xgboost_module(analyzer, perception, save_dir, libs)
                
                if xgb_result is None:
                    print(f"    ‚ùå XGBoostÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # MODULE 2: Lasso Feature Selection
                print(f"üéØ MODULE 2: Lasso Feature Selection for {perception}")
                lasso_result = run_enhanced_lasso_module(analyzer, perception, save_dir)
                
                if lasso_result is None:
                    print(f"    ‚ùå LassoÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # XGBoost vs LassoÂØπÊØîÂàÜÊûê
                X_features, feature_names = create_strict_abd_interactions(analyzer)
                comparison_result = create_xgb_lasso_comparison(
                    xgb_result, lasso_result, feature_names, perception, save_dir
                )
                
                # MODULE 3: Ensemble Strategy
                print(f"üîó MODULE 3: Ensemble Strategy for {perception}")
                ensemble_result = run_integrated_ensemble_module(analyzer, perception, save_dir, 
                                                               {'xgb': xgb_result, 'lasso': lasso_result})
                
                # ‰øùÂ≠òÁªìÊûú
                all_results[perception] = {
                    'xgb': xgb_result,
                    'lasso': lasso_result,
                    'ensemble': ensemble_result
                }
                
                print(f"\nüìä {perception.upper()} ÊÄßËÉΩÊÄªÁªì:")
                print(f"  ‚Ä¢ XGBoost: R¬≤ = {xgb_result['test_score']:.4f}")
                print(f"  ‚Ä¢ Lasso: R¬≤ = {lasso_result['lasso_score']:.4f}")
                print(f"  ‚Ä¢ Elastic-Net: R¬≤ = {lasso_result['elastic_score']:.4f}")
                print(f"  ‚Ä¢ Ensemble: R¬≤ = {ensemble_result['ensemble_score']:.4f}")
                
            except Exception as e:
                print(f"    ‚ùå {perception} ÂàÜÊûêÂ§±Ë¥•: {str(e)}")
                continue
        
        print(f"\nüéâ ‰∏ªÂàÜÊûêÂÆåÊàê!")
        print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {save_dir}")
        
        # ============ LCZÂàÜÂå∫ÂàÜÊûê ============
        print("\n" + "="*80)
        print("üèôÔ∏è ÂºÄÂßãLCZÂàÜÂå∫ÂàÜÊûê")
        print("="*80)
        
        # LCZÂàÜÊûêÈúÄË¶ÅÂàÜÊûêÊâÄÊúâ6‰∏™perceptionÁª¥Â∫¶
        lcz_perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
        print(f"üìä LCZÂàÜÊûêÂ∞ÜÂàÜÊûêÊâÄÊúâ6‰∏™perceptionÁª¥Â∫¶: {lcz_perception_cols}")
        
        lcz_save_dir = f"{save_dir}/LCZ_Analysis"
        os.makedirs(lcz_save_dir, exist_ok=True)
        
        lcz_results = create_lcz_stability_analysis(analyzer, lcz_perception_cols, lcz_save_dir, libs)
        
        if lcz_results:
            print(f"\nüéâ LCZÂàÜÂå∫ÂàÜÊûêÂÆåÊàê! ÂÖ±ÂàÜÊûê‰∫Ü {len(lcz_results)} ‰∏™LCZÂàÜÂå∫")
            print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {lcz_save_dir}")
            try:
                print("üìù ÁîüÊàêLCZÂØπÊØîÊó•Âøó...")
                create_lcz_comparison_log(lcz_results, lcz_save_dir)
            except Exception as log_error:
                print(f"  ‚ö†Ô∏è LCZÊó•ÂøóÁîüÊàêÂ§±Ë¥•: {str(log_error)}")
        else:
            print("\n‚ùå LCZÂàÜÂå∫ÂàÜÊûêÊú™ËÉΩÂÆåÊàê")
        
        # ============ NTL‰∏ìÈ°πÂàÜÊûê ============
        print("\n" + "="*80)
        print("üåô ÂºÄÂßãNTL‰∏ìÈ°πÂàÜÊûê")
        print("="*80)
        
        # NTLÂàÜÊûêÈúÄË¶ÅÂàÜÊûêÊâÄÊúâ6‰∏™perceptionÁª¥Â∫¶
        ntl_perception_cols = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
        print(f"üìä NTLÂàÜÊûêÂ∞ÜÂàÜÊûêÊâÄÊúâ6‰∏™perceptionÁª¥Â∫¶: {ntl_perception_cols}")
        
        ntl_save_dir = f"{save_dir}/NTL_Analysis"
        os.makedirs(ntl_save_dir, exist_ok=True)
        
        ntl_results = create_ntl_analysis(analyzer, ntl_perception_cols, ntl_save_dir, libs)
        ntl_lcz_results = create_ntl_lcz_analysis(analyzer, ntl_perception_cols, ntl_save_dir, libs)
        
        if ntl_results or ntl_lcz_results:
            print(f"\nüéâ NTLÂàÜÊûêÂÆåÊàê!")
            print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {ntl_save_dir}")
        else:
            print("\n‚ùå NTLÂàÜÊûêÊú™ËÉΩÂÆåÊàê")
        
        print(f"\nüéâüéâ ÊâÄÊúâÂàÜÊûêÂ∑≤ÂÆåÊàêÔºÅÊâÄÊúâÁªìÊûú‰øùÂ≠òÂú®: {save_dir}/")
        
    except Exception as e:
        print(f"‚ùå Á®ãÂ∫èÊâßË°åÂ§±Ë¥•: {str(e)}")
        import traceback
        traceback.print_exc()

def plot_data_histograms(analyzer, save_dir):
    """ÁîüÊàêÊâÄÊúâXÂíåYÂèòÈáèÁöÑÊµÖÊ©ôËâ≤Áõ¥ÊñπÂõæ"""
    print("\nüìä ÁîüÊàêÊï∞ÊçÆÂàÜÂ∏ÉÁõ¥ÊñπÂõæ...")
    
    # ÊÑüÁü•ÂèòÈáè (YÂèòÈáè)
    y_vars = ['safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring']
    available_y_vars = [var for var in y_vars if var in analyzer.merged_data.columns]
    
    # ËØ≠‰πâÂèòÈáè (XÂèòÈáè) - ÂèñÂâç20‰∏™‰∏ªË¶ÅÁöÑ
    x_vars = analyzer.semantic_classes[:20] if len(analyzer.semantic_classes) > 20 else analyzer.semantic_classes
    
    # ÂàõÂª∫Áõ¥ÊñπÂõæÁõÆÂΩï
    hist_dir = f"{save_dir}/data_histograms"
    os.makedirs(hist_dir, exist_ok=True)
    
    # ÊµÖÊ©ôËâ≤
    orange_color = '#FFB366'
    
    print(f"üéØ ÂàÜÊûêÁöÑYÂèòÈáè: {available_y_vars}")
    print(f"üéØ ÂàÜÊûêÁöÑXÂèòÈáè (Ââç20‰∏™): {x_vars[:20] if len(x_vars) > 20 else x_vars}")
    
    # ‰∏∫YÂèòÈáèÁîüÊàêÁõ¥ÊñπÂõæ
    if available_y_vars:
        n_y = len(available_y_vars)
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, y_var in enumerate(available_y_vars):
            if i < len(axes):
                data = analyzer.merged_data[y_var].dropna()
                axes[i].hist(data, bins=30, color=orange_color, alpha=0.7, edgecolor='white')
                axes[i].set_title(f'{y_var.title()} Distribution', fontweight='bold')
                axes[i].set_xlabel(y_var.title())
                axes[i].set_ylabel('Frequency')
                axes[i].grid(True, alpha=0.3)
        
        # ÈöêËóèÂ§ö‰ΩôÁöÑÂ≠êÂõæ
        for i in range(len(available_y_vars), len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{hist_dir}/y_variables_histograms.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ YÂèòÈáèÁõ¥ÊñπÂõæÂ∑≤‰øùÂ≠ò: {len(available_y_vars)} ‰∏™ÂèòÈáè")
    
    # ‰∏∫XÂèòÈáèÁîüÊàêÁõ¥ÊñπÂõæ
    if x_vars:
        n_plots = min(20, len(x_vars))
        n_rows = 4
        n_cols = 5
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 16))
        axes = axes.flatten()
        
        for i, x_var in enumerate(x_vars[:n_plots]):
            if x_var in analyzer.merged_data.columns:
                data = analyzer.merged_data[x_var].dropna()
                axes[i].hist(data, bins=25, color=orange_color, alpha=0.7, edgecolor='white')
                axes[i].set_title(f'{x_var.title()}', fontweight='bold', fontsize=10)
                axes[i].set_xlabel(x_var.replace('_', ' ').title(), fontsize=8)
                axes[i].set_ylabel('Frequency', fontsize=8)
                axes[i].grid(True, alpha=0.3)
                axes[i].tick_params(axis='both', which='major', labelsize=7)
        
        # ÈöêËóèÂ§ö‰ΩôÁöÑÂ≠êÂõæ
        for i in range(n_plots, len(axes)):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{hist_dir}/x_variables_histograms.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ XÂèòÈáèÁõ¥ÊñπÂõæÂ∑≤‰øùÂ≠ò: {n_plots} ‰∏™ÂèòÈáè")
    
    print(f"üìÅ Áõ¥ÊñπÂõæ‰øùÂ≠òÂú®: {hist_dir}")

def create_lcz_stability_analysis(analyzer, perception_cols, save_dir, libs):
    """ÂàõÂª∫LCZÁ®≥ÂÆöÊÄßÂàÜÊûê - ÂØπÊØè‰∏™LCZÂàÜÂå∫ÂàÜÂà´Ë∑ëÂÆåÊï¥ÁöÑÂàÜÊûê"""
    print("\nüèôÔ∏è LCZ STABILITY ANALYSIS - ÊØè‰∏™LCZÂàÜÂå∫ÂÆåÊï¥ÂàÜÊûê")
    print("="*80)
    
    # È¶ñÂÖàÁîüÊàêÊï∞ÊçÆÂàÜÂ∏ÉÁõ¥ÊñπÂõæ
    plot_data_histograms(analyzer, save_dir)
    
    # Ê£ÄÊü•LCZÂàóÊòØÂê¶Â≠òÂú®
    if 'LCZ' not in analyzer.merged_data.columns:
        print("‚ùå Êú™ÊâæÂà∞LCZÂàóÔºåË∑≥ËøáLCZÂàÜÊûê")
        return
    
    # ÁõÆÊ†áLCZÁ±ªÂûã
    target_lcz_types = [1, 2, 3, 4, 9, 11]
    lcz_names = {
        1: 'Compact High-rise',
        2: 'Compact Mid-rise', 
        3: 'Compact Low-rise',
        4: 'Open High-rise',
        9: 'Sparsely Built',
        11: 'Dense Trees'
    }
    
    # Á≠õÈÄâÁõÆÊ†áLCZÊï∞ÊçÆ
    lcz_data = analyzer.merged_data[analyzer.merged_data['LCZ'].isin(target_lcz_types)]
    print(f"üìä LCZÁ≠õÈÄâÂêéÊï∞ÊçÆÈáè: {len(lcz_data)}/{len(analyzer.merged_data)} ({len(lcz_data)/len(analyzer.merged_data)*100:.1f}%)")
    
    # ‰∏∫ÊØè‰∏™LCZÁ±ªÂûãÂàÜÂà´Ë∑ëÂÆåÊï¥ÂàÜÊûê
    all_lcz_results = {}
    
    for lcz_type in target_lcz_types:
        lcz_subset = lcz_data[lcz_data['LCZ'] == lcz_type]
        if len(lcz_subset) < 50:
            print(f"‚ö†Ô∏è LCZ {lcz_type} Ê†∑Êú¨Èáè‰∏çË∂≥ ({len(lcz_subset)})ÔºåË∑≥Ëøá")
            continue
            
        print(f"\n{'='*60}")
        print(f"üèôÔ∏è LCZ {lcz_type} ({lcz_names[lcz_type]}) - ÂÆåÊï¥ÂàÜÊûêÂºÄÂßã")
        print(f"üìä Ê†∑Êú¨Êï∞: {len(lcz_subset)}")
        print("="*60)
        
        # ÂàõÂª∫LCZ‰∏ìÁî®ÂàÜÊûêÂô®
        lcz_analyzer = FixedOptimizedInteractionAnalyzer()
        lcz_analyzer.merged_data = lcz_subset.copy()
        
        # ÂàõÂª∫LCZ‰∏ìÁî®ÁªìÊûúÁõÆÂΩï
        lcz_save_dir = f"{save_dir}/LCZ_{lcz_type}_{lcz_names[lcz_type].replace(' ', '_')}"
        os.makedirs(lcz_save_dir, exist_ok=True)
        
        # ‰∏∫ÊØè‰∏™ÊÑüÁü•Áª¥Â∫¶Ë∑ëÂÆåÊï¥ÂàÜÊûê
        lcz_results = {}
        
        for perception in perception_cols:
            print(f"\nüéØ LCZ {lcz_type} - ÊÑüÁü•Áª¥Â∫¶: {perception.upper()}")
            print("-" * 50)
            
            try:
                # MODULE 1: XGBoost + SHAP
                print(f"üîç MODULE 1: XGBoost + SHAP for LCZ {lcz_type} - {perception}")
                xgb_result = run_enhanced_xgboost_module(lcz_analyzer, perception, lcz_save_dir, libs)
                
                if xgb_result is None:
                    print(f"    ‚ùå XGBoostÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # MODULE 2: Lasso Feature Selection
                print(f"üéØ MODULE 2: Lasso Feature Selection for LCZ {lcz_type} - {perception}")
                lasso_result = run_enhanced_lasso_module(lcz_analyzer, perception, lcz_save_dir)
                
                if lasso_result is None:
                    print(f"    ‚ùå LassoÊ®°ÂùóÂ§±Ë¥•")
                    continue
                
                # MODULE 3: Ensemble Strategy
                print(f"üîó MODULE 3: Ensemble Strategy for LCZ {lcz_type} - {perception}")
                ensemble_result = run_integrated_ensemble_module(lcz_analyzer, perception, lcz_save_dir, 
                                                               {'xgb': xgb_result, 'lasso': lasso_result})
                
                # ‰øùÂ≠òÁªìÊûú
                lcz_results[perception] = {
                    'xgb': xgb_result,
                    'lasso': lasso_result,
                    'ensemble': ensemble_result
                }
                
                print(f"\nüìä LCZ {lcz_type} - {perception} ÊÄßËÉΩÊÄªÁªì:")
                print(f"  ‚Ä¢ XGBoost: R¬≤ = {xgb_result['test_score']:.4f}")
                print(f"  ‚Ä¢ Lasso: R¬≤ = {lasso_result['lasso_score']:.4f}")
                print(f"  ‚Ä¢ Elastic-Net: R¬≤ = {lasso_result['elastic_score']:.4f}")
                print(f"  ‚Ä¢ Ensemble: R¬≤ = {ensemble_result['ensemble_score']:.4f}")
                
            except Exception as e:
                print(f"    ‚ùå LCZ {lcz_type} - {perception} ÂàÜÊûêÂ§±Ë¥•: {str(e)}")
                continue
        
        # ‰øùÂ≠òËøô‰∏™LCZÁöÑÊâÄÊúâÁªìÊûú
        all_lcz_results[lcz_type] = {
            'name': lcz_names[lcz_type],
            'sample_count': len(lcz_subset),
            'results': lcz_results,
            'save_dir': lcz_save_dir
        }
        
        print(f"\n‚úÖ LCZ {lcz_type} ({lcz_names[lcz_type]}) ÂÆåÊï¥ÂàÜÊûêÂÆåÊàê!")
        print(f"üìÅ ÁªìÊûú‰øùÂ≠òÂú®: {lcz_save_dir}")
    
    print(f"\nüéâ LCZÂàÜÂå∫ÂàÜÊûêÂÆåÊàê! ÂÖ±ÂàÜÊûê‰∫Ü {len(all_lcz_results)} ‰∏™LCZÂàÜÂå∫")
    
    # üÜï Êñ∞Â¢ûÔºöÂàõÂª∫LCZÂêàÂπ∂ÂØπÊØîÁöÑSHAP DependenceÂõæ
    create_lcz_combined_shap_dependence(analyzer, all_lcz_results, perception_cols, save_dir, libs)
    
    return all_lcz_results

def create_lcz_combined_shap_dependence(analyzer, all_lcz_results, perception_cols, save_dir, libs):
    """
    üÜï ÂàõÂª∫LCZÂêàÂπ∂ÂØπÊØîÁöÑSHAP DependenceÂõæ
    Â∞ÜLCZ 1, 4, 9, 11ÁöÑÊï∞ÊçÆÊîæÂú®Âêå‰∏ÄÂº†Âõæ‰∏äÔºåÁî®‰∏çÂêåÈ¢úËâ≤Âå∫ÂàÜ
    ÊØè‰∏™ÂèòÈáè‰∏ÄÂº†ÂõæÔºåÂåÖÂê´4‰∏™LCZÂàÜÂå∫ÁöÑÊï£ÁÇπ+ÈòàÂÄºÊõ≤Á∫ø+ÁΩÆ‰ø°Âå∫Èó¥
    """
    print("\n" + "="*80)
    print("üé® ÂàõÂª∫LCZÂêàÂπ∂ÂØπÊØîSHAP DependenceÂõæ (LCZ 1, 4, 9, 11)")
    print("="*80)
    
    # ÁõÆÊ†áLCZÁ±ªÂûã - Âè™ÂØπÊØîËøô4‰∏™
    target_lcz_for_combined = [1, 4, 9, 11]
    lcz_names = {
        1: 'Compact High-rise',
        4: 'Open High-rise',
        9: 'Sparsely Built',
        11: 'Dense Trees'
    }
    
    # Áî®Êà∑ÊåáÂÆöÁöÑ4ÁßçÈ¢úËâ≤
    lcz_colors = {
        1: {'point': '#FF6B4A', 'line': '#E24A33'},    # Ê©ôÁ∫¢Ëâ≤
        4: {'point': '#4ECDC4', 'line': '#20B2AA'},    # ËìùÁªøËâ≤
        9: {'point': '#9B59B6', 'line': '#8E44AD'},    # Á¥´Ëâ≤
        11: {'point': '#A4D037', 'line': '#7CB342'}    # ÈªÑÁªøËâ≤
    }
    
    # ÂàõÂª∫‰øùÂ≠òÁõÆÂΩï
    combined_dir = f"{save_dir}/LCZ_Combined_Comparison"
    os.makedirs(combined_dir, exist_ok=True)
    
    # Ê£ÄÊü•Âì™‰∫õLCZÊúâÁªìÊûú
    available_lcz = [lcz for lcz in target_lcz_for_combined if lcz in all_lcz_results]
    if len(available_lcz) < 2:
        print(f"‚ö†Ô∏è ÂèØÁî®LCZÂàÜÂå∫‰∏çË∂≥ ({len(available_lcz)}‰∏™)ÔºåË∑≥ËøáÂêàÂπ∂ÂØπÊØîÂõæ")
        return
    
    print(f"üìä Â∞ÜÂêàÂπ∂ÂØπÊØîÁöÑLCZÂàÜÂå∫: {available_lcz}")
    
    # Ëé∑ÂèñÁâπÂæÅÂàóË°®Ôºà‰ªéÁ¨¨‰∏Ä‰∏™ÂèØÁî®ÁöÑLCZÁªìÊûú‰∏≠Ëé∑ÂèñÔºâ
    first_lcz = available_lcz[0]
    first_perception = perception_cols[0] if perception_cols else 'safe'
    
    if first_perception not in all_lcz_results[first_lcz]['results']:
        print(f"‚ö†Ô∏è Êú™ÊâæÂà∞ÊÑüÁü•Áª¥Â∫¶ {first_perception} ÁöÑÁªìÊûú")
        return
    
    xgb_result = all_lcz_results[first_lcz]['results'][first_perception].get('xgb')
    if xgb_result is None:
        print("‚ö†Ô∏è Êú™ÊâæÂà∞XGBoostÁªìÊûú")
        return
    
    feature_names = xgb_result.get('feature_names', [])
    if not feature_names:
        print("‚ö†Ô∏è Êú™ÊâæÂà∞ÁâπÂæÅÂêçÁß∞")
        return
    
    print(f"üìä Â∞Ü‰∏∫ {len(feature_names)} ‰∏™ÁâπÂæÅÂàõÂª∫ÂêàÂπ∂ÂØπÊØîÂõæ")
    
    # ‰∏∫ÊØè‰∏™ÊÑüÁü•Áª¥Â∫¶ÂàõÂª∫ÂêàÂπ∂Âõæ
    for perception in perception_cols:
        print(f"\nüéØ Â§ÑÁêÜÊÑüÁü•Áª¥Â∫¶: {perception}")
        
        # Êî∂ÈõÜÊâÄÊúâLCZÁöÑSHAPÊï∞ÊçÆ
        lcz_shap_data = {}
        
        for lcz_type in available_lcz:
            if perception not in all_lcz_results[lcz_type]['results']:
                continue
            
            xgb_res = all_lcz_results[lcz_type]['results'][perception].get('xgb')
            if xgb_res is None:
                continue
            
            # Ëé∑ÂèñSHAPÊï∞ÊçÆ
            shap_values = xgb_res.get('shap_values')
            X_sample = xgb_res.get('X_sample')
            feat_names = xgb_res.get('feature_names')
            
            if shap_values is not None and X_sample is not None:
                lcz_shap_data[lcz_type] = {
                    'shap_values': shap_values,
                    'X_sample': X_sample,
                    'feature_names': feat_names
                }
        
        if len(lcz_shap_data) < 2:
            print(f"  ‚ö†Ô∏è {perception}ÂèØÁî®LCZÊï∞ÊçÆ‰∏çË∂≥ÔºåË∑≥Ëøá")
            continue
        
        print(f"  ‚úÖ Êî∂ÈõÜÂà∞ {len(lcz_shap_data)} ‰∏™LCZÂàÜÂå∫ÁöÑSHAPÊï∞ÊçÆ")
        
        # Ëé∑ÂèñÂÖ±ÂêåÁöÑÁâπÂæÅÂàóË°®
        common_features = None
        for lcz_type, data in lcz_shap_data.items():
            if common_features is None:
                common_features = set(data['feature_names'])
            else:
                common_features = common_features.intersection(set(data['feature_names']))
        
        # üÜï Á°Æ‰øùÂåÖÂê´ÊâÄÊúâÈáçË¶ÅÁöÑÊéßÂà∂ÂèòÈáèÔºåÊåâÈáçË¶ÅÊÄßÊéíÂ∫èËÄå‰∏çÊòØÂ≠óÊØçÊéíÂ∫è
        # ‰ºòÂÖàÊòæÁ§∫ÁöÑÊéßÂà∂ÂèòÈáèÂàóË°®
        priority_features = [
            'AVGIL', 'illumination_uniformity', 'spots_area', 'ADCG', 
            'spatial_lag_Wy', 'ntl_mean', 'POP_20_50',
            'safe', 'lively', 'beautiful', 'wealthy', 'depressing', 'boring'
        ]
        
        # Êåâ‰ºòÂÖàÁ∫ßÊéíÂ∫èÁâπÂæÅ
        common_features_list = list(common_features)
        priority_sorted = []
        remaining = []
        
        for pf in priority_features:
            if pf in common_features_list:
                priority_sorted.append(pf)
        
        for cf in common_features_list:
            if cf not in priority_sorted:
                remaining.append(cf)
        
        # ‰ºòÂÖàÁâπÂæÅÂú®ÂâçÔºåÂÖ∂‰ΩôÊåâÂ≠óÊØçÊéíÂ∫è
        common_features = priority_sorted + sorted(remaining)
        
        print(f"  üìä ÂÖ±ÂêåÁâπÂæÅÊï∞: {len(common_features)}")
        print(f"  üìä ÂåÖÂê´ÊéßÂà∂ÂèòÈáè: {[f for f in priority_features if f in common_features]}")
        
        # ÂàÜÈ°µÂàõÂª∫ÂõæË°® (ÊØèÈ°µ16‰∏™ÁâπÂæÅ)
        per_page = 16
        n_cols = 4
        n_rows = 4
        n_pages = int(np.ceil(len(common_features) / per_page))
        
        for page in range(max(1, n_pages)):
            start_idx = page * per_page
            end_idx = min(len(common_features), (page + 1) * per_page)
            features_page = common_features[start_idx:end_idx]
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))
            axes = np.array(axes).reshape(n_rows, n_cols)
            
            for i, feature in enumerate(features_page):
                r = i // n_cols
                c = i % n_cols
                ax = axes[r, c]
                
                # ‰∏∫ÊØè‰∏™LCZÁªòÂà∂Êï£ÁÇπ+Êõ≤Á∫ø+ÁΩÆ‰ø°Âå∫Èó¥
                for lcz_type in available_lcz:
                    if lcz_type not in lcz_shap_data:
                        continue
                    
                    data = lcz_shap_data[lcz_type]
                    feat_names = data['feature_names']
                    
                    if feature not in feat_names:
                        continue
                    
                    feat_idx = feat_names.index(feature)
                    x_vals = data['X_sample'].iloc[:, feat_idx].values
                    y_vals = data['shap_values'][:, feat_idx]
                    
                    point_color = lcz_colors[lcz_type]['point']
                    line_color = lcz_colors[lcz_type]['line']
                    label = f"LCZ{lcz_type}"
                    
                    # ÁªòÂà∂Êï£ÁÇπ+Êõ≤Á∫ø+ÁΩÆ‰ø°Âå∫Èó¥
                    _draw_dependence_with_ci(ax, x_vals, y_vals, point_color, line_color, label)
                
                ax.set_xlabel(feature.replace('_', ' ').title(), fontsize=9, fontweight='bold')
                ax.set_ylabel('SHAP value', fontsize=9)
                ax.legend(loc='best', fontsize=7, framealpha=0.8)
                ax.grid(True, alpha=0.25)
            
            # ÈöêËóèÁ©∫ÁôΩÂ≠êÂõæ
            for j in range(len(features_page), per_page):
                r = j // n_cols
                c = j % n_cols
                axes[r, c].axis('off')
            
            # Ê∑ªÂä†Âõæ‰æãËØ¥Êòé
            legend_text = " | ".join([f"LCZ{lcz}: {lcz_names[lcz]}" for lcz in available_lcz])
            
            fig.suptitle(
                f'LCZ Combined SHAP Dependence - {perception.title()}\n{legend_text}',
                fontsize=14, fontweight='bold'
            )
            plt.tight_layout()
            
            suffix = f"_p{page+1}" if n_pages > 1 else ""
            save_path = f'{combined_dir}/lcz_combined_dependence_{perception}{suffix}.png'
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            print(f"  ‚úÖ ‰øùÂ≠ò: {save_path}")
    
    print(f"\n‚úÖ LCZÂêàÂπ∂ÂØπÊØîSHAP DependenceÂõæÂ∑≤‰øùÂ≠òÂà∞: {combined_dir}")


def _draw_dependence_with_ci(ax, x_vals, y_vals, point_color, line_color, label):
    """
    ÁªòÂà∂SHAP dependenceÊï£ÁÇπÂõæ + ÈùûÁ∫øÊÄßÈòàÂÄºÊõ≤Á∫ø + 95%ÁΩÆ‰ø°Âå∫Èó¥
    """
    import pandas as pd
    import numpy as np
    from scipy.interpolate import UnivariateSpline
    
    df = pd.DataFrame({'x': x_vals, 'y': y_vals}).dropna()
    if len(df) < 10:
        ax.scatter(x_vals, y_vals, s=8, alpha=0.3, color=point_color, edgecolor='none', label=label)
        return
    
    df = df.sort_values('x')
    xs = np.linspace(df['x'].quantile(0.02), df['x'].quantile(0.98), 200)
    
    # ÊãüÂêàÊõ≤Á∫ø
    try:
        if df['x'].nunique() >= 5:
            # ÂàÜÁÆ±È¢ÑÂπ≥Êªë
            if len(df) >= 50:
                q = np.linspace(0.02, 0.98, 30)
                q_edges = df['x'].quantile(q).values
                q_edges = np.unique(q_edges)
                if len(q_edges) >= 5:
                    bins = np.digitize(df['x'].values, q_edges, right=True)
                    x_med, y_med = [], []
                    for b in np.unique(bins):
                        mask = bins == b
                        if mask.sum() > 2:
                            x_med.append(np.median(df['x'].values[mask]))
                            y_med.append(np.median(df['y'].values[mask]))
                    if len(x_med) >= 5:
                        x_fit = np.array(x_med)
                        y_fit = np.array(y_med)
                    else:
                        x_fit = df['x'].values
                        y_fit = df['y'].values
                else:
                    x_fit = df['x'].values
                    y_fit = df['y'].values
            else:
                x_fit = df['x'].values
                y_fit = df['y'].values
            
            s_val = max(1e-6, len(y_fit) * np.var(y_fit) * 1.0)
            spline = UnivariateSpline(x_fit, y_fit, s=s_val)
            ys = spline(xs)
        else:
            coefs = np.polyfit(df['x'].values, df['y'].values, deg=1)
            ys = np.polyval(coefs, xs)
    except Exception:
        coefs = np.polyfit(df['x'].values, df['y'].values, deg=1)
        ys = np.polyval(coefs, xs)
    
    # BootstrapÁΩÆ‰ø°Âå∫Èó¥
    rng = np.random.RandomState(42)
    n = len(df)
    n_boot = 100
    boot = []
    
    for _ in range(n_boot):
        idx = rng.randint(0, n, n)
        try:
            df_b = df.iloc[idx].sort_values('x')
            if df_b['x'].nunique() >= 5 and len(df_b) >= 20:
                s_val_b = max(1e-6, len(df_b) * np.var(df_b['y']) * 1.0)
                sp = UnivariateSpline(df_b['x'].values, df_b['y'].values, s=s_val_b)
                boot.append(sp(xs))
            else:
                coefs_b = np.polyfit(df_b['x'].values, df_b['y'].values, deg=1)
                boot.append(np.polyval(coefs_b, xs))
        except Exception:
            try:
                coefs_b = np.polyfit(df['x'].values[idx], df['y'].values[idx], deg=1)
                boot.append(np.polyval(coefs_b, xs))
            except:
                pass
    
    if len(boot) > 10:
        boot = np.vstack(boot)
        lower = np.percentile(boot, 2.5, axis=0)
        upper = np.percentile(boot, 97.5, axis=0)
    else:
        lower = ys - 0.1 * np.abs(ys)
        upper = ys + 0.1 * np.abs(ys)
    
    # ÁªòÂà∂ÔºöÊï£ÁÇπ + ÁΩÆ‰ø°Âå∫Èó¥ + Êõ≤Á∫ø
    ax.scatter(df['x'], df['y'], s=8, alpha=0.25, color=point_color, edgecolor='none')
    ax.fill_between(xs, lower, upper, color=line_color, alpha=0.12, linewidth=0)
    ax.plot(xs, ys, color=line_color, linewidth=1.5, label=label)


# Á¨¨‰∫å‰∏™mainÂáΩÊï∞Â∑≤Âà†Èô§ÔºàÈáçÂ§çÂÆö‰πâÔºâÔºå‰ΩøÁî®Á¨¨‰∏Ä‰∏™mainÂáΩÊï∞ÔºàÂåÖÂê´LCZÂíåNTLÂàÜÊûêÔºâ

def create_comprehensive_performance_summary_table(all_model_summary, save_dir, analyzer):
    """ÂàõÂª∫ÂÆåÊï¥ÁöÑÊ®°ÂûãÊÄßËÉΩÊ±áÊÄªË°®Ê†º - Excel + ÂèØËßÜÂåñ"""
    print("  üìä ÁîüÊàêÁªºÂêàÊ®°ÂûãÊÄßËÉΩÊ±áÊÄªË°®Ê†º...")
    
    # ÂàõÂª∫DataFrame
    summary_df = pd.DataFrame(all_model_summary).T
    
    # Ê∑ªÂä†ÁªüËÆ°‰ø°ÊÅØ
    summary_df['Mean_R2_Across_Models'] = summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].mean(axis=1)
    summary_df['Std_R2_Across_Models'] = summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].std(axis=1)
    summary_df['Model_Consistency'] = 1 - summary_df['Std_R2_Across_Models']  # ‰∏ÄËá¥ÊÄßÊåáÊï∞
    
    # ÊéíÂ∫èÂπ∂ÈáçÊñ∞ÊéíÂàóÂàó
    columns_order = [
        'Best_Model_Score', 'XGBoost_R2', 'XGBoost_Train_R2', 'Lasso_R2', 'ElasticNet_R2',
        'Ensemble_R2', 'NTL_Baseline_R2', 'Full_Interaction_R2', 'Improvement_vs_NTL',
        'Lasso_Features_Selected', 'ElasticNet_Features_Selected', 
        'Mean_R2_Across_Models', 'Std_R2_Across_Models', 'Model_Consistency'
    ]
    summary_df = summary_df[columns_order]
    
    # ‰øùÂ≠òÂà∞Excel
    excel_path = f'{save_dir}/comprehensive_model_performance_summary.xlsx'
    with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
        # ‰∏ªË¶ÅÁªìÊûúË°®
        summary_df.to_excel(writer, sheet_name='Model_Performance', index=True)
        
        # Ê∑ªÂä†ÁªüËÆ°Ê±áÊÄª
        stats_summary = pd.DataFrame({
            'Metric': ['Best_Overall_R2', 'Average_R2', 'Std_R2', 'Min_R2', 'Max_R2'],
            'Value': [
                summary_df['Best_Model_Score'].max(),
                summary_df['Best_Model_Score'].mean(),
                summary_df['Best_Model_Score'].std(),
                summary_df['Best_Model_Score'].min(),
                summary_df['Best_Model_Score'].max()
            ]
        })
        stats_summary.to_excel(writer, sheet_name='Summary_Statistics', index=False)
        
        # Ê∑ªÂä†Ê®°ÂûãÊéíÂêç
        model_ranking = pd.DataFrame({
            'Perception': summary_df.index,
            'Best_Model': summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].idxmax(axis=1),
            'Best_Score': summary_df['Best_Model_Score'],
            'Worst_Model': summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].idxmin(axis=1),
            'Score_Range': summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].max(axis=1) - 
                          summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].min(axis=1)
        })
        model_ranking.to_excel(writer, sheet_name='Model_Ranking', index=False)
        
        # ÁâπÂæÅÈÄâÊã©ÂàÜÊûê
        feature_analysis = pd.DataFrame({
            'Perception': summary_df.index,
            'Lasso_Features': summary_df['Lasso_Features_Selected'],
            'ElasticNet_Features': summary_df['ElasticNet_Features_Selected'],
            'Feature_Difference': summary_df['ElasticNet_Features_Selected'] - summary_df['Lasso_Features_Selected'],
            'Feature_Selection_Efficiency': summary_df['Best_Model_Score'] / (summary_df['Lasso_Features_Selected'] + 1)
        })
        feature_analysis.to_excel(writer, sheet_name='Feature_Analysis', index=False)
    
    print(f"    ‚úÖ ExcelÊä•ÂëäÂ∑≤‰øùÂ≠ò: {excel_path}")
    
    # ÂàõÂª∫ÂèØËßÜÂåñÊ±áÊÄª
    user_colors = {
        'primary': '#4B0082',     # Deep purple
        'secondary': '#20B2AA',   # Light sea green/teal  
        'accent1': '#6A5ACD',     # Slate blue
        'accent2': '#48D1CC',     # Medium turquoise
        'accent3': '#9370DB',     # Medium purple
        'accent4': '#40E0D0',     # Turquoise
    }
    
    # 1. Ê®°ÂûãÊÄßËÉΩÁÉ≠ÂäõÂõæ
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # ÁÉ≠ÂäõÂõæÊï∞ÊçÆ
    heatmap_data = summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].T
    
    im1 = axes[0,0].imshow(heatmap_data.values, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
    axes[0,0].set_xticks(range(len(heatmap_data.columns)))
    axes[0,0].set_xticklabels([col.title() for col in heatmap_data.columns], rotation=45)
    axes[0,0].set_yticks(range(len(heatmap_data.index)))
    axes[0,0].set_yticklabels([idx.replace('_R2', '') for idx in heatmap_data.index])
    axes[0,0].set_title('Model Performance Heatmap (R¬≤ Scores)', fontweight='bold', fontsize=14)
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÊ≥®
    for i in range(len(heatmap_data.index)):
        for j in range(len(heatmap_data.columns)):
            value = heatmap_data.iloc[i, j]
            color = 'white' if value < 0.5 else 'black'
            axes[0,0].text(j, i, f'{value:.3f}', ha='center', va='center', 
                         color=color, fontweight='bold', fontsize=10)
    
    plt.colorbar(im1, ax=axes[0,0], label='R¬≤ Score')
    
    # 2. ÊúÄ‰Ω≥Ê®°ÂûãÂàÜÂ∏É
    best_models = summary_df[['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].idxmax(axis=1)
    model_counts = best_models.value_counts()
    
    colors_pie = [user_colors['primary'], user_colors['secondary'], user_colors['accent1'], user_colors['accent2']]
    axes[0,1].pie(model_counts.values, labels=[label.replace('_R2', '') for label in model_counts.index], 
                 colors=colors_pie[:len(model_counts)], autopct='%1.1f%%', startangle=90)
    axes[0,1].set_title('Best Model Distribution\nAcross Perceptions', fontweight='bold', fontsize=14)
    
    # 3. ÊîπËøõÁ®ãÂ∫¶Êù°ÂΩ¢Âõæ
    improvement_data = summary_df['Improvement_vs_NTL'].fillna(0)
    bars = axes[1,0].bar(range(len(improvement_data)), improvement_data.values, 
                        color=user_colors['accent3'], alpha=0.8, edgecolor='white', linewidth=1)
    axes[1,0].set_xticks(range(len(improvement_data)))
    axes[1,0].set_xticklabels([idx.title() for idx in improvement_data.index], rotation=45)
    axes[1,0].set_ylabel('Improvement vs NTL Baseline (%)', fontweight='bold')
    axes[1,0].set_title('Model Improvement Over NTL Baseline', fontweight='bold', fontsize=14)
    axes[1,0].grid(True, alpha=0.3, axis='y')
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for bar, value in zip(bars, improvement_data.values):
        if not np.isnan(value):
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(improvement_data.values)*0.02,
                          f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')
    
    # 4. ÁâπÂæÅÈÄâÊã©ÊïàÁéá
    lasso_features = summary_df['Lasso_Features_Selected']
    elastic_features = summary_df['ElasticNet_Features_Selected']
    
    x_pos = np.arange(len(lasso_features))
    width = 0.35
    
    bars1 = axes[1,1].bar(x_pos - width/2, lasso_features, width, 
                         label='Lasso', color=user_colors['secondary'], alpha=0.8)
    bars2 = axes[1,1].bar(x_pos + width/2, elastic_features, width,
                         label='Elastic-Net', color=user_colors['primary'], alpha=0.8)
    
    axes[1,1].set_xlabel('Perception Dimensions', fontweight='bold')
    axes[1,1].set_ylabel('Number of Selected Features', fontweight='bold')
    axes[1,1].set_title('Feature Selection Comparison\nLasso vs Elastic-Net', fontweight='bold', fontsize=14)
    axes[1,1].set_xticks(x_pos)
    axes[1,1].set_xticklabels([idx.title() for idx in lasso_features.index], rotation=45)
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3, axis='y')
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for bar, value in zip(bars1, lasso_features):
        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(max(lasso_features), max(elastic_features))*0.02,
                      f'{int(value)}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    for bar, value in zip(bars2, elastic_features):
        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(max(lasso_features), max(elastic_features))*0.02,
                      f'{int(value)}', ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    fig.suptitle('Comprehensive Model Performance Analysis\nAll 6 Perception Dimensions with Control Variables', 
                fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/model_performance_heatmap.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # 3. ÁâπÂæÅÈáçË¶ÅÊÄßÊ±áÊÄªÂèØËßÜÂåñ
    create_feature_importance_summary(analyzer, save_dir, user_colors)
    
    print(f"    ‚úÖ ÂèØËßÜÂåñÊ±áÊÄªÂ∑≤‰øùÂ≠ò:")
    print(f"      ‚Ä¢ model_performance_heatmap.png - Ê®°ÂûãÊÄßËÉΩÁÉ≠ÂäõÂõæ")
    print(f"      ‚Ä¢ feature_importance_summary.png - ÁâπÂæÅÈáçË¶ÅÊÄßÊ±áÊÄª")
    
    # ÊâìÂç∞ÊéßÂà∂Âè∞Ê±áÊÄªË°®Ê†º
    print(f"\nüìä COMPREHENSIVE MODEL PERFORMANCE SUMMARY TABLE")
    print("="*120)
    print(f"{'Perception':<12} {'Best R¬≤':<8} {'XGBoost':<8} {'Lasso':<8} {'Elastic':<8} {'Ensemble':<8} {'NTL Base':<8} {'Improvement':<12} {'Best Model':<12}")
    print("-" * 120)
    
    for perception in summary_df.index:
        row = summary_df.loc[perception]
        best_model = summary_df.loc[perception, ['XGBoost_R2', 'Lasso_R2', 'ElasticNet_R2', 'Ensemble_R2']].idxmax().replace('_R2', '')
        
        print(f"{perception.capitalize():<12} {row['Best_Model_Score']:<8.4f} {row['XGBoost_R2']:<8.4f} "
              f"{row['Lasso_R2']:<8.4f} {row['ElasticNet_R2']:<8.4f} {row['Ensemble_R2']:<8.4f} "
              f"{row['NTL_Baseline_R2']:<8.4f} {row['Improvement_vs_NTL']:>10.1f}% {best_model:<12}")
    
    print("-" * 120)
    print(f"{'AVERAGE':<12} {summary_df['Best_Model_Score'].mean():<8.4f} {summary_df['XGBoost_R2'].mean():<8.4f} "
          f"{summary_df['Lasso_R2'].mean():<8.4f} {summary_df['ElasticNet_R2'].mean():<8.4f} "
          f"{summary_df['Ensemble_R2'].mean():<8.4f} {summary_df['NTL_Baseline_R2'].mean():<8.4f} "
          f"{summary_df['Improvement_vs_NTL'].mean():>10.1f}% {'Ensemble':<12}")
    
    # ÊéßÂà∂ÂèòÈáè‰ΩøÁî®ÊÉÖÂÜµÁªüËÆ°
    control_vars = ['AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity', ]   #'predicted_spillover'
    available_controls = [col for col in control_vars if col in analyzer.merged_data.columns]
    
    print(f"\nüìä CONTROL VARIABLES USAGE SUMMARY")
    print("="*80)
    print(f"Available Control Variables: {len(available_controls)}/{len(control_vars)}")
    print(f"Successfully Loaded: {', '.join(available_controls) if available_controls else 'None'}")
    print(f"Missing Variables: {', '.join([col for col in control_vars if col not in available_controls])}")
    print(f"Total Features per Model: {len(USER_SEMANTIC_CLASSES) * 7 + len(available_controls)} (semantic + control)")
    
    print(f"\nüìä MODEL ARCHITECTURE SUMMARY")
    print("="*80)
    print(f"Semantic Classes: {len(USER_SEMANTIC_CLASSES)} (user-specified)")
    print(f"Interaction Terms: 7 per semantic (A+B+D+AB+AD+BD+ABD)")
    print(f"Control Variables: {len(available_controls)}")
    print(f"Total Base Features: {len(USER_SEMANTIC_CLASSES) * 7 + len(available_controls)}")
    print(f"Log Transformation: log(perception + 1)")
    print(f"Cross-validation: 30% test split, random_state=42")
    
    return summary_df

def create_safety_focused_analysis(analyzer, save_dir, perception_name='safe'):
    """Create perception-focused analysis with 4 scatter plots as requested
    
    Args:
        analyzer: data analyzer object
        save_dir: directory to save plots
        perception_name: perception dimension to analyze (default: 'safe')
    """
    print(f"\nüéØ {perception_name.upper()}-FOCUSED ANALYSIS - Creating 4 specialized scatter plots")
    print("="*60)
    
    # Mako + Orange color scheme as requested
    mako_orange_colors = {
        'light_orange': '#FFB366',    # ÊµÖÊ©ôËâ≤
        'blue_green': '#4ECDC4',      # ËìùÁªøËâ≤  
        'yellow_green': '#A8E6CF',    # ÈªÑÁªøËâ≤
        'gray_blue_purple': '#7B68EE' # ÁÅ∞ËìùÁ¥´Ëâ≤
    }
    
    # Check if perception exists in data
    if perception_name not in analyzer.merged_data.columns:
        print(f"  ‚ö†Ô∏è Perception '{perception_name}' not found in data, skipping...")
        return
    
    # Focus on the specified perception
    perception = perception_name
    y = np.log(analyzer.merged_data[perception] + 1)
    print(f"  üìä Analyzing {perception.upper()} perception: {len(y)} samples")
    
    # Prepare all models and data
    models_data = prepare_safety_models(analyzer, y)
    
    # Create 4 plots with perception-specific filenames
    create_safety_linear_plots(models_data, y, save_dir, mako_orange_colors, perception_name)
    create_safety_nonlinear_plots(models_data, y, save_dir, mako_orange_colors, perception_name)
    
    # Create performance improvement chart
    create_safety_performance_chart(models_data, save_dir, mako_orange_colors, perception_name)
    
    print(f"  ‚úÖ {perception_name.upper()}-focused analysis completed!")

def prepare_safety_models(analyzer, y):
    """Prepare all models for safety analysis"""
    print("  üîß Preparing models for Safety analysis...")
    
    models_data = {}
    
    # 1. Full model features (A+B+D+AB+AD+BD+ABD + Controls)
    X_full, feature_names_full = create_strict_abd_interactions(analyzer)
    X_full_train, X_full_test, y_train, y_test = train_test_split(
        X_full, y, test_size=0.3, random_state=42
    )
    
    # Full XGBoost
    full_xgb = GradientBoostingRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, random_state=42)
    full_xgb.fit(X_full_train, y_train)
    full_xgb_pred = full_xgb.predict(X_full_test)
    
    # Full Lasso
    scaler_full = StandardScaler()
    X_full_train_scaled = scaler_full.fit_transform(X_full_train)
    X_full_test_scaled = scaler_full.transform(X_full_test)
    full_lasso = LassoCV(alphas=np.logspace(-5, 2, 100), cv=5, random_state=42)
    full_lasso.fit(X_full_train_scaled, y_train)
    full_lasso_pred = full_lasso.predict(X_full_test_scaled)
    
    models_data['full'] = {
        'X_test': X_full_test, 'y_test': y_test,
        'xgb_pred': full_xgb_pred, 'lasso_pred': full_lasso_pred,
        'xgb_model': full_xgb, 'lasso_model': full_lasso,
        'scaler': scaler_full
    }
    
    # 2. NTL only (baseline 1)
    if 'DN' in analyzer.merged_data.columns:
        X_ntl = analyzer.merged_data[['DN']].fillna(0)
        X_ntl_train, X_ntl_test, y_ntl_train, y_ntl_test = train_test_split(
            X_ntl, y, test_size=0.3, random_state=42
        )
        
        # NTL XGBoost
        ntl_xgb = GradientBoostingRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, random_state=42)
        ntl_xgb.fit(X_ntl_train, y_ntl_train)
        ntl_xgb_pred = ntl_xgb.predict(X_ntl_test)
        
        # NTL Lasso
        scaler_ntl = StandardScaler()
        X_ntl_train_scaled = scaler_ntl.fit_transform(X_ntl_train)
        X_ntl_test_scaled = scaler_ntl.transform(X_ntl_test)
        ntl_lasso = LassoCV(alphas=np.logspace(-5, 2, 100), cv=5, random_state=42)
        ntl_lasso.fit(X_ntl_train_scaled, y_ntl_train)
        ntl_lasso_pred = ntl_lasso.predict(X_ntl_test_scaled)
        
        models_data['ntl'] = {
            'X_test': X_ntl_test, 'y_test': y_ntl_test,
            'xgb_pred': ntl_xgb_pred, 'lasso_pred': ntl_lasso_pred,
            'xgb_model': ntl_xgb, 'lasso_model': ntl_lasso,
            'scaler': scaler_ntl
        }
    
    # 3. Semantic only (baseline 2)
    X_semantic, semantic_features = create_semantic_with_controls_model(analyzer)
    if X_semantic is not None:
        # Remove control variables, keep only semantic A features
        semantic_only_cols = [col for col in X_semantic.columns if col.startswith('A_')]
        if semantic_only_cols:
            X_semantic_only = X_semantic[semantic_only_cols]
            X_sem_train, X_sem_test, y_sem_train, y_sem_test = train_test_split(
                X_semantic_only, y, test_size=0.3, random_state=42
            )
            
            # Semantic XGBoost
            sem_xgb = GradientBoostingRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, random_state=42)
            sem_xgb.fit(X_sem_train, y_sem_train)
            sem_xgb_pred = sem_xgb.predict(X_sem_test)
            
            # Semantic Lasso
            scaler_sem = StandardScaler()
            X_sem_train_scaled = scaler_sem.fit_transform(X_sem_train)
            X_sem_test_scaled = scaler_sem.transform(X_sem_test)
            sem_lasso = LassoCV(alphas=np.logspace(-5, 2, 100), cv=5, random_state=42)
            sem_lasso.fit(X_sem_train_scaled, y_sem_train)
            sem_lasso_pred = sem_lasso.predict(X_sem_test_scaled)
            
            models_data['semantic'] = {
                'X_test': X_sem_test, 'y_test': y_sem_test,
                'xgb_pred': sem_xgb_pred, 'lasso_pred': sem_lasso_pred,
                'xgb_model': sem_xgb, 'lasso_model': sem_lasso,
                'scaler': scaler_sem
            }
    
    print(f"    ‚úÖ Prepared {len(models_data)} model groups")
    return models_data

def create_safety_linear_plots(models_data, y, save_dir, colors, perception_name='safe'):
    """Create 2 linear scatter plots with fit lines and confidence intervals - FIXED LAYOUT"""
    print("  üìä Creating linear scatter plots...")
    
    perception_display = perception_name.capitalize()
    
    # Plot 1: Full model comparison (XGBoost vs Lasso)
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))  # Fixed size for consistent canvas
    
    if 'full' in models_data:
        data = models_data['full']
        y_test = data['y_test']
        
        # Calculate R¬≤ for layering (best model on top)
        from sklearn.metrics import r2_score
        lasso_r2 = r2_score(y_test, data['lasso_pred'])
        xgb_r2 = r2_score(y_test, data['xgb_pred'])
        
        # Plot worse model first (will be underneath)
        if lasso_r2 > xgb_r2:
            # Lasso is better, plot XGBoost first
            plot_scatter_with_fit_line(ax, y_test, data['xgb_pred'], 
                                     colors['light_orange'], 'Full XGBoost', alpha=0.6)
            plot_scatter_with_fit_line(ax, y_test, data['lasso_pred'], 
                                     colors['blue_green'], 'Full Lasso', alpha=0.7)
        else:
            # XGBoost is better, plot Lasso first (XGBoost will be on top)
            plot_scatter_with_fit_line(ax, y_test, data['lasso_pred'], 
                                     colors['blue_green'], 'Full Lasso', alpha=0.6)
            plot_scatter_with_fit_line(ax, y_test, data['xgb_pred'], 
                                     colors['light_orange'], 'Full XGBoost', alpha=0.7)
    
        # Perfect prediction line
        min_val = min(y_test.min(), min(data['xgb_pred'].min(), data['lasso_pred'].min()))
        max_val = max(y_test.max(), max(data['xgb_pred'].max(), data['lasso_pred'].max()))
        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2, label='Perfect Prediction')
        
        # Fixed axis limits to maintain 1:1 aspect ratio precision
        buffer = (max_val - min_val) * 0.05
        ax.set_xlim(min_val - buffer, max_val + buffer)
        ax.set_ylim(min_val - buffer, max_val + buffer)
    
    ax.set_xlabel(f'True {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_ylabel(f'Predicted {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_title(f'Full Model Comparison: XGBoost vs Lasso\n(Complete A+B+D+AB+AD+BD+ABD + Controls) - {perception_display}', 
                fontweight='bold', fontsize=14)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')
    
    # Legend with fixed position to avoid canvas compression
    ax.legend(loc='upper left', fontsize=9, framealpha=0.9)
    
    plt.subplots_adjust(right=0.95)  # Reserve space but don't compress canvas
    plt.savefig(f'{save_dir}/{perception_name}_full_model_linear.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Plot 2: Baseline comparison (NTL vs Semantic)
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))  # Fixed size for consistent canvas
    
    # Collect all data for proper layering and axis limits
    all_y_true = []
    all_y_pred = []
    model_data = []
    
    if 'ntl' in models_data:
        data = models_data['ntl']
        y_test_ntl = data['y_test']
        all_y_true.extend(y_test_ntl)
        all_y_pred.extend(data['xgb_pred'])
        all_y_pred.extend(data['lasso_pred'])
        
        # Calculate R¬≤ for both NTL models
        ntl_xgb_r2 = r2_score(y_test_ntl, data['xgb_pred'])
        ntl_lasso_r2 = r2_score(y_test_ntl, data['lasso_pred'])
        
        model_data.append(('ntl', 'xgb', y_test_ntl, data['xgb_pred'], colors['yellow_green'], 'Baseline1 NTL (XGBoost)', ntl_xgb_r2))
        model_data.append(('ntl', 'lasso', y_test_ntl, data['lasso_pred'], colors['gray_blue_purple'], 'Baseline1 NTL (Lasso)', ntl_lasso_r2))
    
    if 'semantic' in models_data:
        data = models_data['semantic']
        y_test_sem = data['y_test']
        all_y_true.extend(y_test_sem)
        all_y_pred.extend(data['xgb_pred'])
        all_y_pred.extend(data['lasso_pred'])
        
        # Calculate R¬≤ for both Semantic models
        sem_xgb_r2 = r2_score(y_test_sem, data['xgb_pred'])
        sem_lasso_r2 = r2_score(y_test_sem, data['lasso_pred'])
        
        model_data.append(('semantic', 'xgb', y_test_sem, data['xgb_pred'], colors['light_orange'], 'Baseline2 Semantic (XGBoost)', sem_xgb_r2))
        model_data.append(('semantic', 'lasso', y_test_sem, data['lasso_pred'], colors['blue_green'], 'Baseline2 Semantic (Lasso)', sem_lasso_r2))
    
    # Sort by R¬≤ score (lowest first, so best model is plotted last and appears on top)
    model_data.sort(key=lambda x: x[6])  # Sort by R¬≤ score
    
    # Plot models in order (worst to best)
    for i, (model_type, algorithm, y_true, y_pred, color, label, r2) in enumerate(model_data):
        alpha = 0.5 + (i * 0.1)  # Gradually increase alpha, best model most opaque
        plot_scatter_with_fit_line(ax, y_true, y_pred, color, label, alpha=alpha)
    
    # Perfect prediction line
    if all_y_true and all_y_pred:
        min_val = min(min(all_y_true), min(all_y_pred))
        max_val = max(max(all_y_true), max(all_y_pred))
        ax.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, linewidth=2, label='Perfect Prediction')
        
        # Fixed axis limits to maintain 1:1 aspect ratio precision
        buffer = (max_val - min_val) * 0.05
        ax.set_xlim(min_val - buffer, max_val + buffer)
        ax.set_ylim(min_val - buffer, max_val + buffer)
    
    ax.set_xlabel(f'True {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_ylabel(f'Predicted {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_title(f'Baseline Models Comparison\n(NTL Only vs Semantic Only) - {perception_display}', 
                fontweight='bold', fontsize=14)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')
    
    # Legend with fixed position to avoid canvas compression
    ax.legend(loc='upper left', fontsize=8, framealpha=0.9)
    
    plt.subplots_adjust(right=0.95)  # Reserve space but don't compress canvas
    plt.savefig(f'{save_dir}/{perception_name}_baseline_linear.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("    ‚úÖ Linear plots completed with fixed canvas and proper layering")

def create_safety_nonlinear_plots(models_data, y, save_dir, colors, perception_name='safe'):
    """Create 2 nonlinear threshold curve plots - FIXED LAYOUT"""
    print("  üìä Creating nonlinear threshold curve plots...")
    
    perception_display = perception_name.capitalize()
    
    # Plot 3: Full model nonlinear curves
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))  # Fixed size for consistent canvas
    
    if 'full' in models_data:
        data = models_data['full']
        y_test = data['y_test']
        
        # Calculate R¬≤ for layering (best model on top)
        from sklearn.metrics import r2_score
        lasso_r2 = r2_score(y_test, data['lasso_pred'])
        xgb_r2 = r2_score(y_test, data['xgb_pred'])
        
        # Plot worse model first (will be underneath)
        if lasso_r2 > xgb_r2:
            # Lasso is better, plot XGBoost first
            create_threshold_curve(ax, y_test, data['xgb_pred'], 
                                 colors['light_orange'], 'Full XGBoost (Nonlinear)', smooth=True)
            create_threshold_curve(ax, y_test, data['lasso_pred'], 
                                 colors['blue_green'], 'Full Lasso (Nonlinear)', smooth=True)
        else:
            # XGBoost is better, plot Lasso first (XGBoost will be on top)
            create_threshold_curve(ax, y_test, data['lasso_pred'], 
                                 colors['blue_green'], 'Full Lasso (Nonlinear)', smooth=True)
            create_threshold_curve(ax, y_test, data['xgb_pred'], 
                                 colors['light_orange'], 'Full XGBoost (Nonlinear)', smooth=True)
        
        # Set fixed axis limits for 1:1 precision
        min_val = min(y_test.min(), min(data['xgb_pred'].min(), data['lasso_pred'].min()))
        max_val = max(y_test.max(), max(data['xgb_pred'].max(), data['lasso_pred'].max()))
        buffer = (max_val - min_val) * 0.05
        ax.set_xlim(min_val - buffer, max_val + buffer)
        ax.set_ylim(min_val - buffer, max_val + buffer)
    
    ax.set_xlabel(f'True {perception_display} Values (Sorted)', fontweight='bold', fontsize=12)
    ax.set_ylabel(f'Predicted {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_title(f'Full Model Nonlinear Threshold Curves\n(Smooth Curves with 95% CI) - {perception_display}', 
                fontweight='bold', fontsize=14)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')  # Maintain 1:1 aspect ratio
    
    # Legend with fixed position to avoid canvas compression
    ax.legend(loc='upper left', fontsize=9, framealpha=0.9)
    
    plt.subplots_adjust(right=0.95)  # Reserve space but don't compress canvas
    plt.savefig(f'{save_dir}/{perception_name}_full_model_nonlinear.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    # Plot 4: Baseline nonlinear curves
    fig, ax = plt.subplots(1, 1, figsize=(10, 8))  # Fixed size for consistent canvas
    
    # Collect all data for proper layering and axis limits
    all_y_true = []
    all_y_pred = []
    model_data = []
    
    if 'ntl' in models_data:
        data = models_data['ntl']
        y_test_ntl = data['y_test']
        all_y_true.extend(y_test_ntl)
        all_y_pred.extend(data['xgb_pred'])
        all_y_pred.extend(data['lasso_pred'])
        
        # Calculate R¬≤ for both NTL models
        ntl_xgb_r2 = r2_score(y_test_ntl, data['xgb_pred'])
        ntl_lasso_r2 = r2_score(y_test_ntl, data['lasso_pred'])
        
        model_data.append((y_test_ntl, data['xgb_pred'], colors['yellow_green'], 'Baseline1 NTL (XGBoost)', ntl_xgb_r2))
        model_data.append((y_test_ntl, data['lasso_pred'], colors['gray_blue_purple'], 'Baseline1 NTL (Lasso)', ntl_lasso_r2))
    
    if 'semantic' in models_data:
        data = models_data['semantic']
        y_test_sem = data['y_test']
        all_y_true.extend(y_test_sem)
        all_y_pred.extend(data['xgb_pred'])
        all_y_pred.extend(data['lasso_pred'])
        
        # Calculate R¬≤ for both Semantic models
        sem_xgb_r2 = r2_score(y_test_sem, data['xgb_pred'])
        sem_lasso_r2 = r2_score(y_test_sem, data['lasso_pred'])
        
        model_data.append((y_test_sem, data['xgb_pred'], colors['light_orange'], 'Baseline2 Semantic (XGBoost)', sem_xgb_r2))
        model_data.append((y_test_sem, data['lasso_pred'], colors['blue_green'], 'Baseline2 Semantic (Lasso)', sem_lasso_r2))
    
    # Sort by R¬≤ score (lowest first, so best model is plotted last and appears on top)
    model_data.sort(key=lambda x: x[4])  # Sort by R¬≤ score
    
    # Plot models in order (worst to best)
    for y_true, y_pred, color, label, r2 in model_data:
        create_threshold_curve(ax, y_true, y_pred, color, label, smooth=True)
    
    # Set fixed axis limits for 1:1 precision
    if all_y_true and all_y_pred:
        min_val = min(min(all_y_true), min(all_y_pred))
        max_val = max(max(all_y_true), max(all_y_pred))
        buffer = (max_val - min_val) * 0.05
        ax.set_xlim(min_val - buffer, max_val + buffer)
        ax.set_ylim(min_val - buffer, max_val + buffer)
    
    ax.set_xlabel(f'True {perception_display} Values (Sorted)', fontweight='bold', fontsize=12)
    ax.set_ylabel(f'Predicted {perception_display} Values', fontweight='bold', fontsize=12)
    ax.set_title(f'Baseline Models Nonlinear Threshold Curves\n(Smooth Curves with 95% CI) - {perception_display}', 
                fontweight='bold', fontsize=14)
    ax.grid(True, alpha=0.3)
    ax.set_aspect('equal', adjustable='box')  # Maintain 1:1 aspect ratio
    
    # Legend with fixed position to avoid canvas compression
    ax.legend(loc='upper left', fontsize=8, framealpha=0.9)
    
    plt.subplots_adjust(right=0.95)  # Reserve space but don't compress canvas
    plt.savefig(f'{save_dir}/{perception_name}_baseline_nonlinear.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("    ‚úÖ Nonlinear plots completed with fixed canvas and proper layering")

def plot_scatter_with_fit_line(ax, y_true, y_pred, color, label, alpha=0.6):
    """Plot scatter with fit line and confidence interval, return R¬≤ and slope for legend"""
    from scipy import stats
    
    # Calculate statistics
    slope, intercept, r_value, p_value, std_err = stats.linregress(y_true, y_pred)
    r_squared = r_value**2
    
    # Create enhanced label with R¬≤ and slope
    enhanced_label = f'{label} (R¬≤={r_squared:.3f}, Slope={slope:.3f})'
    
    # Scatter plot - ALWAYS USE CIRCLES (marker='o')
    ax.scatter(y_true, y_pred, alpha=alpha, s=35, color=color, marker='o', 
              edgecolors='white', linewidth=0.5, label=enhanced_label)
    
    # Fit line with slightly higher alpha for better models
    line_alpha = min(0.9, alpha + 0.2)
    line_x = np.linspace(y_true.min(), y_true.max(), 100)
    line_y = slope * line_x + intercept
    
    ax.plot(line_x, line_y, color=color, alpha=line_alpha, linewidth=2.5)
    
    # 95% Confidence interval with adjusted alpha
    residuals = y_pred - (slope * y_true + intercept)
    mse = np.mean(residuals**2)
    ci = 1.96 * np.sqrt(mse)
    
    ci_alpha = min(0.25, alpha * 0.4)  # Confidence interval alpha based on scatter alpha
    ax.fill_between(line_x, line_y - ci, line_y + ci, 
                   color=color, alpha=ci_alpha)
    
    return r_squared, slope

def create_threshold_curve(ax, y_true, y_pred, color, label, smooth=True):
    """Create smooth nonlinear threshold curve with confidence intervals"""
    from scipy.interpolate import UnivariateSpline
    from scipy import stats
    
    # Sort data by true values
    sorted_indices = np.argsort(y_true)
    y_true_sorted = y_true.iloc[sorted_indices] if hasattr(y_true, 'iloc') else y_true[sorted_indices]
    y_pred_sorted = y_pred[sorted_indices]
    
    if smooth:
        # Create smooth curve using spline
        try:
            # Use quantile-based binning for smoother curves
            n_bins = min(50, len(y_true_sorted) // 10)
            quantiles = np.linspace(0, 1, n_bins)
            bin_edges = np.quantile(y_true_sorted, quantiles)
            bin_edges = np.unique(bin_edges)  # Remove duplicates
            
            if len(bin_edges) < 5:
                # Fallback to regular binning
                bin_edges = np.linspace(y_true_sorted.min(), y_true_sorted.max(), n_bins)
            
            # Calculate bin centers and means
            bin_centers = []
            bin_means = []
            bin_stds = []
            
            for i in range(len(bin_edges) - 1):
                mask = (y_true_sorted >= bin_edges[i]) & (y_true_sorted < bin_edges[i + 1])
                if mask.sum() > 2:  # Need at least 3 points
                    bin_centers.append((bin_edges[i] + bin_edges[i + 1]) / 2)
                    bin_means.append(y_pred_sorted[mask].mean())
                    bin_stds.append(y_pred_sorted[mask].std())
            
            if len(bin_centers) >= 4:  # Need at least 4 points for spline
                bin_centers = np.array(bin_centers)
                bin_means = np.array(bin_means)
                bin_stds = np.array(bin_stds)
                
                # Create spline
                s_param = len(bin_centers) * np.var(bin_means) * 0.5  # Smoothing parameter
                spline = UnivariateSpline(bin_centers, bin_means, s=s_param)
                
                # Generate smooth curve
                x_smooth = np.linspace(bin_centers.min(), bin_centers.max(), 200)
                y_smooth = spline(x_smooth)
                
                # Plot smooth curve
                ax.plot(x_smooth, y_smooth, color=color, linewidth=3, label=label, alpha=0.9)
                
                # Add confidence bands using binned standard deviations
                std_spline = UnivariateSpline(bin_centers, bin_stds, s=s_param)
                y_std_smooth = std_spline(x_smooth)
                
                ax.fill_between(x_smooth, y_smooth - 1.96 * y_std_smooth, 
                               y_smooth + 1.96 * y_std_smooth, 
                               color=color, alpha=0.2)
                
                return
        except:
            pass  # Fall back to simple method
    
    # Fallback: simple moving average
    window_size = max(5, len(y_true_sorted) // 20)
    y_pred_smooth = pd.Series(y_pred_sorted).rolling(window=window_size, center=True, min_periods=1).mean()
    y_pred_std = pd.Series(y_pred_sorted).rolling(window=window_size, center=True, min_periods=1).std()
    
    ax.plot(y_true_sorted, y_pred_smooth, color=color, linewidth=3, label=label, alpha=0.9)
    
    # Confidence interval
    ax.fill_between(y_true_sorted, 
                   y_pred_smooth - 1.96 * y_pred_std.fillna(0), 
                   y_pred_smooth + 1.96 * y_pred_std.fillna(0), 
                   color=color, alpha=0.2)

def create_safety_performance_chart(models_data, save_dir, colors, perception_name='safe'):
    """Create perception-specific performance improvement chart with blue-green base and orange improvement"""
    print(f"  üìä Creating {perception_name} performance improvement chart...")
    
    perception_display = perception_name.capitalize()
    
    # Collect performance data
    performance_data = {}
    
    if 'ntl' in models_data:
        data = models_data['ntl']
        performance_data['NTL Baseline'] = {
            'xgb_score': r2_score(data['y_test'], data['xgb_pred']),
            'lasso_score': r2_score(data['y_test'], data['lasso_pred'])
        }
    
    if 'semantic' in models_data:
        data = models_data['semantic']  
        performance_data['Semantic Baseline'] = {
            'xgb_score': r2_score(data['y_test'], data['xgb_pred']),
            'lasso_score': r2_score(data['y_test'], data['lasso_pred'])
        }
    
    if 'full' in models_data:
        data = models_data['full']
        performance_data['Full Model'] = {
            'xgb_score': r2_score(data['y_test'], data['xgb_pred']),
            'lasso_score': r2_score(data['y_test'], data['lasso_pred'])
        }
    
    # Create stacked bar chart
    fig, ax = plt.subplots(1, 1, figsize=(12, 8))
    
    models = list(performance_data.keys())
    xgb_scores = [performance_data[model]['xgb_score'] for model in models]
    lasso_scores = [performance_data[model]['lasso_score'] for model in models]
    
    # Calculate baseline (use NTL as baseline)
    if 'NTL Baseline' in performance_data:
        baseline_xgb = performance_data['NTL Baseline']['xgb_score']
        baseline_lasso = performance_data['NTL Baseline']['lasso_score']
    else:
        baseline_xgb = min(xgb_scores) if xgb_scores else 0
        baseline_lasso = min(lasso_scores) if lasso_scores else 0
    
    # Calculate improvements
    xgb_baseline = [baseline_xgb] * len(models)
    xgb_improvements = [max(0, score - baseline_xgb) for score in xgb_scores]
    
    lasso_baseline = [baseline_lasso] * len(models)
    lasso_improvements = [max(0, score - baseline_lasso) for score in lasso_scores]
    
    x = np.arange(len(models))
    width = 0.35
    
    # XGBoost bars (left side)
    bars1_base = ax.bar(x - width/2, xgb_baseline, width, 
                       label='XGBoost Baseline', color=colors['blue_green'], alpha=0.7)
    bars1_imp = ax.bar(x - width/2, xgb_improvements, width, bottom=xgb_baseline,
                      label='XGBoost Improvement', color=colors['light_orange'], alpha=0.8)
    
    # Lasso bars (right side)
    bars2_base = ax.bar(x + width/2, lasso_baseline, width,
                       label='Lasso Baseline', color=colors['blue_green'], alpha=0.5)
    bars2_imp = ax.bar(x + width/2, lasso_improvements, width, bottom=lasso_baseline,
                      label='Lasso Improvement', color=colors['light_orange'], alpha=0.6)
    
    # Add value labels
    for i, (xgb_score, lasso_score) in enumerate(zip(xgb_scores, lasso_scores)):
        # XGBoost total score
        ax.text(i - width/2, xgb_score + 0.01, f'{xgb_score:.3f}', 
               ha='center', va='bottom', fontweight='bold', fontsize=10)
        # Lasso total score
        ax.text(i + width/2, lasso_score + 0.01, f'{lasso_score:.3f}',
               ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # Improvement percentages
        if i > 0:  # Skip baseline itself
            xgb_imp_pct = (xgb_improvements[i] / baseline_xgb * 100) if baseline_xgb > 0 else 0
            lasso_imp_pct = (lasso_improvements[i] / baseline_lasso * 100) if baseline_lasso > 0 else 0
            
            if xgb_imp_pct > 1:
                ax.text(i - width/2, xgb_score + 0.02, f'+{xgb_imp_pct:.0f}%',
                       ha='center', va='bottom', fontsize=8, color='darkorange', fontweight='bold')
            if lasso_imp_pct > 1:
                ax.text(i + width/2, lasso_score + 0.02, f'+{lasso_imp_pct:.0f}%',
                       ha='center', va='bottom', fontsize=8, color='darkorange', fontweight='bold')
    
    ax.set_xlabel('Model Types', fontweight='bold', fontsize=12)
    ax.set_ylabel('R¬≤ Score', fontweight='bold', fontsize=12)
    ax.set_title(f'{perception_display} Perception Model Performance\nBlue-Green: Baseline, Orange: Improvement', 
                fontweight='bold', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(models)
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    ax.set_ylim(0, max(max(xgb_scores), max(lasso_scores)) * 1.15)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/{perception_name}_performance_improvement.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print("    ‚úÖ Safety performance chart completed")

def create_feature_importance_summary(analyzer, save_dir, user_colors):
    """ÂàõÂª∫ÁâπÂæÅÈáçË¶ÅÊÄßÊ±áÊÄªÂèØËßÜÂåñ"""
    print("    üîç ÁîüÊàêÁâπÂæÅÈáçË¶ÅÊÄßÊ±áÊÄª...")
    
    # ÂàÜÊûêËØ≠‰πâÁ±ªÂà´ÁöÑÈáçË¶ÅÊÄßÊ®°Âºè
    semantic_importance = {}
    interaction_importance = {'A': [], 'B': [], 'D': [], 'AB': [], 'AD': [], 'BD': [], 'ABD': []}
    
    # Ê£ÄÊü•ÂèØÁî®ÁöÑÊéßÂà∂ÂèòÈáè
    control_vars = ['AVGIL', 'spots_area', 'ADCG', 'illumination_uniformity', ]   #'predicted_spillover'
    available_controls = [col for col in control_vars if col in analyzer.merged_data.columns]
    
    # Ê®°ÊãüÁâπÂæÅÈáçË¶ÅÊÄßÂàÜÊûêÔºàÂü∫‰∫éËØ≠‰πâÁ±ªÂà´Ôºâ
    for semantic in USER_SEMANTIC_CLASSES:
        if semantic in analyzer.merged_data.columns:
            # ËÆ°ÁÆóÁÆÄÂçïÁöÑËØ≠‰πâÁõ∏ÂÖ≥ÊÄß‰Ωú‰∏∫ÈáçË¶ÅÊÄßÊåáÊ†á
            correlations = []
            perception_cols = ['safe', 'beautiful', 'lively', 'wealthy', 'depressing', 'boring']
            
            for perception in perception_cols:
                if perception in analyzer.merged_data.columns:
                    corr = analyzer.merged_data[semantic].corr(analyzer.merged_data[perception])
                    correlations.append(abs(corr) if not np.isnan(corr) else 0)
            
            semantic_importance[semantic] = np.mean(correlations) if correlations else 0
    
    # ÂàõÂª∫ÁâπÂæÅÈáçË¶ÅÊÄßÂèØËßÜÂåñ
    fig, axes = plt.subplots(2, 2, figsize=(20, 16))
    
    # 1. ËØ≠‰πâÁ±ªÂà´ÈáçË¶ÅÊÄßÊéíÂêç
    if semantic_importance:
        sorted_semantics = sorted(semantic_importance.items(), key=lambda x: x[1], reverse=True)
        semantic_names = [item[0].title() for item in sorted_semantics]
        semantic_scores = [item[1] for item in sorted_semantics]
        
        bars = axes[0,0].barh(range(len(semantic_names)), semantic_scores, 
                             color=user_colors['primary'], alpha=0.8, edgecolor='white', linewidth=1)
        axes[0,0].set_yticks(range(len(semantic_names)))
        axes[0,0].set_yticklabels(semantic_names)
        axes[0,0].set_xlabel('Average Correlation with Perceptions', fontweight='bold')
        axes[0,0].set_title('Semantic Class Importance Ranking\n(Average Correlation)', fontweight='bold')
        axes[0,0].grid(True, alpha=0.3, axis='x')
        
        # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
        for bar, score in zip(bars, semantic_scores):
            axes[0,0].text(score + max(semantic_scores)*0.02, bar.get_y() + bar.get_height()/2,
                          f'{score:.3f}', ha='left', va='center', fontweight='bold')
    
    # 2. ‰∫§‰∫íÈ°πÁ±ªÂûãÂàÜÂ∏É
    interaction_types = ['A (Pixel)', 'B (Brightness)', 'D (Depth)', 'AB (Pixel√óBrightness)', 
                        'AD (Pixel√óDepth)', 'BD (Brightness√óDepth)', 'ABD (Triple)']
    interaction_counts = [len(USER_SEMANTIC_CLASSES)] * 7  # ÊØèÁßç‰∫§‰∫íÁ±ªÂûãÈÉΩÊúâÁõ∏ÂêåÊï∞Èáè
    
    colors_interaction = [user_colors['primary'], user_colors['secondary'], user_colors['accent1'], 
                         user_colors['accent2'], user_colors['accent3'], user_colors['accent4'], '#FF0000']
    
    axes[0,1].pie(interaction_counts, labels=interaction_types, colors=colors_interaction, 
                 autopct='%1.0f', startangle=90)
    axes[0,1].set_title('Interaction Feature Distribution\n(A+B+D+AB+AD+BD+ABD)', fontweight='bold')
    
    # 3. ÊéßÂà∂ÂèòÈáè‰ΩøÁî®ÊÉÖÂÜµ
    control_status = ['Available', 'Missing']
    control_counts = [len(available_controls), len(control_vars) - len(available_controls)]
    control_colors = [user_colors['accent2'], user_colors['accent3']]
    
    bars = axes[1,0].bar(control_status, control_counts, color=control_colors, alpha=0.8, 
                        edgecolor='white', linewidth=1)
    axes[1,0].set_ylabel('Number of Variables', fontweight='bold')
    axes[1,0].set_title('Control Variables Status\n(AVGIL, spots_area, ADCG, illumination_uniformity)', fontweight='bold')   # predicted_spillover
    axes[1,0].grid(True, alpha=0.3, axis='y')
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æÂíåÂèòÈáèÂêç
    for bar, count, status in zip(bars, control_counts, control_status):
        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(control_counts)*0.02,
                      f'{count}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        if status == 'Available' and available_controls:
            vars_text = '\n'.join(available_controls)
            axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,
                          vars_text, ha='center', va='center', fontweight='bold', fontsize=9,
                          bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
    
    # 4. ÁâπÂæÅÁª¥Â∫¶Ê±áÊÄª
    feature_categories = ['Semantic A', 'Semantic B', 'Semantic D', 'Two-way AB', 'Two-way AD', 'Two-way BD', 'Three-way ABD', 'Controls']
    feature_counts = [len(USER_SEMANTIC_CLASSES)] * 7 + [len(available_controls)]
    
    bars = axes[1,1].bar(range(len(feature_categories)), feature_counts, 
                        color=[user_colors['primary'], user_colors['secondary'], user_colors['accent1'],
                              user_colors['accent2'], user_colors['accent3'], user_colors['accent4'], 
                              '#FF0000', user_colors['accent3']], alpha=0.8, edgecolor='white', linewidth=1)
    
    axes[1,1].set_xticks(range(len(feature_categories)))
    axes[1,1].set_xticklabels(feature_categories, rotation=45, ha='right')
    axes[1,1].set_ylabel('Number of Features', fontweight='bold')
    axes[1,1].set_title('Feature Count by Category\n(Total Features per Model)', fontweight='bold')
    axes[1,1].grid(True, alpha=0.3, axis='y')
    
    # Ê∑ªÂä†Êï∞ÂÄºÊ†áÁ≠æ
    for bar, count in zip(bars, feature_counts):
        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(feature_counts)*0.02,
                      f'{count}', ha='center', va='bottom', fontweight='bold')
    
    # Ê∑ªÂä†ÊÄªËÆ°Ê†áÊ≥®
    total_features = sum(feature_counts)
    axes[1,1].text(0.98, 0.98, f'Total Features: {total_features}', 
                  transform=axes[1,1].transAxes, ha='right', va='top',
                  bbox=dict(boxstyle='round', facecolor=user_colors['accent2'], alpha=0.3),
                  fontsize=12, fontweight='bold')
    
    fig.suptitle('Feature Importance & Architecture Summary\nA+B+D+AB+AD+BD+ABD Model with Control Variables', 
                fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.savefig(f'{save_dir}/feature_importance_summary.png', 
               dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()
    
    print(f"      ‚úÖ ÁâπÂæÅÈáçË¶ÅÊÄßÊ±áÊÄªÂÆåÊàê")

if __name__ == "__main__":
    # ËøêË°å‰∏ªÂàÜÊûêÔºàÂ∑≤ÂåÖÂê´LCZÂíåNTLÂàÜÊûêÔºâ
    main()